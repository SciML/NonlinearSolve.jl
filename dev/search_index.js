var documenterSearchIndex = {"docs":
[{"location":"basics/nonlinear_solution/#solution","page":"Nonlinear Solutions","title":"Nonlinear Solutions","text":"","category":"section"},{"location":"basics/nonlinear_solution/#SciMLBase.AbstractNonlinearSolution","page":"Nonlinear Solutions","title":"SciMLBase.AbstractNonlinearSolution","text":"abstract type AbstractNonlinearSolution{T, N} <: SciMLBase.AbstractNoTimeSolution{T, N}\n\n\n\n\n\n","category":"type"},{"location":"basics/nonlinear_solution/#SciMLBase.NonlinearSolution","page":"Nonlinear Solutions","title":"SciMLBase.NonlinearSolution","text":"struct NonlinearSolution{T, N, uType, R, P, A, O, uType2, S, Tr} <: SciMLBase.AbstractNonlinearSolution{T, N}\n\nRepresentation of the solution to a nonlinear equation defined by a NonlinearProblem, or the steady state solution to a differential equation defined by a SteadyStateProblem.\n\nFields\n\nu: the representation of the nonlinear equation's solution.\nresid: the residual of the solution.\nprob: the original NonlinearProblem/SteadyStateProblem that was solved.\nalg: the algorithm type used by the solver.\noriginal: if the solver is wrapped from an alternative solver ecosystem, such as NLsolve.jl, then this is the original return from said solver library.\nretcode: the return code from the solver. Used to determine whether the solver solved successfully or whether it exited due to an error. For more details, see the return code documentation.\nleft: if the solver is bracketing method, this is the final left bracket value.\nright: if the solver is bracketing method, this is the final right bracket value.\nstats: statistics of the solver, such as the number of function evaluations required.\n\n\n\n\n\n","category":"type"},{"location":"basics/nonlinear_solution/#Statistics","page":"Nonlinear Solutions","title":"Statistics","text":"","category":"section"},{"location":"basics/nonlinear_solution/#SciMLBase.NLStats","page":"Nonlinear Solutions","title":"SciMLBase.NLStats","text":"mutable struct NLStats\n\nStatistics from the nonlinear equation solver about the solution process.\n\nFields\n\nnf: Number of function evaluations.\nnjacs: Number of Jacobians created during the solve.\nnfactors: Number of factorzations of the jacobian required for the solve.\nnsolve: Number of linear solves W\b required for the solve.\nnsteps: Total number of iterations for the nonlinear solver.\n\n\n\n\n\n","category":"type"},{"location":"basics/nonlinear_solution/#Return-Code","page":"Nonlinear Solutions","title":"Return Code","text":"","category":"section"},{"location":"basics/nonlinear_solution/#SciMLBase.ReturnCode.Success","page":"Nonlinear Solutions","title":"SciMLBase.ReturnCode.Success","text":"ReturnCode.Success\n\nThe success state of the solver. If this return code is given, then the solving process was successful, but no extra information about that success is given.\n\nCommon Reasons for Seeing this Return Code\n\nThis is the most common return code and most solvers will give this return code if the solving process went as expected without any errors or detected numerical issues.\n\nProperties\n\nsuccessful_retcode = true\n\n\n\n\n\n","category":"constant"},{"location":"basics/nonlinear_solution/#SciMLBase.ReturnCode.ConvergenceFailure","page":"Nonlinear Solutions","title":"SciMLBase.ReturnCode.ConvergenceFailure","text":"ReturnCode.ConvergenceFailure\n\nA failure exit state of the solver. If this return code is given, then the solving process was unsuccessful because internal nonlinear solver iterations failed to converge.\n\nCommon Reasons for Seeing this Return Code\n\nThe most common reason for seeing this return code is because an inappropriate nonlinear solver was chosen. If fixed point iteration is used on a stiff problem, it will be faster by avoiding the Jacobian but it will make a stiff ODE solver not stable for stiff problems!\nFor nonlinear solvers, this can occur if certain threshold was exceeded. For example, in approximate jacobian solvers like Broyden, Klement, etc. if the number of jacobian resets exceeds the threshold, then this return code is given.\n\nProperties\n\nsuccessful_retcode = false\n\n\n\n\n\n","category":"constant"},{"location":"basics/nonlinear_solution/#SciMLBase.ReturnCode.Unstable","page":"Nonlinear Solutions","title":"SciMLBase.ReturnCode.Unstable","text":"ReturnCode.Unstable\n\nA failure exit state of the solver. If this return code is given, then the solving process was unsuccessful and exited early because the unstable_check function, as given by the unstable_check common keyword argument (or its default), give a true at the current state.\n\nCommon Reasons for Seeing this Return Code\n\nThe most common reason for seeing this return code is because u contains a NaN or Inf value. The default unstable_check only checks for these values.\n\nProperties\n\nsuccessful_retcode = false\n\n\n\n\n\n","category":"constant"},{"location":"basics/nonlinear_solution/#SciMLBase.ReturnCode.MaxIters","page":"Nonlinear Solutions","title":"SciMLBase.ReturnCode.MaxIters","text":"ReturnCode.MaxIters\n\nA failure exit state of the solver. If this return code is given, then the solving process was unsuccessful and exited early because the solver's iterations hit the maxiters either set by default or by the user in the solve/init command.\n\nNote about Nonlinear Optimization\n\nIn nonlinear optimization, many solvers (such as OptimizationOptimisers.Adam) do not have an exit criteria other than iters == maxiters. In this case, the solvers will iterate until maxiters and exit with a Success return code, as that is a successful run of the solver and not considered to be an error state. Solves with early termination criteria, such as Optim.BFGS exiting when the gradient is sufficiently close to zero, will give ReturnCode.MaxIters on exits which require the maximum iteration.\n\nCommon Reasons for Seeing this Return Code\n\nThis commonly occurs in ODE solving if a non-stiff method (e.g. Tsit5) is used in an algorithm choice for a stiff ODE. It is recommended that in such cases, one tries a stiff ODE solver.\nThis commonly occurs in optimization and nonlinear solvers if the tolerance on solve to too low and cannot be achieved due to floating point error or the condition number of the solver matrix. Double check that the chosen tolerance is numerically possible.\n\nProperties\n\nsuccessful_retcode = false\n\n\n\n\n\n","category":"constant"},{"location":"basics/nonlinear_solution/#SciMLBase.ReturnCode.Failure","page":"Nonlinear Solutions","title":"SciMLBase.ReturnCode.Failure","text":"ReturnCode.Failure\n\nA failure exit state of the solver. If this return code is given, then the solving process was unsuccessful but no extra information is given.\n\nCommon Reasons for Seeing this Return Code\n\nThe most common reason for seeing this return code is because the solver is a wrapped solver (i.e. a Fortran code) which does not provide any extra information about its exit state. If this is from a Julia-based solver, please open an issue.\n\nProperties\n\nsuccessful_retcode = false\n\n\n\n\n\n","category":"constant"},{"location":"basics/nonlinear_solution/#SciMLBase.ReturnCode.InternalLineSearchFailed","page":"Nonlinear Solutions","title":"SciMLBase.ReturnCode.InternalLineSearchFailed","text":"ReturnCode.InternalLineSearchFailed\n\nInternal Line Search used by the algorithm has failed.\n\nProperties\n\nsuccessful_retcode = false\n\n\n\n\n\n","category":"constant"},{"location":"basics/nonlinear_solution/#SciMLBase.ReturnCode.Stalled","page":"Nonlinear Solutions","title":"SciMLBase.ReturnCode.Stalled","text":"ReturnCode.Stalled\n\nThe solution has stalled. This is only returned by algorithms for which stalling is a failure mode, such as on a NonlinearProblem where the found solution is larger than the accepted tolerance.\n\nProperties\n\nsuccessful_retcode = false\n\n\n\n\n\n","category":"constant"},{"location":"basics/nonlinear_solution/#SciMLBase.ReturnCode.ShrinkThresholdExceeded","page":"Nonlinear Solutions","title":"SciMLBase.ReturnCode.ShrinkThresholdExceeded","text":"ReturnCode.ShrinkThresholdExceeded\n\nThe trust region radius was shrunk more times than the provided threshold.\n\nProperties\n\nsuccessful_retcode = false\n\n\n\n\n\n","category":"constant"},{"location":"basics/faq/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"basics/faq/#How-is-the-performance-of-Julia's-NonlinearSolve.jl-vs-MATLAB's-fzero?","page":"Frequently Asked Questions","title":"How is the performance of Julia's NonlinearSolve.jl vs MATLAB's fzero?","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This is addressed in a Twitter thread with the author of the improved fzero. On the test example:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"import NonlinearSolve as NLS\nimport BenchmarkTools\n\nconst N = 100_000;\nlevels = 1.5 .* rand(N);\nout = zeros(N);\nmyfun(x, lv) = x * sin(x) - lv\n\nfunction f(out, levels, u0)\n    for i in 1:N\n        out[i] = NLS.solve(\n            NLS.IntervalNonlinearProblem{false}(\n                NLS.IntervalNonlinearFunction{false}(myfun), u0, levels[i]),\n            NLS.Falsi()).u\n    end\nend\n\nfunction f2(out, levels, u0)\n    for i in 1:N\n        out[i] = NLS.solve(\n            NLS.NonlinearProblem{false}(NLS.NonlinearFunction{false}(myfun), u0, levels[i]),\n            NLS.SimpleNewtonRaphson()).u\n    end\nend\n\nBenchmarkTools.@btime f(out, levels, (0.0, 2.0))\nBenchmarkTools.@btime f2(out, levels, 1.0)","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"MATLAB 2022a achieves 1.66s. Try this code yourself: we receive 0.009 seconds, or a 184x speedup.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For more information on performance of SciML, see the SciMLBenchmarks.","category":"page"},{"location":"basics/faq/#The-solver-tried-to-set-a-Dual-Number-in-my-Vector-of-Floats.-How-do-I-fix-that?","page":"Frequently Asked Questions","title":"The solver tried to set a Dual Number in my Vector of Floats. How do I fix that?","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This is a common problem that occurs if the code was not written to be generic based on the input types. For example, consider this example taken from this issue","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"import NonlinearSolve as NLS\nimport Random\n\nfunction fff_incorrect(var, p)\n    v_true = [1.0, 0.1, 2.0, 0.5]\n    xx = [1.0, 2.0, 3.0, 4.0]\n    xx[1] = var[1] - v_true[1]\n    return var - v_true\nend\n\nv_true = [1.0, 0.1, 2.0, 0.5]\nv_init = v_true .+ Random.randn!(similar(v_true)) * 0.1\n\nprob_oop = NLS.NonlinearLeastSquaresProblem{false}(fff_incorrect, v_init)\ntry\n    sol = NLS.solve(prob_oop, NLS.LevenbergMarquardt(); maxiters = 10000, abstol = 1e-8)\ncatch e\n    @error e\nend","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Essentially what happened was, NonlinearSolve checked that we can use ForwardDiff.jl to differentiate the function based on the input types. However, this function has xx = [1.0, 2.0, 3.0, 4.0] followed by a xx[1] = var[1] - v_true[1] where var might be a Dual number. This causes the error. To fix it:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Specify the autodiff to be AutoFiniteDiff\nimport ADTypes\nsol = NLS.solve(prob_oop, NLS.LevenbergMarquardt(; autodiff = ADTypes.AutoFiniteDiff());\n    maxiters = 10000, abstol = 1e-8)\nThis worked but, Finite Differencing is not the recommended approach in any scenario.\nRewrite the function to use PreallocationTools.jl or write it as\nfunction fff_correct(var, p)\n    v_true = [1.0, 0.1, 2.0, 0.5]\n    xx = eltype(var)[1.0, 2.0, 3.0, 4.0]\n    xx[1] = var[1] - v_true[1]\n    return xx - v_true\nend\n\nprob_oop = NLS.NonlinearLeastSquaresProblem{false}(fff_correct, v_init)\nsol = NLS.solve(prob_oop, NLS.LevenbergMarquardt(); maxiters = 10000, abstol = 1e-8)","category":"page"},{"location":"basics/faq/#I-thought-NonlinearSolve.jl-was-type-stable-and-fast.-But-it-isn't,-why?","page":"Frequently Asked Questions","title":"I thought NonlinearSolve.jl was type-stable and fast. But it isn't, why?","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"It is hard to say why your code is not fast. Take a look at the Diagnostics API to pin-point the problem. One common issue is that there is type instability.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you are using the defaults for the autodiff and your problem is not a scalar or using static arrays, ForwardDiff will create type unstable code and lead to dynamic dispatch internally. See this simple example:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"import NonlinearSolve as NLS\nimport InteractiveUtils\nimport ADTypes\n\nf(u, p) = @. u^2 - p\n\nprob = NLS.NonlinearProblem{false}(f, 1.0, 2.0)\n\nInteractiveUtils.@code_warntype NLS.solve(prob, NLS.NewtonRaphson())\nnothing # hide","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Notice that this was type-stable, since it is a scalar problem. Now what happens for static arrays","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"import StaticArrays\n\nprob = NLS.NonlinearProblem{false}(f, StaticArrays.@SVector([1.0, 2.0]), 2.0)\n\nInteractiveUtils.@code_warntype NLS.solve(prob, NLS.NewtonRaphson())\nnothing # hide","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Again Type-Stable! Now let's try using a regular array:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"prob = NLS.NonlinearProblem(f, [1.0, 2.0], 2.0)\n\nInteractiveUtils.@code_warntype NLS.solve(prob, NLS.NewtonRaphson())\nnothing # hide","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Ah it is still type stable. But internally since the chunksize is not statically inferred, it will be dynamic and lead to dynamic dispatch. To fix this, we directly specify the chunksize:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"InteractiveUtils.@code_warntype NLS.solve(\n    prob,\n    NLS.NewtonRaphson(;\n        autodiff = ADTypes.AutoForwardDiff(; chunksize = NLS.pickchunksize(prob.u0))\n    )\n)\nnothing # hide","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"And boom! Type stable again. We always recommend picking the chunksize via NonlinearSolveBase.pickchunksize, however, if you manually specify the chunksize, it must be ‚â§ length of input. However, a very large chunksize can lead to excessive compilation times and slowdown.","category":"page"},{"location":"basics/faq/#NonlinearSolveBase.pickchunksize","page":"Frequently Asked Questions","title":"NonlinearSolveBase.pickchunksize","text":"pickchunksize(x) = pickchunksize(length(x))\npickchunksize(x::Int)\n\nDetermine the chunk size for ForwardDiff and PolyesterForwardDiff based on the input length.\n\n\n\n\n\n","category":"function"},{"location":"native/steadystatediffeq/#SteadyStateDiffEq.jl","page":"SteadyStateDiffEq.jl","title":"SteadyStateDiffEq.jl","text":"","category":"section"},{"location":"native/steadystatediffeq/","page":"SteadyStateDiffEq.jl","title":"SteadyStateDiffEq.jl","text":"This is a wrapper package for using ODE solvers from DifferentialEquations.jl into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"native/steadystatediffeq/","page":"SteadyStateDiffEq.jl","title":"SteadyStateDiffEq.jl","text":"import Pkg\nPkg.add(\"SteadyStateDiffEq\")\nimport SteadyStateDiffEq as SSDE","category":"page"},{"location":"native/steadystatediffeq/","page":"SteadyStateDiffEq.jl","title":"SteadyStateDiffEq.jl","text":"These methods can be used independently of the rest of NonlinearSolve.jl","category":"page"},{"location":"native/steadystatediffeq/","page":"SteadyStateDiffEq.jl","title":"SteadyStateDiffEq.jl","text":"Pages = [\"steadystatediffeq.md\"]","category":"page"},{"location":"native/steadystatediffeq/#Solver-API","page":"SteadyStateDiffEq.jl","title":"Solver API","text":"","category":"section"},{"location":"native/steadystatediffeq/#SteadyStateDiffEq.DynamicSS","page":"SteadyStateDiffEq.jl","title":"SteadyStateDiffEq.DynamicSS","text":"DynamicSS(alg = nothing; tspan = Inf)\n\nRequires that an ODE algorithm is given as the first argument.  The absolute and relative tolerances specify the termination conditions on the derivative's closeness to zero.  This internally uses the TerminateSteadyState callback from the Callback Library. The simulated time for which given ODE is solved can be limited by tspan.  If tspan is a number, it is equivalent to passing (zero(tspan), tspan).\n\nExample usage:\n\nusing SteadyStateDiffEq, OrdinaryDiffEq\nsol = solve(prob, DynamicSS(Tsit5()))\n\nusing Sundials\nsol = solve(prob, DynamicSS(CVODE_BDF()); dt = 1.0)\n\nnote: Note\nThe default alg of nothing works only if DifferentialEquations.jl is installed and loaded.\n\nnote: Note\nIf you use CVODE_BDF you may need to give a starting dt via dt = .....*\n\n\n\n\n\n","category":"type"},{"location":"native/steadystatediffeq/#SteadyStateDiffEq.SSRootfind","page":"SteadyStateDiffEq.jl","title":"SteadyStateDiffEq.SSRootfind","text":"SSRootfind(alg = nothing)\n\nUse a Nonlinear Solver to find the steady state. Requires that a nonlinear solver is given as the first argument.\n\nnote: Note\nThe default alg of nothing works only if NonlinearSolve.jl is installed and loaded.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/operators/#Custom-SciML-Operators","page":"Custom SciML Operators","title":"Custom SciML Operators","text":"","category":"section"},{"location":"devdocs/operators/#Low-Rank-Jacobian-Operators","page":"Custom SciML Operators","title":"Low-Rank Jacobian Operators","text":"","category":"section"},{"location":"devdocs/operators/#NonlinearSolveQuasiNewton.BroydenLowRankJacobian","page":"Custom SciML Operators","title":"NonlinearSolveQuasiNewton.BroydenLowRankJacobian","text":"BroydenLowRankJacobian{T}(U, V·µÄ, idx, cache, alpha)\n\nLow Rank Approximation of the Jacobian Matrix. Currently only used for LimitedMemoryBroyden. This computes the Jacobian as U times V^T.\n\n\n\n\n\n","category":"type"},{"location":"basics/nonlinear_problem/#problems","page":"Nonlinear Problems","title":"Nonlinear Problems","text":"","category":"section"},{"location":"basics/nonlinear_problem/#The-Four-Types-of-Nonlinear-Problems","page":"Nonlinear Problems","title":"The Four Types of Nonlinear Problems","text":"","category":"section"},{"location":"basics/nonlinear_problem/","page":"Nonlinear Problems","title":"Nonlinear Problems","text":"NonlinearSolve.jl tackles four related types of nonlinear systems:","category":"page"},{"location":"basics/nonlinear_problem/","page":"Nonlinear Problems","title":"Nonlinear Problems","text":"Interval rootfinding problems. I.e., find the t in t_0 t_f such that f(t) = 0.\nSystems of nonlinear equations, i.e., find the u such that f(u) = 0.\nSteady state problems, i.e., find the u such that u = f(ut) has reached steady state, i.e., 0 = f(u ).\nThe nonlinear least squares problem, which is an under/over-constrained nonlinear system which might not be satisfiable, i.e. there may be no u such that f(u) = 0, and thus we find the u which minimizes ||f(u)|| in the least squares sense.","category":"page"},{"location":"basics/nonlinear_problem/","page":"Nonlinear Problems","title":"Nonlinear Problems","text":"The first is for solving scalar rootfinding problems, i.e., finding a single number, and requires that a bracketing interval is known. For a bracketing interval, one must have that the sign of f(t_0) is opposite the sign of f(t_f), thus guaranteeing a root in the interval.","category":"page"},{"location":"basics/nonlinear_problem/","page":"Nonlinear Problems","title":"Nonlinear Problems","text":"note: Note\nInterval rootfinding problems allow for f to return an array, in which case the interval rootfinding problem is interpreted as finding the first t such that any of the components of the array hit zero.","category":"page"},{"location":"basics/nonlinear_problem/","page":"Nonlinear Problems","title":"Nonlinear Problems","text":"The second type of nonlinear system can be multidimensional, and thus no ordering nor boundaries are assumed to be known. For a system of nonlinear equations, f can return an array, and the solver seeks the value of u for which all outputs of f are simultaneously zero.","category":"page"},{"location":"basics/nonlinear_problem/","page":"Nonlinear Problems","title":"Nonlinear Problems","text":"The last type if equivalent to a nonlinear system, but with the extra interpretation of having a potentially preferred unique root. That is, when there are multiple u such that f(u) = 0, the NonlinearProblem does not have a preferred solution, while for the SteadyStateProblem the preferred solution is the u(‚àû) that would arise from solving the ODE u' = f(u,t).","category":"page"},{"location":"basics/nonlinear_problem/","page":"Nonlinear Problems","title":"Nonlinear Problems","text":"warning: Warning\nMost solvers for SteadyStateProblem do not guarantee the preferred solution and instead will solve for some u in the set of solutions. The documentation of the nonlinear solvers will note if they return the preferred solution.","category":"page"},{"location":"basics/nonlinear_problem/#Problem-Construction-Details","page":"Nonlinear Problems","title":"Problem Construction Details","text":"","category":"section"},{"location":"basics/nonlinear_problem/#SciMLBase.IntervalNonlinearProblem","page":"Nonlinear Problems","title":"SciMLBase.IntervalNonlinearProblem","text":"Defines an interval nonlinear system problem. Documentation Page: https://docs.sciml.ai/NonlinearSolve/stable/basics/nonlinear_problem/\n\nMathematical Specification of an Interval Nonlinear Problem\n\nTo define a Nonlinear Problem, you simply need to give the function f which defines the nonlinear system:\n\nf(tp) = u = 0\n\nalong with an interval tspan, t in t_0t_f, within which the root should be found. f should be specified as f(t,p) (or in-place as f(u,t,p)), and tspan should be a Tuple{T,T} where T <: Number.\n\nnote: Note\nThe output value u is not required to be a scalar. When u is an AbstractArray, the problem is a simultaneous interval nonlinear problem where the solvers are made to give the first t for which any of the u hit zero. Currently, none of the solvers support this mode.\n\nProblem Type\n\nConstructors\n\nIntervalNonlinearProblem(f::NonlinearFunction, tspan, p = NullParameters(); kwargs...)\nIntervalNonlinearProblem{isinplace}(f, tspan, p = NullParameters(); kwargs...)\n\nisinplace optionally sets whether the function is in-place or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFields\n\nf: The function in the problem.\ntspan: The interval in which the root is to be found.\np: The parameters for the problem. Defaults to NullParameters.\nkwargs: The keyword arguments passed on to the solvers.\n\n\n\n\n\n","category":"type"},{"location":"basics/nonlinear_problem/#SciMLBase.NonlinearProblem","page":"Nonlinear Problems","title":"SciMLBase.NonlinearProblem","text":"Defines a nonlinear system problem. Documentation Page: https://docs.sciml.ai/NonlinearSolve/stable/basics/nonlinear_problem/\n\nMathematical Specification of a Nonlinear Problem\n\nTo define a Nonlinear Problem, you simply need to give the function f which defines the nonlinear system:\n\nf(up) = 0\n\nand an initial guess u‚ÇÄ of where f(u, p) = 0. f should be specified as f(u, p) (or in-place as f(du, u, p)), and u‚ÇÄ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u‚ÇÄ; one is allowed to provide u‚ÇÄ as arbitrary matrices / higher-dimension tensors as well.\n\nProblem Type\n\nConstructors\n\nNonlinearProblem(f::NonlinearFunction, u0, p = NullParameters(); kwargs...)\nNonlinearProblem{isinplace}(f, u0, p = NullParameters(); kwargs...)\n\nisinplace optionally sets whether the function is in-place or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the NonlinearFunctions page.\n\nFields\n\nf: The function in the problem.\nu0: The initial guess for the root.\np: The parameters for the problem. Defaults to NullParameters.\nkwargs: The keyword arguments passed on to the solvers.\n\n\n\n\n\n","category":"type"},{"location":"basics/nonlinear_problem/#SciMLBase.SteadyStateProblem","page":"Nonlinear Problems","title":"SciMLBase.SteadyStateProblem","text":"Defines a steady state ODE problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/steadystatetypes/\n\nMathematical Specification of a Steady State Problem\n\nTo define a Steady State Problem, you simply need to give the function f which defines the ODE:\n\nfracdudt = f(u p t)\n\nand an initial guess u_0 of where f(u, p, t) = 0. f should be specified as f(u, p, t) (or in-place as f(du, u, p, t)), and u‚ÇÄ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u‚ÇÄ; one is allowed to provide u‚ÇÄ as arbitrary matrices / higher dimension tensors as well.\n\nNote that for the steady-state to be defined, we must have that f is autonomous, that is f is independent of t. But the form which matches the standard ODE solver should still be used. The steady state solvers interpret the f by fixing t = infty.\n\nProblem Type\n\nConstructors\n\nSteadyStateProblem(f::ODEFunction, u0, p = NullParameters(); kwargs...)\nSteadyStateProblem{isinplace, specialize}(f, u0, p = NullParameters(); kwargs...)\n\nisinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. specialize optionally controls the specialization level. See the specialization levels section of the SciMLBase documentation for more details. The default is AutoSpecialize.\n\nParameters are optional, and if not given, a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nAdditionally, the constructor from ODEProblems is provided:\n\nSteadyStateProblem(prob::ODEProblem)\n\nParameters are optional, and if not given, a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The function in the ODE.\nu0: The initial guess for the steady state.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\nSpecial Solution Fields\n\nThe SteadyStateSolution type is different from the other DiffEq solutions because it does not have temporal information.\n\n\n\n\n\n","category":"type"},{"location":"basics/nonlinear_problem/#SciMLBase.NonlinearLeastSquaresProblem","page":"Nonlinear Problems","title":"SciMLBase.NonlinearLeastSquaresProblem","text":"Defines a nonlinear least squares problem.\n\nMathematical Specification of a Nonlinear Least Squares Problem\n\nTo define a Nonlinear Problem, you simply need to give the function f which defines the nonlinear system:\n\nundersetxmin  f(x p) \n\nand an initial guess u_0 for the minimization problem. f should be specified as f(u p) (or in-place as f(du u p)), and u_0 should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u_0; one is allowed to provide u_0 as arbitrary matrices / higher-dimension tensors as well.\n\nProblem Type\n\nConstructors\n\nNonlinearLeastSquaresProblem(f::NonlinearFunction, u0, p = NullParameters(); kwargs...)\nNonlinearLeastSquaresProblem{isinplace}(f, u0, p = NullParameters(); kwargs...)\n\nisinplace optionally sets whether the function is in-place or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters.\n\nFor specifying Jacobians and mass matrices, see the NonlinearFunctions page.\n\nFields\n\nf: The function in the problem.\nu0: The initial guess for the solution.\np: The parameters for the problem. Defaults to NullParameters.\nkwargs: The keyword arguments passed on to the solvers.\n\n\n\n\n\n","category":"type"},{"location":"api/fixedpointacceleration/#FixedPointAcceleration.jl","page":"FixedPointAcceleration.jl","title":"FixedPointAcceleration.jl","text":"","category":"section"},{"location":"api/fixedpointacceleration/","page":"FixedPointAcceleration.jl","title":"FixedPointAcceleration.jl","text":"This is a extension for importing solvers from FixedPointAcceleration.jl into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/fixedpointacceleration/","page":"FixedPointAcceleration.jl","title":"FixedPointAcceleration.jl","text":"import Pkg\nPkg.add(\"FixedPointAcceleration\")\nimport FixedPointAcceleration\nimport NonlinearSolve as NLS","category":"page"},{"location":"api/fixedpointacceleration/#Solver-API","page":"FixedPointAcceleration.jl","title":"Solver API","text":"","category":"section"},{"location":"api/fixedpointacceleration/#NonlinearSolve.FixedPointAccelerationJL","page":"FixedPointAcceleration.jl","title":"NonlinearSolve.FixedPointAccelerationJL","text":"FixedPointAccelerationJL(;\n    algorithm = :Anderson, m = missing, condition_number_threshold = missing,\n    extrapolation_period = missing, replace_invalids = :NoAction\n)\n\nWrapper over FixedPointAcceleration.jl for solving Fixed Point Problems. We allow using this algorithm to solve root finding problems as well.\n\nKeyword Arguments\n\nalgorithm: The algorithm to use. Can be :Anderson, :MPE, :RRE, :VEA, :SEA, :Simple, :Aitken or :Newton.\nm: The number of previous iterates to use for the extrapolation. Only valid for :Anderson.\ncondition_number_threshold: The condition number threshold for Least Squares Problem. Only valid for :Anderson.\nextrapolation_period: The number of iterates between extrapolations. Only valid for :MPE, :RRE, :VEA and :SEA. Defaults to 7 for :MPE & :RRE, and 6 for :SEA and :VEA. For :SEA and :VEA, this must be a multiple of 2.\nreplace_invalids: The method to use for replacing invalid iterates. Can be :ReplaceInvalids, :ReplaceVector or :NoAction.\n\nnote: Note\nThis algorithm is only available if FixedPointAcceleration.jl is installed and loaded.\n\n\n\n\n\n","category":"type"},{"location":"api/nlsolve/#NLsolve.jl","page":"NLsolve.jl","title":"NLsolve.jl","text":"","category":"section"},{"location":"api/nlsolve/","page":"NLsolve.jl","title":"NLsolve.jl","text":"This is a extension for importing solvers from NLsolve.jl into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/nlsolve/","page":"NLsolve.jl","title":"NLsolve.jl","text":"import Pkg\nPkg.add(\"NLsolve\")\nimport NLsolve\nimport NonlinearSolve as NLS","category":"page"},{"location":"api/nlsolve/#Solver-API","page":"NLsolve.jl","title":"Solver API","text":"","category":"section"},{"location":"api/nlsolve/#NonlinearSolve.NLsolveJL","page":"NLsolve.jl","title":"NonlinearSolve.NLsolveJL","text":"NLsolveJL(;\n    method = :trust_region, autodiff = :central, linesearch = Static(),\n    linsolve = (x, A, b) -> copyto!(x, A\\b), factor = one(Float64), autoscale = true,\n    m = 10, beta = one(Float64)\n)\n\nKeyword Arguments\n\nmethod: the choice of method for solving the nonlinear system.\nautodiff: the choice of method for generating the Jacobian. Defaults to :central or central differencing via FiniteDiff.jl. The other choices are :forward or ADTypes similar to other solvers in NonlinearSolve.\nlinesearch: the line search method to be used within the solver method. The choices are line search types from LineSearches.jl.\nlinsolve: a function linsolve(x, A, b) that solves Ax = b.\nfactor: determines the size of the initial trust region. This size is set to the product of factor and the euclidean norm of u0 if nonzero, or else to factor itself.\nautoscale: if true, then the variables will be automatically rescaled. The scaling factors are the norms of the Jacobian columns.\nm: the amount of history in the Anderson method. Naive \"Picard\"-style iteration can be achieved by setting m=0, but that isn't advisable for contractions whose Lipschitz constants are close to 1. If convergence fails, though, you may consider lowering it.\nbeta: It is also known as DIIS or Pulay mixing, this method is based on the acceleration of the fixed-point iteration x‚Çô‚Çä‚ÇÅ = x‚Çô + beta*f(x‚Çô), where by default beta = 1.\n\nwarning: Warning\nLine Search Algorithms from LineSearch.jl aren't supported by NLsolveJL. Instead, use the line search algorithms from LineSearches.jl.\n\nSubmethod Choice\n\nChoices for methods in NLsolveJL:\n\n:anderson: Anderson-accelerated fixed-point iteration\n:broyden: Broyden's quasi-Newton method\n:newton: Classical Newton method with an optional line search\n:trust_region: Trust region Newton method (the default choice)\n\nFor more information on these arguments, consult the NLsolve.jl documentation.\n\nnote: Note\nThis algorithm is only available if NLsolve.jl is installed and loaded.\n\n\n\n\n\n","category":"type"},{"location":"api/siamfanlequations/#SIAMFANLEquations.jl","page":"SIAMFANLEquations.jl","title":"SIAMFANLEquations.jl","text":"","category":"section"},{"location":"api/siamfanlequations/","page":"SIAMFANLEquations.jl","title":"SIAMFANLEquations.jl","text":"This is an extension for importing solvers from SIAMFANLEquations.jl into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/siamfanlequations/","page":"SIAMFANLEquations.jl","title":"SIAMFANLEquations.jl","text":"import Pkg\nPkg.add(\"SIAMFANLEquations\")\nimport SIAMFANLEquations\nimport NonlinearSolve as NLS","category":"page"},{"location":"api/siamfanlequations/#Solver-API","page":"SIAMFANLEquations.jl","title":"Solver API","text":"","category":"section"},{"location":"api/siamfanlequations/#NonlinearSolve.SIAMFANLEquationsJL","page":"SIAMFANLEquations.jl","title":"NonlinearSolve.SIAMFANLEquationsJL","text":"SIAMFANLEquationsJL(;\n    method = :newton, delta = 1e-3, linsolve = nothing, autodiff = missing\n)\n\nKeyword Arguments\n\nmethod: the choice of method for solving the nonlinear system.\ndelta: initial pseudo time step, default is 1e-3.\nlinsolve : JFNK linear solvers, choices are gmres and bicgstab.\nm: Depth for Anderson acceleration, default as 0 for Picard iteration.\nbeta: Anderson mixing parameter, change f(x) to (1-beta)x+beta*f(x), equivalent to accelerating damped Picard iteration.\nautodiff: Defaults to missing, which means we will default to letting SIAMFANLEquations construct the jacobian if f.jac is not provided. In other cases, we use it to generate a jacobian similar to other NonlinearSolve solvers.\n\nSubmethod Choice\n\n:newton: Classical Newton method.\n:pseudotransient: Pseudo transient method.\n:secant: Secant method for scalar equations.\n:anderson: Anderson acceleration for fixed point iterations.\n\nnote: Note\nThis algorithm is only available if SIAMFANLEquations.jl is installed and loaded.\n\n\n\n\n\n","category":"type"},{"location":"native/diagnostics/#diagnostics_api_reference","page":"Diagnostics API","title":"Diagnostics API","text":"","category":"section"},{"location":"native/diagnostics/#Timer-Outputs","page":"Diagnostics API","title":"Timer Outputs","text":"","category":"section"},{"location":"native/diagnostics/","page":"Diagnostics API","title":"Diagnostics API","text":"These functions are not exported since the names have a potential for conflict.","category":"page"},{"location":"native/diagnostics/#NonlinearSolveBase.enable_timer_outputs","page":"Diagnostics API","title":"NonlinearSolveBase.enable_timer_outputs","text":"enable_timer_outputs()\n\nEnable TimerOutput for all NonlinearSolve algorithms. This is useful for debugging but has some overhead, so it is disabled by default.\n\n\n\n\n\n","category":"function"},{"location":"native/diagnostics/#NonlinearSolveBase.disable_timer_outputs","page":"Diagnostics API","title":"NonlinearSolveBase.disable_timer_outputs","text":"disable_timer_outputs()\n\nDisable TimerOutput for all NonlinearSolve algorithms. This should be used when NonlinearSolve is being used in performance-critical code.\n\n\n\n\n\n","category":"function"},{"location":"native/diagnostics/#NonlinearSolveBase.@static_timeit","page":"Diagnostics API","title":"NonlinearSolveBase.@static_timeit","text":"@static_timeit to name expr\n\nLike TimerOutputs.@timeit_debug but has zero overhead if TimerOutputs is disabled via NonlinearSolveBase.disable_timer_outputs().\n\n\n\n\n\n","category":"macro"},{"location":"native/diagnostics/#Tracing-API","page":"Diagnostics API","title":"Tracing API","text":"","category":"section"},{"location":"native/diagnostics/#NonlinearSolveBase.TraceAll","page":"Diagnostics API","title":"NonlinearSolveBase.TraceAll","text":"TraceAll(freq)\nTraceAll(; print_frequency = 1, store_frequency::Int = 1)\n\nTraceWithJacobianConditionNumber + Store the Jacobian, u, f(u), and Œ¥u.\n\nwarning: Warning\nThis is very expensive and makes copies of the Jacobian, u, f(u), and Œ¥u.\n\nSee also TraceMinimal and TraceWithJacobianConditionNumber.\n\n\n\n\n\n","category":"function"},{"location":"native/diagnostics/#NonlinearSolveBase.TraceWithJacobianConditionNumber","page":"Diagnostics API","title":"NonlinearSolveBase.TraceWithJacobianConditionNumber","text":"TraceWithJacobianConditionNumber(freq)\nTraceWithJacobianConditionNumber(; print_frequency = 1, store_frequency::Int = 1)\n\nTraceMinimal + Print the Condition Number of the Jacobian.\n\nSee also TraceMinimal and TraceAll.\n\n\n\n\n\n","category":"function"},{"location":"native/diagnostics/#NonlinearSolveBase.TraceMinimal","page":"Diagnostics API","title":"NonlinearSolveBase.TraceMinimal","text":"TraceMinimal(freq)\nTraceMinimal(; print_frequency = 1, store_frequency::Int = 1)\n\nTrace Minimal Information\n\nIteration Number\nf(u) inf-norm\nStep 2-norm\n\nSee also TraceWithJacobianConditionNumber and TraceAll.\n\n\n\n\n\n","category":"function"},{"location":"basics/sparsity_detection/#sparsity-detection","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"","category":"section"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"This section describes how to enable Sparsity Detection. For a detailed tutorial on how to use this in an actual problem, see this tutorial on Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems.","category":"page"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"Notation wise we are trying to solve for x such that nlfunc(x) = 0.","category":"page"},{"location":"basics/sparsity_detection/#Big-Table-for-Determining-Sparsity-Detection-and-Coloring-Algorithms","page":"(Semi-)Automatic Sparsity Detection","title":"Big Table for Determining Sparsity Detection and Coloring Algorithms","text":"","category":"section"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"f.sparsity f.jac_prototype f.colorvec Sparsity Detection Coloring Algorithm\n‚ùå ‚ùå Any NoSparsityDetector() NoColoringAlgorithm()\n‚ùå Not Structured Any NoSparsityDetector() NoColoringAlgorithm()\n‚ùå Structured ‚úÖ KnownJacobianSparsityDetector(f.jac_prototype) GreedyColoringAlgorithm(LargestFirst())\n‚ùå Structured ‚ùå KnownJacobianSparsityDetector(f.jac_prototype) GreedyColoringAlgorithm(LargestFirst())\n- - - - -\nAbstractMatrix ‚ùå ‚úÖ KnownJacobianSparsityDetector(f.sparsity) ConstantColoringAlgorithm(f.colorvec)\nAbstractMatrix ‚ùå ‚ùå KnownJacobianSparsityDetector(f.sparsity) GreedyColoringAlgorithm(LargestFirst())\nAbstractMatrix Not Structured ‚úÖ KnownJacobianSparsityDetector(f.sparsity) ConstantColoringAlgorithm(f.colorvec)\nAbstractMatrix Not Structured ‚ùå KnownJacobianSparsityDetector(f.sparsity) GreedyColoringAlgorithm(LargestFirst())\nAbstractMatrix Structured Any üî¥ üî¥\n- - - - -\nAbstractSparsityDetector ‚ùå Any f.sparsity GreedyColoringAlgorithm(LargestFirst())\nAbstractSparsityDetector Not Structured ‚úÖ f.sparsity ConstantColoringAlgorithm(f.colorvec)\nAbstractSparsityDetector Not Structured ‚ùå f.sparsity GreedyColoringAlgorithm(LargestFirst())\nAbstractSparsityDetector Structured ‚úÖ KnownJacobianSparsityDetector(f.jac_prototype) ConstantColoringAlgorithm(f.colorvec)\nAbstractSparsityDetector Structured ‚ùå KnownJacobianSparsityDetector(f.jac_prototype) GreedyColoringAlgorithm(LargestFirst())","category":"page"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"Structured means either a AbstractSparseMatrix or ArrayInterface.isstructured(x) is true.\n‚ùå means not provided (default)\n‚úÖ means provided\nüî¥ means an error will be thrown\nProviding a colorvec without specifying either sparsity or jac_prototype with a sparse or structured matrix will cause us to ignore the colorvec.\nThe function calls demonstrated above are simply pseudo-code to show the general idea.","category":"page"},{"location":"basics/sparsity_detection/#Case-I:-Sparse-Jacobian-Prototype-is-Provided","page":"(Semi-)Automatic Sparsity Detection","title":"Case I: Sparse Jacobian Prototype is Provided","text":"","category":"section"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"Let's say you have a Sparse Jacobian Prototype jac_prototype, in this case you can create your problem as follows:","category":"page"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"prob = NonlinearProblem(NonlinearFunction(nlfunc; jac_prototype = jac_prototype), x0)","category":"page"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"NonlinearSolve will automatically perform matrix coloring and use sparse differentiation.","category":"page"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"Now you can help the solver further by providing the color vector. This can be done by","category":"page"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"prob = NonlinearProblem(\n    NonlinearFunction(nlfunc; jac_prototype = jac_prototype, colorvec = colorvec), x0)","category":"page"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"If the colorvec is not provided, then it is computed on demand.","category":"page"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"note: Note\nOne thing to be careful about in this case is that colorvec is dependent on the autodiff backend used. ADTypes.mode(ad) isa ADTypes.ForwardMode will assume that the colorvec is the column colorvec, otherwise we will assume that the colorvec is the row colorvec.","category":"page"},{"location":"basics/sparsity_detection/#Case-II:-Sparsity-Detection-algorithm-is-provided","page":"(Semi-)Automatic Sparsity Detection","title":"Case II: Sparsity Detection algorithm is provided","text":"","category":"section"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"If you don't have a Sparse Jacobian Prototype, but you know the which sparsity detection algorithm you want to use, then you can create your problem as follows:","category":"page"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"prob = NonlinearProblem(\n    NonlinearFunction(nlfunc; sparsity = SymbolicsSparsityDetector()), x0)  # Remember to have Symbolics.jl loaded\n# OR\nprob = NonlinearProblem(\n    NonlinearFunction(nlfunc; sparsity = TracerSparsityDetector()), x0) # From SparseConnectivityTracer.jl","category":"page"},{"location":"basics/sparsity_detection/","page":"(Semi-)Automatic Sparsity Detection","title":"(Semi-)Automatic Sparsity Detection","text":"Refer to the documentation of DifferentiationInterface.jl and SparseConnectivityTracer.jl for more information on sparsity detection algorithms.","category":"page"},{"location":"api/SciMLJacobianOperators/#SciMLJacobianOperators.jl","page":"SciMLJacobianOperators.jl","title":"SciMLJacobianOperators.jl","text":"","category":"section"},{"location":"api/SciMLJacobianOperators/","page":"SciMLJacobianOperators.jl","title":"SciMLJacobianOperators.jl","text":"This is a subpackage on NonlinearSolve providing a general purpose JacVec and VecJac operator built on top on DifferentiationInterface.jl.","category":"page"},{"location":"api/SciMLJacobianOperators/","page":"SciMLJacobianOperators.jl","title":"SciMLJacobianOperators.jl","text":"import Pkg\nPkg.add(\"SciMLJacobianOperators\")\nimport SciMLJacobianOperators","category":"page"},{"location":"api/SciMLJacobianOperators/#Jacobian-API","page":"SciMLJacobianOperators.jl","title":"Jacobian API","text":"","category":"section"},{"location":"api/SciMLJacobianOperators/#SciMLJacobianOperators.JacobianOperator","page":"SciMLJacobianOperators.jl","title":"SciMLJacobianOperators.JacobianOperator","text":"JacobianOperator{iip, T} <: AbstractJacobianOperator{T} <: AbstractSciMLOperator{T}\n\nA Jacobian Operator Provides both JVP and VJP without materializing either (if possible).\n\nConstructor\n\nJacobianOperator(prob::AbstractNonlinearProblem, fu, u; jvp_autodiff = nothing,\n    vjp_autodiff = nothing, skip_vjp::Val = Val(false), skip_jvp::Val = Val(false))\n\nBy default, the JacobianOperator will compute JVP. Use Base.adjoint or Base.transpose to switch to VJP.\n\nComputing the VJP\n\nComputing the VJP is done according to the following rules:\n\nIf f has a vjp method, then we use that.\nIf f has a jac method and no vjp_autodiff is provided, then we use jac * v.\nIf vjp_autodiff is provided we using DifferentiationInterface.jl to compute the VJP.\n\nComputing the JVP\n\nComputing the JVP is done according to the following rules:\n\nIf f has a jvp method, then we use that.\nIf f has a jac method and no jvp_autodiff is provided, then we use v * jac.\nIf jvp_autodiff is provided we using DifferentiationInterface.jl to compute the JVP.\n\nSpecial Case (Number)\n\nFor Number inputs, VJP and JVP are not distinct. Hence, if either vjp or jvp is provided, then we use that. If neither is provided, then we use v * jac if jac is provided. Finally, we use the respective autodiff methods to compute the derivative using DifferentiationInterface.jl and multiply by v.\n\nMethods Provided\n\nwarning: Warning\nCurrently it is expected that p during problem construction is same as p during operator evaluation. This restriction will be lifted in the future.\n\n(op::JacobianOperator)(v, u, p): Computes ‚àÇf(u, p)/‚àÇu * v or ‚àÇf(u, p)/‚àÇu·µÄ * v.\n(op::JacobianOperator)(res, v, u, p): Computes ‚àÇf(u, p)/‚àÇu * v or ‚àÇf(u, p)/‚àÇu·µÄ * v and stores the result in res.\n\nSee also VecJacOperator and JacVecOperator.\n\n\n\n\n\n","category":"type"},{"location":"api/SciMLJacobianOperators/#SciMLJacobianOperators.VecJacOperator","page":"SciMLJacobianOperators.jl","title":"SciMLJacobianOperators.VecJacOperator","text":"VecJacOperator(args...; autodiff = nothing, kwargs...)\n\nConstructs a JacobianOperator which only provides the VJP using the vjp_autodiff = autodiff.\n\n\n\n\n\n","category":"function"},{"location":"api/SciMLJacobianOperators/#SciMLJacobianOperators.JacVecOperator","page":"SciMLJacobianOperators.jl","title":"SciMLJacobianOperators.JacVecOperator","text":"JacVecOperator(args...; autodiff = nothing, kwargs...)\n\nConstructs a JacobianOperator which only provides the JVP using the jvp_autodiff = autodiff.\n\n\n\n\n\n","category":"function"},{"location":"api/SciMLJacobianOperators/#Stateful-Operators","page":"SciMLJacobianOperators.jl","title":"Stateful Operators","text":"","category":"section"},{"location":"api/SciMLJacobianOperators/#SciMLJacobianOperators.StatefulJacobianOperator","page":"SciMLJacobianOperators.jl","title":"SciMLJacobianOperators.StatefulJacobianOperator","text":"StatefulJacobianOperator(jac_op::JacobianOperator, u, p)\n\nWrapper over a JacobianOperator which stores the input u and p and defines mul! and * for computing VJPs and JVPs.\n\n\n\n\n\n","category":"type"},{"location":"api/SciMLJacobianOperators/#SciMLJacobianOperators.StatefulJacobianNormalFormOperator","page":"SciMLJacobianOperators.jl","title":"SciMLJacobianOperators.StatefulJacobianNormalFormOperator","text":"StatefulJacobianNormalFormOperator(vjp_operator, jvp_operator, cache)\n\nThis constructs a Normal Form Jacobian Operator, i.e. it constructs the operator corresponding to J·µÄJ where J is the Jacobian Operator. This is not meant to be directly constructed, rather it is constructed with * on two StatefulJacobianOperators.\n\n\n\n\n\n","category":"type"},{"location":"native/solvers/#NonlinearSolve.jl-Solvers","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolve.jl Solvers","text":"","category":"section"},{"location":"native/solvers/","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolve.jl Solvers","text":"These are the native solvers of NonlinearSolve.jl.","category":"page"},{"location":"native/solvers/","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolve.jl Solvers","text":"Pages = [\"solvers.md\"]","category":"page"},{"location":"native/solvers/#General-Keyword-Arguments","page":"NonlinearSolve.jl Solvers","title":"General Keyword Arguments","text":"","category":"section"},{"location":"native/solvers/","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolve.jl Solvers","text":"Several Algorithms share the same specification for common keyword arguments. Those are documented in this section to avoid repetition. Certain algorithms might have additional considerations for these keyword arguments, which are documented in the algorithm's documentation.","category":"page"},{"location":"native/solvers/","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolve.jl Solvers","text":"linsolve: the LinearSolve.jl solvers used for the linear solves within the Newton method. Defaults to nothing, which means it uses the LinearSolve.jl default algorithm choice. For more information on available algorithm choices, see the LinearSolve.jl documentation.\nlinesearch: the line search algorithm to use. Defaults to NoLineSearch(), which means that no line search is performed.\nautodiff: determines the backend used for the Jacobian. Note that this argument is ignored if an analytical Jacobian is passed, as that will be used instead. Defaults to nothing which means that a default is selected according to the problem specification! Valid choices are types from ADTypes.jl.\nvjp_autodiff: similar to autodiff, but is used to compute Jacobian Vector Products. Ignored if the NonlinearFunction contains the jvp function.\nvjp_autodiff: similar to autodiff, but is used to compute Vector Jacobian Products. Ignored if the NonlinearFunction contains the vjp function.\nconcrete_jac: whether to build a concrete Jacobian. If a Krylov-subspace method is used, then the Jacobian will not be constructed and instead direct Jacobian-Vector products J*v are computed using forward-mode automatic differentiation or finite differencing tricks (without ever constructing the Jacobian). However, if the Jacobian is still needed, for example for a preconditioner, concrete_jac = true can be passed in order to force the construction of the Jacobian.","category":"page"},{"location":"native/solvers/#Nonlinear-Solvers","page":"NonlinearSolve.jl Solvers","title":"Nonlinear Solvers","text":"","category":"section"},{"location":"native/solvers/#NonlinearSolveFirstOrder.NewtonRaphson","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveFirstOrder.NewtonRaphson","text":"NewtonRaphson(;\n    concrete_jac = nothing, linsolve = nothing, linesearch = missing,\n    autodiff = nothing, vjp_autodiff = nothing, jvp_autodiff = nothing\n)\n\nAn advanced NewtonRaphson implementation with support for efficient handling of sparse matrices via colored automatic differentiation and preconditioned linear solvers. Designed for large-scale and numerically-difficult nonlinear systems.\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#NonlinearSolveSpectralMethods.DFSane","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveSpectralMethods.DFSane","text":"DFSane(;\n    sigma_min = 1 // 10^10, sigma_max = 1e10, sigma_1 = 1, M::Int = 10,\n    gamma = 1 // 10^4, tau_min = 1 // 10, tau_max = 1 // 2, n_exp::Int = 2,\n    max_inner_iterations::Int = 100, eta_strategy = (fn_1, n, x_n, f_n) -> fn_1 / n^2\n)\n\nA low-overhead and allocation-free implementation of the df-sane method for solving large-scale nonlinear systems of equations. For in depth information about all the parameters and the algorithm, see La Cruz et al. [2].\n\nKeyword Arguments\n\nsigma_min: the minimum value of the spectral coefficient œÉ which is related to the step size in the algorithm. Defaults to 1e-10.\nsigma_max: the maximum value of the spectral coefficient œÉ‚Çô which is related to the step size in the algorithm. Defaults to 1e10.\n\nFor other keyword arguments, see RobustNonMonotoneLineSearch in LineSearch.jl.\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#NonlinearSolveQuasiNewton.Broyden","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveQuasiNewton.Broyden","text":"Broyden(;\n    max_resets::Int = 100, linesearch = nothing, reset_tolerance = nothing,\n    init_jacobian::Val = Val(:identity), autodiff = nothing, alpha = nothing,\n    update_rule = Val(:good_broyden)\n)\n\nAn implementation of Broyden's Method [3] with resetting and line search.\n\nKeyword Arguments\n\nmax_resets: the maximum number of resets to perform. Defaults to 100.\nreset_tolerance: the tolerance for the reset check. Defaults to sqrt(eps(real(eltype(u)))).\nalpha: If init_jacobian is set to Val(:identity), then the initial Jacobian inverse is set to be (Œ±I)‚Åª¬π. Defaults to nothing which implies Œ± = max(norm(u), 1) / (2 * norm(fu)).\ninit_jacobian: the method to use for initializing the jacobian. Defaults to Val(:identity). Choices include:\nVal(:identity): Identity Matrix.\nVal(:true_jacobian): True Jacobian. This is a good choice for differentiable problems.\nupdate_rule: Update Rule for the Jacobian. Choices are:\nVal(:good_broyden): Good Broyden's Update Rule\nVal(:bad_broyden): Bad Broyden's Update Rule\nVal(:diagonal): Only update the diagonal of the Jacobian. This algorithm may be useful for specific problems, but whether it will work may depend strongly on the problem\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#NonlinearSolveQuasiNewton.Klement","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveQuasiNewton.Klement","text":"Klement(;\n    max_resets = 100, linsolve = nothing, linesearch = nothing,\n    alpha = nothing, init_jacobian::Val = Val(:identity),\n    autodiff = nothing\n)\n\nAn implementation of Klement [4] with line search, preconditioning and customizable linear solves. It is recommended to use Broyden for most problems over this.\n\nKeyword Arguments\n\nmax_resets: the maximum number of resets to perform. Defaults to 100.\nalpha: If init_jacobian is set to Val(:identity), then the initial Jacobian inverse is set to be Œ±I. Defaults to 1. Can be set to nothing which implies Œ± = max(norm(u), 1) / (2 * norm(fu)).\ninit_jacobian: the method to use for initializing the jacobian. Defaults to Val(:identity). Choices include:\nVal(:identity): Identity Matrix.\nVal(:true_jacobian): True Jacobian. Our tests suggest that this is not very stable. Instead using Broyden with Val(:true_jacobian) gives faster and more reliable convergence.\nVal(:true_jacobian_diagonal): Diagonal of True Jacobian. This is a good choice for differentiable problems.\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#NonlinearSolveQuasiNewton.LimitedMemoryBroyden","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveQuasiNewton.LimitedMemoryBroyden","text":"LimitedMemoryBroyden(;\n    max_resets::Int = 3, linesearch = nothing, threshold::Val = Val(10),\n    reset_tolerance = nothing, alpha = nothing\n)\n\nAn implementation of LimitedMemoryBroyden [5] with resetting and line search.\n\nKeyword Arguments\n\nmax_resets: the maximum number of resets to perform. Defaults to 3.\nreset_tolerance: the tolerance for the reset check. Defaults to sqrt(eps(real(eltype(u)))).\nthreshold: the number of vectors to store in the low rank approximation. Defaults to Val(10).\nalpha: The initial Jacobian inverse is set to be (Œ±I)‚Åª¬π. Defaults to nothing which implies Œ± = max(norm(u), 1) / (2 * norm(fu)).\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#Nonlinear-Least-Squares-Solvers","page":"NonlinearSolve.jl Solvers","title":"Nonlinear Least Squares Solvers","text":"","category":"section"},{"location":"native/solvers/#NonlinearSolveFirstOrder.GaussNewton","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveFirstOrder.GaussNewton","text":"GaussNewton(;\n    concrete_jac = nothing, linsolve = nothing, linesearch = missing,\n    autodiff = nothing, vjp_autodiff = nothing, jvp_autodiff = nothing\n)\n\nAn advanced GaussNewton implementation with support for efficient handling of sparse matrices via colored automatic differentiation and preconditioned linear solvers. Designed for large-scale and numerically-difficult nonlinear systems.\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#Both-Nonlinear-and-Nonlinear-Least-Squares-Solvers","page":"NonlinearSolve.jl Solvers","title":"Both Nonlinear & Nonlinear Least Squares Solvers","text":"","category":"section"},{"location":"native/solvers/","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolve.jl Solvers","text":"These solvers can be used for both nonlinear and nonlinear least squares problems.","category":"page"},{"location":"native/solvers/#NonlinearSolveFirstOrder.TrustRegion","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveFirstOrder.TrustRegion","text":"TrustRegion(;\n    concrete_jac = nothing, linsolve = nothing,\n    radius_update_scheme = RadiusUpdateSchemes.Simple, max_trust_radius::Real = 0 // 1,\n    initial_trust_radius::Real = 0 // 1, step_threshold::Real = 1 // 10000,\n    shrink_threshold::Real = 1 // 4, expand_threshold::Real = 3 // 4,\n    shrink_factor::Real = 1 // 4, expand_factor::Real = 2 // 1,\n    max_shrink_times::Int = 32,\n    vjp_autodiff = nothing, autodiff = nothing, jvp_autodiff = nothing\n)\n\nAn advanced TrustRegion implementation with support for efficient handling of sparse matrices via colored automatic differentiation and preconditioned linear solvers. Designed for large-scale and numerically-difficult nonlinear systems.\n\nKeyword Arguments\n\nradius_update_scheme: the scheme used to update the trust region radius. Defaults to RadiusUpdateSchemes.Simple. See RadiusUpdateSchemes for more details. For a review on trust region radius update schemes, see Yuan [6].\n\nFor the remaining arguments, see NonlinearSolveFirstOrder.GenericTrustRegionScheme documentation.\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#NonlinearSolveFirstOrder.LevenbergMarquardt","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveFirstOrder.LevenbergMarquardt","text":"LevenbergMarquardt(;\n    linsolve = nothing,\n    damping_initial::Real = 1.0, Œ±_geodesic::Real = 0.75, disable_geodesic = Val(false),\n    damping_increase_factor::Real = 2.0, damping_decrease_factor::Real = 3.0,\n    finite_diff_step_geodesic = 0.1, b_uphill::Real = 1.0, min_damping_D::Real = 1e-8,\n    autodiff = nothing, vjp_autodiff = nothing, jvp_autodiff = nothing\n)\n\nAn advanced Levenberg-Marquardt implementation with the improvements suggested in Transtrum and Sethna [1]. Designed for large-scale and numerically-difficult nonlinear systems.\n\nKeyword Arguments\n\ndamping_initial: the starting value for the damping factor. The damping factor is inversely proportional to the step size. The damping factor is adjusted during each iteration. Defaults to 1.0. See Section 2.1 of Transtrum and Sethna [1].\ndamping_increase_factor: the factor by which the damping is increased if a step is rejected. Defaults to 2.0. See Section 2.1 of Transtrum and Sethna [1].\ndamping_decrease_factor: the factor by which the damping is decreased if a step is accepted. Defaults to 3.0. See Section 2.1 of Transtrum and Sethna [1].\nmin_damping_D: the minimum value of the damping terms in the diagonal damping matrix D·µÄD, where D·µÄD is given by the largest diagonal entries of J·µÄJ yet encountered, where J is the Jacobian. It is suggested by Transtrum and Sethna [1] to use a minimum value of the elements in D·µÄD to prevent the damping from being too small. Defaults to 1e-8.\ndisable_geodesic: Disables Geodesic Acceleration if set to Val(true). It provides a way to trade-off robustness for speed, though in most situations Geodesic Acceleration should not be disabled.\n\nFor the remaining arguments, see GeodesicAcceleration and NonlinearSolveFirstOrder.LevenbergMarquardtTrustRegion documentations.\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#NonlinearSolveFirstOrder.PseudoTransient","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveFirstOrder.PseudoTransient","text":"PseudoTransient(;\n    concrete_jac = nothing, linesearch = missing, alpha_initial = 1e-3,\n    linsolve = nothing,\n    autodiff = nothing, jvp_autodiff = nothing, vjp_autodiff = nothing\n)\n\nAn implementation of PseudoTransient Method [7] that is used to solve steady state problems in an accelerated manner. It uses an adaptive time-stepping to integrate an initial value of nonlinear problem until sufficient accuracy in the desired steady-state is achieved to switch over to Newton's method and gain a rapid convergence. This implementation specifically uses \"switched evolution relaxation\" [8] SER method.\n\nKeyword Arguments\n\nalpha_initial : the initial pseudo time step. It defaults to 1e-3. If it is small, you are going to need more iterations to converge but it can be more stable.\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#Polyalgorithms","page":"NonlinearSolve.jl Solvers","title":"Polyalgorithms","text":"","category":"section"},{"location":"native/solvers/#NonlinearSolveBase.NonlinearSolvePolyAlgorithm","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveBase.NonlinearSolvePolyAlgorithm","text":"NonlinearSolvePolyAlgorithm(algs; start_index::Int = 1)\n\nA general way to define PolyAlgorithms for NonlinearProblem and NonlinearLeastSquaresProblem. This is a container for a tuple of algorithms that will be tried in order until one succeeds. If none succeed, then the algorithm with the lowest residual is returned.\n\nArguments\n\nalgs: a tuple of algorithms to try in-order! (If this is not a Tuple, then the returned algorithm is not type-stable).\n\nKeyword Arguments\n\nstart_index: the index to start at. Defaults to 1.\n\nExample\n\nusing NonlinearSolve\n\nalg = NonlinearSolvePolyAlgorithm((NewtonRaphson(), Broyden()))\n\n\n\n\n\n","category":"type"},{"location":"native/solvers/#NonlinearSolve.FastShortcutNonlinearPolyalg","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolve.FastShortcutNonlinearPolyalg","text":"FastShortcutNonlinearPolyalg(\n    ::Type{T} = Float64;\n    concrete_jac = nothing,\n    linsolve = nothing,\n    must_use_jacobian::Val = Val(false),\n    prefer_simplenonlinearsolve::Val = Val(false),\n    autodiff = nothing, vjp_autodiff = nothing, jvp_autodiff = nothing,\n    u0_len::Union{Int, Nothing} = nothing\n) where {T}\n\nA polyalgorithm focused on balancing speed and robustness. It first tries less robust methods for more performance and then tries more robust techniques if the faster ones fail.\n\nArguments\n\nT: The eltype of the initial guess. It is only used to check if some of the algorithms are compatible with the problem type. Defaults to Float64.\n\nKeyword Arguments\n\nu0_len: The length of the initial guess. If this is nothing, then the length of the initial guess is not checked. If this is an integer and it is less than 25, we use jacobian based methods.\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#NonlinearSolve.FastShortcutNLLSPolyalg","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolve.FastShortcutNLLSPolyalg","text":"FastShortcutNLLSPolyalg(\n    ::Type{T} = Float64;\n    concrete_jac = nothing,\n    linsolve = nothing,\n    autodiff = nothing, vjp_autodiff = nothing, jvp_autodiff = nothing\n)\n\nA polyalgorithm focused on balancing speed and robustness. It first tries less robust methods for more performance and then tries more robust techniques if the faster ones fail.\n\nArguments\n\nT: The eltype of the initial guess. It is only used to check if some of the algorithms are compatible with the problem type. Defaults to Float64.\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#NonlinearSolveFirstOrder.RobustMultiNewton","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveFirstOrder.RobustMultiNewton","text":"RobustMultiNewton(\n    ::Type{T} = Float64;\n    concrete_jac = nothing,\n    linsolve = nothing,\n    autodiff = nothing, vjp_autodiff = nothing, jvp_autodiff = nothing\n)\n\nA polyalgorithm focused on robustness. It uses a mixture of Newton methods with different globalizing techniques (trust region updates, line searches, etc.) in order to find a method that is able to adequately solve the minimization problem.\n\nBasically, if this algorithm fails, then \"most\" good ways of solving your problem fail and you may need to think about reformulating the model (either there is an issue with the model, or more precision / more stable linear solver choice is required).\n\nArguments\n\nT: The eltype of the initial guess. It is only used to check if some of the algorithms are compatible with the problem type. Defaults to Float64.\n\n\n\n\n\n","category":"function"},{"location":"native/solvers/#Advanced-Solvers","page":"NonlinearSolve.jl Solvers","title":"Advanced Solvers","text":"","category":"section"},{"location":"native/solvers/","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolve.jl Solvers","text":"All of the previously mentioned solvers are wrappers around the following solvers. These are meant for advanced users and allow building custom solvers.","category":"page"},{"location":"native/solvers/#NonlinearSolveQuasiNewton.QuasiNewtonAlgorithm","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveQuasiNewton.QuasiNewtonAlgorithm","text":"QuasiNewtonAlgorithm(;\n    linesearch = missing, trustregion = missing, descent, update_rule, reinit_rule,\n    initialization, max_resets::Int = typemax(Int), name::Symbol = :unknown,\n    max_shrink_times::Int = typemax(Int), concrete_jac = Val(false)\n)\n\nNonlinear Solve Algorithms using an Iterative Approximation of the Jacobian. Most common examples include Broyden's Method.\n\nKeyword Arguments\n\ntrustregion: Globalization using a Trust Region Method. This needs to follow the NonlinearSolveBase.AbstractTrustRegionMethod interface.\ndescent: The descent method to use to compute the step. This needs to follow the NonlinearSolveBase.AbstractDescentDirection interface.\nmax_shrink_times: The maximum number of times the trust region radius can be shrunk before the algorithm terminates.\nupdate_rule: The update rule to use to update the Jacobian. This needs to follow the NonlinearSolveBase.AbstractApproximateJacobianUpdateRule interface.\nreinit_rule: The reinitialization rule to use to reinitialize the Jacobian. This needs to follow the NonlinearSolveBase.AbstractResetCondition interface.\ninitialization: The initialization method to use to initialize the Jacobian. This needs to follow the NonlinearSolveBase.AbstractJacobianInitialization interface.\n\n\n\n\n\n","category":"type"},{"location":"native/solvers/#NonlinearSolveFirstOrder.GeneralizedFirstOrderAlgorithm","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveFirstOrder.GeneralizedFirstOrderAlgorithm","text":"GeneralizedFirstOrderAlgorithm(;\n    descent, linesearch = missing,\n    trustregion = missing, autodiff = nothing, vjp_autodiff = nothing,\n    jvp_autodiff = nothing, max_shrink_times::Int = typemax(Int),\n    concrete_jac = Val(false), name::Symbol = :unknown\n)\n\nThis is a Generalization of First-Order (uses Jacobian) Nonlinear Solve Algorithms. The most common example of this is Newton-Raphson Method.\n\nFirst Order here refers to the order of differentiation, and should not be confused with the order of convergence.\n\nKeyword Arguments\n\ntrustregion: Globalization using a Trust Region Method. This needs to follow the NonlinearSolveBase.AbstractTrustRegionMethod interface.\ndescent: The descent method to use to compute the step. This needs to follow the NonlinearSolveBase.AbstractDescentDirection interface.\nmax_shrink_times: The maximum number of times the trust region radius can be shrunk before the algorithm terminates.\n\n\n\n\n\n","category":"type"},{"location":"native/solvers/#NonlinearSolveSpectralMethods.GeneralizedDFSane","page":"NonlinearSolve.jl Solvers","title":"NonlinearSolveSpectralMethods.GeneralizedDFSane","text":"GeneralizedDFSane(; linesearch, sigma_min, sigma_max, sigma_1, name::Symbol = :unknown)\n\nA generalized version of the DF-SANE algorithm. This algorithm is a Jacobian-Free Spectral Method.\n\nArguments\n\nlinesearch: Globalization using a Line Search Method. This is not optional currently, but that restriction might be lifted in the future.\nsigma_min: The minimum spectral parameter allowed. This is used to ensure that the spectral parameter is not too small.\nsigma_max: The maximum spectral parameter allowed. This is used to ensure that the spectral parameter is not too large.\nsigma_1: The initial spectral parameter. If this is not provided, then the algorithm initializes it as sigma_1 = <u, u> / <u, f(u)>.\n\n\n\n\n\n","category":"type"},{"location":"native/descent/#Descent-Subroutines","page":"Descent Subroutines","title":"Descent Subroutines","text":"","category":"section"},{"location":"native/descent/","page":"Descent Subroutines","title":"Descent Subroutines","text":"The following subroutines are available for computing the descent direction.","category":"page"},{"location":"native/descent/","page":"Descent Subroutines","title":"Descent Subroutines","text":"Pages = [\"descent.md\"]","category":"page"},{"location":"native/descent/#Core-Subroutines","page":"Descent Subroutines","title":"Core Subroutines","text":"","category":"section"},{"location":"native/descent/#NonlinearSolveBase.NewtonDescent","page":"Descent Subroutines","title":"NonlinearSolveBase.NewtonDescent","text":"NewtonDescent(; linsolve = nothing)\n\nCompute the descent direction as J Œ¥u = -fu. For non-square Jacobian problems, this is commonly referred to as the Gauss-Newton Descent.\n\nSee also Dogleg, SteepestDescent, DampedNewtonDescent.\n\n\n\n\n\n","category":"type"},{"location":"native/descent/#NonlinearSolveBase.SteepestDescent","page":"Descent Subroutines","title":"NonlinearSolveBase.SteepestDescent","text":"SteepestDescent(; linsolve = nothing)\n\nCompute the descent direction as Œ¥u = -J·µÄfu. The linear solver and preconditioner are only used if J is provided in the inverted form.\n\nSee also Dogleg, NewtonDescent, DampedNewtonDescent.\n\n\n\n\n\n","category":"type"},{"location":"native/descent/#NonlinearSolveBase.DampedNewtonDescent","page":"Descent Subroutines","title":"NonlinearSolveBase.DampedNewtonDescent","text":"DampedNewtonDescent(; linsolve = nothing, initial_damping, damping_fn)\n\nA Newton descent algorithm with damping. The damping factor is computed using the damping_fn function. The descent direction is computed as (J·µÄJ + ŒªD·µÄD) Œ¥u = -fu. For non-square Jacobians, we default to solving for JŒ¥x = -fu and ‚àöŒª‚ãÖD Œ¥x = 0 simultaneously. If the linear solver can't handle non-square matrices, we use the normal form equations (J·µÄJ + ŒªD·µÄD) Œ¥u = J·µÄ fu. Note that this factorization is often the faster choice, but it is not as numerically stable as the least squares solver.\n\nThe damping factor returned must be a non-negative number.\n\nKeyword Arguments\n\ninitial_damping: the initial damping factor to use\ndamping_fn: the function to use to compute the damping factor. This must satisfy the NonlinearSolveBase.AbstractDampingFunction interface.\n\n\n\n\n\n","category":"type"},{"location":"native/descent/#Special-Trust-Region-Descent-Subroutines","page":"Descent Subroutines","title":"Special Trust Region Descent Subroutines","text":"","category":"section"},{"location":"native/descent/#NonlinearSolveBase.Dogleg","page":"Descent Subroutines","title":"NonlinearSolveBase.Dogleg","text":"Dogleg(; linsolve = nothing)\n\nSwitch between Newton's method and the steepest descent method depending on the size of the trust region. The trust region is specified via keyword argument trust_region to solve!.\n\nSee also SteepestDescent, NewtonDescent, DampedNewtonDescent.\n\n\n\n\n\n","category":"type"},{"location":"native/descent/#Special-Levenberg-Marquardt-Descent-Subroutines","page":"Descent Subroutines","title":"Special Levenberg Marquardt Descent Subroutines","text":"","category":"section"},{"location":"native/descent/#NonlinearSolveBase.GeodesicAcceleration","page":"Descent Subroutines","title":"NonlinearSolveBase.GeodesicAcceleration","text":"GeodesicAcceleration(; descent, finite_diff_step_geodesic, Œ±)\n\nUses the descent algorithm to compute the velocity and acceleration terms for the geodesic acceleration method. The velocity and acceleration terms are then combined to compute the descent direction.\n\nThis method in its current form was developed for LevenbergMarquardt. Performance for other methods are not theoretically or experimentally verified.\n\nKeyword Arguments\n\ndescent: the descent algorithm to use for computing the velocity and acceleration.\nfinite_diff_step_geodesic: the step size used for finite differencing used to calculate the geodesic acceleration. Defaults to 0.1 which means that the step size is approximately 10% of the first-order step. See Section 3 of [1].\nŒ±: a factor that determines if a step is accepted or rejected. To incorporate geodesic acceleration as an addition to the Levenberg-Marquardt algorithm, it is necessary that acceptable steps meet the condition frac2av le alpha_textgeodesic, where a is the geodesic acceleration, v is the Levenberg-Marquardt algorithm's step (velocity along a geodesic path) and Œ±_geodesic is some number of order 1. For most problems Œ±_geodesic = 0.75 is a good value but for problems where convergence is difficult Œ±_geodesic = 0.1 is an effective choice. Defaults to 0.75. See Section 3 of Transtrum and Sethna [1].\n\n\n\n\n\n","category":"type"},{"location":"tutorials/modelingtoolkit/#modelingtoolkit","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"","category":"section"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"ModelingToolkit.jl is a symbolic-numeric modeling system for the Julia SciML ecosystem. It adds a high-level interactive interface for the numerical solvers which can make it easy to symbolically modify and generate equations to be solved. The basic form of using ModelingToolkit looks as follows:","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"import ModelingToolkit as MTK\nimport NonlinearSolve as NLS\n\nMTK.@variables x y z\nMTK.@parameters œÉ œÅ Œ≤\n\n# Define a nonlinear system\neqs = [0 ~ œÉ * (y - x), 0 ~ x * (œÅ - z) - y, 0 ~ x * y - Œ≤ * z]\nMTK.@mtkbuild ns = MTK.NonlinearSystem(eqs, [x, y, z], [œÉ, œÅ, Œ≤])\n\nu0 = [x => 1.0, y => 0.0, z => 0.0]\n\nps = [œÉ => 10.0\n      œÅ => 26.0\n      Œ≤ => 8 / 3]\n\nprob = NLS.NonlinearProblem(ns, u0, ps)\nsol = NLS.solve(prob, NLS.NewtonRaphson())","category":"page"},{"location":"tutorials/modelingtoolkit/#Symbolic-Derivations-of-Extra-Functions","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Derivations of Extra Functions","text":"","category":"section"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"As a symbolic system, ModelingToolkit can be used to represent the equations and derive new forms. For example, let's look at the equations:","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"MTK.equations(ns)","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"We can ask it what the Jacobian of our system is via calculate_jacobian:","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"MTK.calculate_jacobian(ns)","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"We can tell MTK to generate a computable form of this analytical Jacobian via jac = true to help the solver use efficient forms:","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"prob = NLS.NonlinearProblem(ns, u0, ps, jac = true)\nsol = NLS.solve(prob, NLS.NewtonRaphson())","category":"page"},{"location":"tutorials/modelingtoolkit/#Symbolic-Simplification-of-Nonlinear-Systems-via-Tearing","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Simplification of Nonlinear Systems via Tearing","text":"","category":"section"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"One of the major reasons for using ModelingToolkit is to allow structural simplification of the systems. It's very easy to write down a mathematical model that, in theory, could be solved more simply. Let's take a look at a quick system:","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"MTK.@variables u1 u2 u3 u4 u5\neqs = [0 ~ u1 - sin(u5), 0 ~ u2 - cos(u1), 0 ~ u3 - hypot(u1, u2),\n    0 ~ u4 - hypot(u2, u3), 0 ~ u5 - hypot(u4, u1)]\nMTK.@named sys = MTK.NonlinearSystem(eqs, [u1, u2, u3, u4, u5], [])","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"If we run structural simplification, we receive the following form:","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"sys = MTK.structural_simplify(sys)","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"MTK.equations(sys)","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"How did it do this? Let's look at the observed to see the relationships that it found:","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"MTK.observed(sys)","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"Using ModelingToolkit, we can build and solve the simplified system:","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"u0 = [u5 .=> 1.0]\nprob = NLS.NonlinearProblem(sys, u0)\nsol = NLS.solve(prob, NLS.NewtonRaphson())","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"We can then use symbolic indexing to retrieve any variable:","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"sol[u1]","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"sol[u2]","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"sol[u3]","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"sol[u4]","category":"page"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"sol[u5]","category":"page"},{"location":"tutorials/modelingtoolkit/#Component-Based-and-Acausal-Modeling","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Component-Based and Acausal Modeling","text":"","category":"section"},{"location":"tutorials/modelingtoolkit/","page":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","title":"Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit","text":"If you're interested in building models in a component or block based form, such as seen in systems like Simulink or Modelica, take a deeper look at ModelingToolkit.jl's documentation which goes into detail on such features.","category":"page"},{"location":"tutorials/code_optimization/#code_optimization","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"","category":"section"},{"location":"tutorials/code_optimization/#General-Code-Optimization-in-Julia","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"General Code Optimization in Julia","text":"","category":"section"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Before starting this tutorial, we recommend the reader to check out one of the many tutorials for optimization Julia code. The following is an incomplete list:","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"The Julia Performance Tips\nMIT 18.337 Course Notes on Optimizing Serial Code\nWhat scientists must know about hardware to write fast code","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"User-side optimizations are important because, for sufficiently difficult problems, most time will be spent inside your f function, the function you are trying to solve. ‚ÄúEfficient solvers\" are those that reduce the required number of f calls to hit the error tolerance. The main ideas for optimizing your nonlinear solver code, or any Julia function, are the following:","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Make it non-allocating\nUse StaticArrays for small arrays\nUse broadcast fusion\nMake it type-stable\nReduce redundant calculations\nMake use of BLAS calls\nOptimize algorithm choice","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"We'll discuss these strategies in the context of nonlinear solvers. Let's start with small systems.","category":"page"},{"location":"tutorials/code_optimization/#Optimizing-Nonlinear-Solver-Code-for-Small-Systems","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Optimizing Nonlinear Solver Code for Small Systems","text":"","category":"section"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Take for example a prototypical small nonlinear solver code in its out-of-place form:","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"import NonlinearSolve as NLS\n\nf(u, p) = u .* u .- p\nu0 = [1.0, 1.0]\np = 2.0\nprob = NLS.NonlinearProblem(f, u0, p)\nsol = NLS.solve(prob, NLS.NewtonRaphson())","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"We can use BenchmarkTools.jl to get more precise timings:","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"import BenchmarkTools\n\nBenchmarkTools.@benchmark NLS.solve(prob, NLS.NewtonRaphson())","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Note that this way of writing the function is a shorthand for:","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"f(u, p) = [u[1] * u[1] - p, u[2] * u[2] - p]","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"where the function f returns an array. This is a common pattern from things like MATLAB's fzero or SciPy's scipy.optimize.fsolve. However, by design it's very slow. In the benchmark you can see that there are many allocations. These allocations cause the memory allocator to take more time than the actual numerics itself, which is one of the reasons why codes from MATLAB and Python end up slow.","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"In order to avoid this issue, we can use a non-allocating \"in-place\" approach. Written out by hand, this looks like:","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"function f(du, u, p)\n    du[1] = u[1] * u[1] - p\n    du[2] = u[2] * u[2] - p\n    return nothing\nend\n\nprob = NLS.NonlinearProblem(f, u0, p)\nBenchmarkTools.@benchmark sol = NLS.solve(prob, NLS.NewtonRaphson())","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Notice how much faster this already runs! We can make this code even simpler by using the .= in-place broadcasting.","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"function f(du, u, p)\n    du .= u .* u .- p\n    return nothing\nend\n\nBenchmarkTools.@benchmark sol = NLS.solve(prob, NLS.NewtonRaphson())","category":"page"},{"location":"tutorials/code_optimization/#Further-Optimizations-for-Small-Nonlinear-Solves-with-Static-Arrays-and-SimpleNonlinearSolve","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Further Optimizations for Small Nonlinear Solves with Static Arrays and SimpleNonlinearSolve","text":"","category":"section"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Allocations are only expensive if they are ‚Äúheap allocations‚Äù. For a more in-depth definition of heap allocations, there are many sources online. But a good working definition is that heap allocations are variable-sized slabs of memory which have to be pointed to, and this pointer indirection costs time. Additionally, the heap has to be managed, and the garbage controllers has to actively keep track of what's on the heap.","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"However, there's an alternative to heap allocations, known as stack allocations. The stack is statically-sized (known at compile time) and thus its accesses are quick. Additionally, the exact block of memory is known in advance by the compiler, and thus re-using the memory is cheap. This means that allocating on the stack has essentially no cost!","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Arrays have to be heap allocated because their size (and thus the amount of memory they take up) is determined at runtime. But there are structures in Julia which are stack-allocated. structs for example are stack-allocated ‚Äúvalue-type‚Äùs. Tuples are a stack-allocated collection. The most useful data structure for NonlinearSolve though is the StaticArray from the package StaticArrays.jl. These arrays have their length determined at compile-time. They are created using macros attached to normal array expressions, for example:","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"import StaticArrays\n\nA = StaticArrays.SA[2.0, 3.0, 5.0]\ntypeof(A)","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Notice that the 3 after SVector gives the size of the SVector. It cannot be changed. Additionally, SVectors are immutable, so we have to create a new SVector to change values. But remember, we don't have to worry about allocations because this data structure is stack-allocated. SArrays have numerous extra optimizations as well: they have fast matrix multiplication, fast QR factorizations, etc. which directly make use of the information about the size of the array. Thus, when possible, they should be used.","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Unfortunately, static arrays can only be used for sufficiently small arrays. After a certain size, they are forced to heap allocate after some instructions and their compile time balloons. Thus, static arrays shouldn't be used if your system has more than ~20 variables. Additionally, only the native Julia algorithms can fully utilize static arrays.","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Let's ***optimize our nonlinear solve using static arrays***. Note that in this case, we want to use the out-of-place allocating form, but this time we want to output a static array. Doing it with broadcasting looks like:","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"f_SA(u, p) = u .* u .- p\n\nu0 = StaticArrays.SA[1.0, 1.0]\np = 2.0\nprob = NLS.NonlinearProblem(f_SA, u0, p)\n\nBenchmarkTools.@benchmark NLS.solve(prob, NLS.NewtonRaphson())","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"Note that only change here is that u0 is made into a StaticArray! If we needed to write f out for a more complex nonlinear case, then we'd simply do the following:","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"f_SA(u, p) = StaticArrays.SA[u[1] * u[1] - p, u[2] * u[2] - p]\n\nBenchmarkTools.@benchmark NLS.solve(prob, NLS.NewtonRaphson())","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"However, notice that this did not give us a speedup but rather a slowdown. This is because many of the methods in NonlinearSolve.jl are designed to scale to larger problems, and thus aggressively do things like caching which is good for large problems but not good for these smaller problems and static arrays. In order to see the full benefit, we need to move to one of the methods from SimpleNonlinearSolve.jl which are designed for these small-scale static examples. Let's now use SimpleNewtonRaphson:","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"BenchmarkTools.@benchmark NLS.solve(prob, NLS.SimpleNewtonRaphson())","category":"page"},{"location":"tutorials/code_optimization/","page":"Code Optimization for Small Nonlinear Systems in Julia","title":"Code Optimization for Small Nonlinear Systems in Julia","text":"And there we go, around 40ns from our starting point of almost 4Œºs!","category":"page"},{"location":"tutorials/iterator_interface/#iterator","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"","category":"section"},{"location":"tutorials/iterator_interface/","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"There is an iterator form of the nonlinear solver which somewhat mirrors the DiffEq integrator interface:","category":"page"},{"location":"tutorials/iterator_interface/","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"import NonlinearSolve as NLS\n\nf(u, p) = u .* u .- 2.0\nu0 = 1.5\nprobB = NLS.NonlinearProblem(f, u0)\n\nnlcache = NLS.init(probB, NLS.NewtonRaphson())","category":"page"},{"location":"tutorials/iterator_interface/","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"init takes the same keyword arguments as solve, but it returns a cache object that satisfies typeof(nlcache) <: AbstractNonlinearSolveCache and can be used to iterate the solver.","category":"page"},{"location":"tutorials/iterator_interface/","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"The iterator interface supports:","category":"page"},{"location":"tutorials/iterator_interface/#CommonSolve.step!-Tuple{NonlinearSolveBase.AbstractNonlinearSolveCache, Vararg{Any}}","page":"Nonlinear Solver Iterator Interface","title":"CommonSolve.step!","text":"step!(\n    cache::AbstractNonlinearSolveCache;\n    recompute_jacobian::Union{Nothing, Bool} = nothing\n)\n\nPerforms one step of the nonlinear solver.\n\nKeyword Arguments\n\nrecompute_jacobian: allows controlling whether the jacobian is recomputed at the current step. If nothing, then the algorithm determines whether to recompute the jacobian. If true or false, then the jacobian is recomputed or not recomputed, respectively. For algorithms that don't use jacobian information, this keyword is ignored with a one-time warning.\n\n\n\n\n\n","category":"method"},{"location":"tutorials/iterator_interface/","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"We can perform 10 steps of the Newton-Raphson solver with the following:","category":"page"},{"location":"tutorials/iterator_interface/","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"for i in 1:10\n    NLS.step!(nlcache)\nend","category":"page"},{"location":"tutorials/iterator_interface/","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"We currently don't implement a Base.iterate interface but that will be added in the future.","category":"page"},{"location":"devdocs/internal_interfaces/#Internal-Abstract-Types","page":"Internal Abstract Types","title":"Internal Abstract Types","text":"","category":"section"},{"location":"devdocs/internal_interfaces/#Solvers","page":"Internal Abstract Types","title":"Solvers","text":"","category":"section"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractNonlinearSolveAlgorithm","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractNonlinearSolveAlgorithm","text":"AbstractNonlinearSolveAlgorithm <: AbstractNonlinearAlgorithm\n\nAbstract Type for all NonlinearSolveBase Algorithms.\n\nInterface Functions\n\nconcrete_jac(alg): whether or not the algorithm uses a concrete Jacobian. Defaults to nothing.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractNonlinearSolveCache","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractNonlinearSolveCache","text":"AbstractNonlinearSolveCache\n\nAbstract Type for all NonlinearSolveBase Caches.\n\nInterface Functions\n\nget_fu(cache): get the residual.\nget_u(cache): get the current state.\nset_fu!(cache, fu): set the residual.\nhas_time_limit(cache): whether or not the solver has a maximum time limit.\nnot_terminated(cache): whether or not the solver has terminated.\nSciMLBase.set_u!(cache, u): set the current state.\nSciMLBase.reinit!(cache, u0; kwargs...): reinitialize the cache with the initial state u0 and any additional keyword arguments.\nSciMLBase.isinplace(cache): whether or not the solver is inplace.\nCommonSolve.step!(cache; kwargs...): See CommonSolve.step! for more details.\nget_abstol(cache): get the abstol provided to the cache.\nget_reltol(cache): get the reltol provided to the cache.\n\nAdditionally implements SymbolicIndexingInterface interface Functions.\n\nExpected Fields in Sub-Types\n\nFor the default interface implementations we expect the following fields to be present in the cache:\n\nfu: the residual.\nu: the current state.\nmaxiters: the maximum number of iterations.\nnsteps: the number of steps taken.\nforce_stop: whether or not the solver has been forced to stop.\nretcode: the return code.\nstats: NLStats object.\nalg: the algorithm.\nmaxtime: the maximum time limit for the solver. (Optional)\ntimer: the timer for the solver. (Optional)\ntotal_time: the total time taken by the solver. (Optional)\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#Descent-Directions","page":"Internal Abstract Types","title":"Descent Directions","text":"","category":"section"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractDescentDirection","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractDescentDirection","text":"AbstractDescentDirection\n\nAbstract Type for all Descent Directions used in NonlinearSolveBase. Given the Jacobian J and the residual fu, these algorithms compute the descent direction Œ¥u.\n\nFor non-square Jacobian problems, if we need to solve a linear solve problem, we use a least squares solver by default, unless the provided linsolve can't handle non-square matrices, in which case we use the normal form equations J·µÄJ Œ¥u = J·µÄ fu. Note that this factorization is often the faster choice, but it is not as numerically stable as the least squares solver.\n\nInternalAPI.init specification\n\nInternalAPI.init(\n    prob::AbstractNonlinearProblem, alg::AbstractDescentDirection, J, fu, u;\n    pre_inverted::Val = Val(false), linsolve_kwargs = (;),\n    abstol = nothing, reltol = nothing, alias_J::Bool = true,\n    shared::Val = Val(1), kwargs...\n)::AbstractDescentCache\n\npre_inverted: whether or not the Jacobian has been pre_inverted.\nlinsolve_kwargs: keyword arguments to pass to the linear solver.\nabstol: absolute tolerance for the linear solver.\nreltol: relative tolerance for the linear solver.\nalias_J: whether or not to alias the Jacobian.\nshared: Store multiple descent directions in the cache. Allows efficient and correct reuse of factorizations if needed.\n\nSome of the algorithms also allow additional keyword arguments. See the documentation for the specific algorithm for more information.\n\nInterface Functions\n\nsupports_trust_region(alg): whether or not the algorithm supports trust region methods. Defaults to false.\nsupports_line_search(alg): whether or not the algorithm supports line search methods. Defaults to false.\n\nSee also NewtonDescent, Dogleg, SteepestDescent, DampedNewtonDescent.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractDescentCache","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractDescentCache","text":"AbstractDescentCache\n\nAbstract Type for all Descent Caches.\n\nInternalAPI.solve! specification\n\nInternalAPI.solve!(\n    cache::AbstractDescentCache, J, fu, u, idx::Val;\n    skip_solve::Bool = false, new_jacobian::Bool = true, kwargs...\n)::DescentResult\n\nJ: Jacobian or Inverse Jacobian (if pre_inverted = Val(true)).\nfu: residual.\nu: current state.\nidx: index of the descent problem to solve and return. Defaults to Val(1).\nskip_solve: Skip the direction computation and return the previous direction. Defaults to false. This is useful for Trust Region Methods where the previous direction was rejected and we want to try with a modified trust region.\nnew_jacobian: Whether the Jacobian has been updated. Defaults to true.\nkwargs: keyword arguments to pass to the linear solver if there is one.\n\nReturned values\n\ndescent_result: Result in a DescentResult.\n\nInterface Functions\n\nget_du(cache): get the descent direction.\nget_du(cache, ::Val{N}): get the Nth descent direction.\nset_du!(cache, Œ¥u): set the descent direction.\nset_du!(cache, Œ¥u, ::Val{N}): set the Nth descent direction.\nlast_step_accepted(cache): whether or not the last step was accepted. Checks if the cache has a last_step_accepted field and returns it if it does, else returns true.\npreinverted_jacobian(cache): whether or not the Jacobian has been preinverted.\nnormal_form(cache): whether or not the linear solver uses normal form.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#Descent-Results","page":"Internal Abstract Types","title":"Descent Results","text":"","category":"section"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.DescentResult","page":"Internal Abstract Types","title":"NonlinearSolveBase.DescentResult","text":"DescentResult(;\n    Œ¥u = missing, u = missing, success::Bool = true, linsolve_success::Bool = true,\n    extras = (;)\n)\n\nConstruct a DescentResult object.\n\nKeyword Arguments\n\nŒ¥u: The descent direction.\nu: The new iterate. This is provided only for multi-step methods currently.\nsuccess: Certain Descent Algorithms can reject a descent direction for example GeodesicAcceleration.\nlinsolve_success: Whether the line search was successful.\nextras: A named tuple containing intermediates computed during the solve. For example, GeodesicAcceleration returns NamedTuple{(:v, :a)} containing the \"velocity\" and \"acceleration\" terms.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#Approximate-Jacobian","page":"Internal Abstract Types","title":"Approximate Jacobian","text":"","category":"section"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractApproximateJacobianStructure","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractApproximateJacobianStructure","text":"AbstractApproximateJacobianStructure\n\nAbstract Type for all Approximate Jacobian Structures used in NonlinearSolve.jl.\n\nInterface Functions\n\nstores_full_jacobian(alg): whether or not the algorithm stores the full Jacobian. Defaults to false.\nget_full_jacobian(cache, alg, J): get the full Jacobian. Defaults to throwing an error if stores_full_jacobian(alg) is false.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractJacobianInitialization","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractJacobianInitialization","text":"AbstractJacobianInitialization\n\nAbstract Type for all Jacobian Initialization Algorithms used in NonlinearSolveBase.\n\nInterface Functions\n\njacobian_initialized_preinverted(alg): whether or not the Jacobian is initialized preinverted. Defaults to false.\n\nInternalAPI.init specification\n\nInternalAPI.init(\n    prob::AbstractNonlinearProblem, alg::AbstractJacobianInitialization, solver,\n    f, fu, u, p;\n    linsolve = missing, internalnorm::IN = L2_NORM, kwargs...\n)::AbstractJacobianCache\n\nAll subtypes need to define (cache::AbstractJacobianCache)(alg::NewSubType, fu, u) which reinitializes the Jacobian in cache.J.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractApproximateJacobianUpdateRule","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractApproximateJacobianUpdateRule","text":"AbstractApproximateJacobianUpdateRule\n\nAbstract Type for all Approximate Jacobian Update Rules used in NonlinearSolveBase.\n\nInterface Functions\n\nstore_inverse_jacobian(alg): Return alg.store_inverse_jacobian\n\nInternalAPI.init specification\n\nInternalAPI.init(\n    prob::AbstractNonlinearProblem, alg::AbstractApproximateJacobianUpdateRule, J, fu, u,\n    du, args...; internalnorm = L2_NORM, kwargs...\n)::AbstractApproximateJacobianUpdateRuleCache\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractApproximateJacobianUpdateRuleCache","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractApproximateJacobianUpdateRuleCache","text":"AbstractApproximateJacobianUpdateRuleCache\n\nAbstract Type for all Approximate Jacobian Update Rule Caches used in NonlinearSolveBase.\n\nInterface Functions\n\nstore_inverse_jacobian(cache): Return store_inverse_jacobian(cache.rule)\n\nInternalAPI.solve! specification\n\nInternalAPI.solve!(\n    cache::AbstractApproximateJacobianUpdateRuleCache, J, fu, u, du; kwargs...\n) --> J / J‚Åª¬π\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractResetCondition","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractResetCondition","text":"AbstractResetCondition\n\nCondition for resetting the Jacobian in Quasi-Newton's methods.\n\nInternalAPI.init specification\n\nInternalAPI.init(\n    alg::AbstractResetCondition, J, fu, u, du, args...; kwargs...\n)::AbstractResetConditionCache\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#Damping-Algorithms","page":"Internal Abstract Types","title":"Damping Algorithms","text":"","category":"section"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractDampingFunction","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractDampingFunction","text":"AbstractDampingFunction\n\nAbstract Type for Damping Functions in DampedNewton.\n\nInternalAPI.init specification\n\nInternalAPI.init(\n    prob::AbstractNonlinearProblem, f::AbstractDampingFunction, initial_damping,\n    J, fu, u, args...;\n    internalnorm = L2_NORM, kwargs...\n)::AbstractDampingFunctionCache\n\nReturns a NonlinearSolveBase.AbstractDampingFunctionCache.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractDampingFunctionCache","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractDampingFunctionCache","text":"AbstractDampingFunctionCache\n\nAbstract Type for the Caches created by AbstractDampingFunctions\n\nInterface Functions\n\nrequires_normal_form_jacobian(alg): whether or not the Jacobian is needed in normal form. No default.\nrequires_normal_form_rhs(alg): whether or not the residual is needed in normal form. No default.\nreturns_norm_form_damping(alg): whether or not the damping function returns the damping factor in normal form. Defaults to requires_normal_form_jacobian(alg) || requires_normal_form_rhs(alg).\n(cache::AbstractDampingFunctionCache)(::Nothing): returns the damping factor. The type of the damping factor returned from solve! is guaranteed to be the same as this.\n\nInternalAPI.solve! specification\n\nInternalAPI.solve!(\n    cache::AbstractDampingFunctionCache, J, fu, u, Œ¥u, descent_stats\n)\n\nReturns the damping factor.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#Trust-Region","page":"Internal Abstract Types","title":"Trust Region","text":"","category":"section"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractTrustRegionMethod","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractTrustRegionMethod","text":"AbstractTrustRegionMethod\n\nAbstract Type for all Trust Region Methods used in NonlinearSolveBase.\n\nInternalAPI.init specification\n\nInternalAPI.init(\n    prob::AbstractNonlinearProblem, alg::AbstractTrustRegionMethod, f, fu, u, p, args...;\n    internalnorm = L2_NORM, kwargs...\n)::AbstractTrustRegionMethodCache\n\n\n\n\n\n","category":"type"},{"location":"devdocs/internal_interfaces/#NonlinearSolveBase.AbstractTrustRegionMethodCache","page":"Internal Abstract Types","title":"NonlinearSolveBase.AbstractTrustRegionMethodCache","text":"AbstractTrustRegionMethodCache\n\nAbstract Type for all Trust Region Method Caches used in NonlinearSolveBase.\n\nInterface Functions\n\nlast_step_accepted(cache): whether or not the last step was accepted. Defaults to cache.last_step_accepted. Should if overloaded if the field is not present.\n\nInternalAPI.solve! specification\n\nInternalAPI.solve!(\n    cache::AbstractTrustRegionMethodCache, J, fu, u, Œ¥u, descent_stats; kwargs...\n)\n\nReturns last_step_accepted, updated u_cache and fu_cache. If the last step was accepted then these values should be copied into the toplevel cache.\n\n\n\n\n\n","category":"type"},{"location":"basics/autodiff/#Automatic-Differentiation-Backends","page":"Automatic Differentiation Backends","title":"Automatic Differentiation Backends","text":"","category":"section"},{"location":"basics/autodiff/","page":"Automatic Differentiation Backends","title":"Automatic Differentiation Backends","text":"note: Note\nWe support all backends supported by DifferentiationInterface.jl. Please refer to the backends page for more information.","category":"page"},{"location":"basics/autodiff/#Summary-of-Finite-Differencing-Backends","page":"Automatic Differentiation Backends","title":"Summary of Finite Differencing Backends","text":"","category":"section"},{"location":"basics/autodiff/","page":"Automatic Differentiation Backends","title":"Automatic Differentiation Backends","text":"AutoFiniteDiff: Finite differencing using FiniteDiff.jl, not optimal but always applicable.\nAutoFiniteDifferences: Finite differencing using FiniteDifferences.jl, not optimal but always applicable.","category":"page"},{"location":"basics/autodiff/#Summary-of-Forward-Mode-AD-Backends","page":"Automatic Differentiation Backends","title":"Summary of Forward Mode AD Backends","text":"","category":"section"},{"location":"basics/autodiff/","page":"Automatic Differentiation Backends","title":"Automatic Differentiation Backends","text":"AutoForwardDiff: The best choice for dense problems.\nAutoPolyesterForwardDiff: Might be faster than AutoForwardDiff for large problems. Requires PolyesterForwardDiff.jl to be installed and loaded.","category":"page"},{"location":"basics/autodiff/#Summary-of-Reverse-Mode-AD-Backends","page":"Automatic Differentiation Backends","title":"Summary of Reverse Mode AD Backends","text":"","category":"section"},{"location":"basics/autodiff/","page":"Automatic Differentiation Backends","title":"Automatic Differentiation Backends","text":"AutoZygote: The fastest choice for non-mutating array-based (BLAS) functions.\nAutoEnzyme: Uses Enzyme.jl Reverse Mode and works for both in-place and out-of-place functions.","category":"page"},{"location":"basics/autodiff/","page":"Automatic Differentiation Backends","title":"Automatic Differentiation Backends","text":"tip: Tip\nFor sparsity detection and sparse AD take a look at sparsity detection.","category":"page"},{"location":"tutorials/large_systems/#large_systems","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"","category":"section"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"This tutorial is for getting into the extra features of using NonlinearSolve.jl. Solving ill-conditioned nonlinear systems requires specializing the linear solver on properties of the Jacobian in order to cut down on the mathcalO(n^3) linear solve and the mathcalO(n^2) back-solves. This tutorial is designed to explain the advanced usage of NonlinearSolve.jl by solving the steady state stiff Brusselator partial differential equation (BRUSS) using NonlinearSolve.jl.","category":"page"},{"location":"tutorials/large_systems/#Definition-of-the-Brusselator-Equation","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Definition of the Brusselator Equation","text":"","category":"section"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"note: Note\nFeel free to skip this section: it simply defines the example problem.","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"The Brusselator PDE is defined as follows:","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"beginalign\n0 = 1 + u^2v - 44u + alpha(fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2) + f(x y t)\n0 = 34u - u^2v + alpha(fracpartial^2 vpartial x^2 + fracpartial^2 vpartial y^2)\nendalign","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"where","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"f(x y t) = begincases\n5  quad textif  (x-03)^2+(y-06)^2  01^2 text and  t  11 \n0  quad textelse\nendcases","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"and the initial conditions are","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"beginalign\nu(x y 0) = 22cdot (y(1-y))^32 \nv(x y 0) = 27cdot (x(1-x))^32\nendalign","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"with the periodic boundary condition","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"beginalign\nu(x+1yt) = u(xyt) \nu(xy+1t) = u(xyt)\nendalign","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"To solve this PDE, we will discretize it into a system of ODEs with the finite difference method. We discretize u and v into arrays of the values at each time point: u[i,j] = u(i*dx,j*dy) for some choice of dx/dy, and same for v. Then our ODE is defined with U[i,j,k] = [u v]. The second derivative operator, the Laplacian, discretizes to become a tridiagonal matrix with [1 -2 1] and a 1 in the top right and bottom left corners. The nonlinear functions are then applied at each point in space (they are broadcast). Use dx=dy=1/32.","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"The resulting NonlinearProblem definition is:","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"import NonlinearSolve as NLS\nimport LinearAlgebra\nimport SparseArrays\nimport LinearSolve as LS\nimport ADTypes\n\nconst N = 32\nconst xyd_brusselator = range(0, stop = 1, length = N)\n\nbrusselator_f(x, y) = (((x - 0.3)^2 + (y - 0.6)^2) <= 0.1^2) * 5.0\nlimit(a, N) = a == N + 1 ? 1 : a == 0 ? N : a\n\nfunction brusselator_2d_loop(du, u, p)\n    A, B, alpha, dx = p\n    alpha = alpha / dx^2\n    @inbounds for I in CartesianIndices((N, N))\n        i, j = Tuple(I)\n        x, y = xyd_brusselator[I[1]], xyd_brusselator[I[2]]\n        ip1, im1, jp1,\n        jm1 = limit(i + 1, N), limit(i - 1, N), limit(j + 1, N),\n        limit(j - 1, N)\n        du[i, j, 1] = alpha * (u[im1, j, 1] + u[ip1, j, 1] + u[i, jp1, 1] + u[i, jm1, 1] -\n                       4u[i, j, 1]) +\n                      B +\n                      u[i, j, 1]^2 * u[i, j, 2] - (A + 1) * u[i, j, 1] + brusselator_f(x, y)\n        du[i, j, 2] = alpha * (u[im1, j, 2] + u[ip1, j, 2] + u[i, jp1, 2] + u[i, jm1, 2] -\n                       4u[i, j, 2]) + A * u[i, j, 1] - u[i, j, 1]^2 * u[i, j, 2]\n    end\nend\np = (3.4, 1.0, 10.0, step(xyd_brusselator))\n\nfunction init_brusselator_2d(xyd)\n    N = length(xyd)\n    u = zeros(N, N, 2)\n    for I in CartesianIndices((N, N))\n        x = xyd[I[1]]\n        y = xyd[I[2]]\n        u[I, 1] = 22 * (y * (1 - y))^(3 / 2)\n        u[I, 2] = 27 * (x * (1 - x))^(3 / 2)\n    end\n    u\nend\n\nu0 = init_brusselator_2d(xyd_brusselator)\nprob_brusselator_2d = NLS.NonlinearProblem(\n    brusselator_2d_loop, u0, p; abstol = 1e-10, reltol = 1e-10\n)","category":"page"},{"location":"tutorials/large_systems/#Choosing-Jacobian-Types","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Choosing Jacobian Types","text":"","category":"section"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"When we are solving this nonlinear problem, the Jacobian must be built at many iterations, and this can be one of the most expensive steps. There are two pieces that must be optimized in order to reach maximal efficiency when solving stiff equations: the sparsity pattern and the construction of the Jacobian. The construction is filling the matrix J with values, while the sparsity pattern is what J to use.","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"The sparsity pattern is given by a prototype matrix, the jac_prototype, which will be copied to be used as J. The default is for J to be a Matrix, i.e. a dense matrix. However, if you know the sparsity of your problem, then you can pass a different matrix type. For example, a SparseMatrixCSC will give a sparse matrix. Other sparse matrix types include:","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"Bidiagonal\nTridiagonal\nSymTridiagonal\nBandedMatrix (BandedMatrices.jl)\nBlockBandedMatrix (BlockBandedMatrices.jl)","category":"page"},{"location":"tutorials/large_systems/#Approximate-Sparsity-Detection-and-Sparse-Jacobians","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Approximate Sparsity Detection & Sparse Jacobians","text":"","category":"section"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"In the next section, we will show how to specify sparsity to trigger automatic sparsity detection.","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"import BenchmarkTools # for @btime\n\nBenchmarkTools.BenchmarkTools.@btime NLS.solve(prob_brusselator_2d, NLS.NewtonRaphson());\nnothing # hide","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"import SparseConnectivityTracer\n\nprob_brusselator_2d_autosparse = NLS.NonlinearProblem(\n    NLS.NonlinearFunction(brusselator_2d_loop; sparsity = SparseConnectivityTracer.TracerSparsityDetector()),\n    u0, p; abstol = 1e-10, reltol = 1e-10\n)\n\nBenchmarkTools.@btime NLS.solve(prob_brusselator_2d_autosparse,\n    NLS.NewtonRaphson(; autodiff = ADTypes.AutoForwardDiff(; chunksize = 12)));\nBenchmarkTools.@btime NLS.solve(prob_brusselator_2d_autosparse,\n    NLS.NewtonRaphson(; autodiff = ADTypes.AutoForwardDiff(; chunksize = 12),\n        linsolve = LS.KLUFactorization()));\nBenchmarkTools.@btime NLS.solve(prob_brusselator_2d_autosparse,\n    NLS.NewtonRaphson(; autodiff = ADTypes.AutoForwardDiff(; chunksize = 12),\n        linsolve = LS.KrylovJL_GMRES()));\nnothing # hide","category":"page"},{"location":"tutorials/large_systems/#Declaring-a-Sparse-Jacobian-with-Automatic-Sparsity-Detection","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Declaring a Sparse Jacobian with Automatic Sparsity Detection","text":"","category":"section"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"Jacobian sparsity is declared by the jac_prototype argument in the NonlinearFunction. Note that you should only do this if the sparsity is high, for example, 0.1% of the matrix is non-zeros, otherwise the overhead of sparse matrices can be higher than the gains from sparse differentiation!","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"One of the useful companion tools for NonlinearSolve.jl is ADTypes.jl that specifies the interface for sparsity detection via jacobian_sparsity. This allows for automatic declaration of Jacobian sparsity types. To see this in action, we can give an example du and u and call jacobian_sparsity on our function with the example arguments, and it will kick out a sparse matrix with our pattern, that we can turn into our jac_prototype.","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"tip: Tip\nExternal packages like SparseConnectivityTracer.jl and Symbolics.jl provide the actual implementation of sparsity detection.","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"import SparseConnectivityTracer\nimport ADTypes\n\nf! = (du, u) -> brusselator_2d_loop(du, u, p)\ndu0 = similar(u0)\njac_sparsity = ADTypes.jacobian_sparsity(f!, du0, u0, SparseConnectivityTracer.TracerSparsityDetector())","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"Notice that Julia gives a nice print out of the sparsity pattern. That's neat, and would be tedious to build by hand! Now we just pass it to the NonlinearFunction like as before:","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"ff = NLS.NonlinearFunction(brusselator_2d_loop; jac_prototype = jac_sparsity)","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"Build the NonlinearProblem:","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"prob_brusselator_2d_sparse = NLS.NonlinearProblem(ff, u0, p; abstol = 1e-10, reltol = 1e-10)","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"Now let's see how the version with sparsity compares to the version without:","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"BenchmarkTools.@btime NLS.solve(prob_brusselator_2d, NLS.NewtonRaphson());\nBenchmarkTools.@btime NLS.solve(prob_brusselator_2d_sparse, NLS.NewtonRaphson());\nBenchmarkTools.@btime NLS.solve(prob_brusselator_2d_sparse, NLS.NewtonRaphson(linsolve = LS.KLUFactorization()));\nnothing # hide","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"Note that depending on the properties of the sparsity pattern, one may want to try alternative linear solvers such as NLS.NewtonRaphson(linsolve = LS.KLUFactorization()) or NLS.NewtonRaphson(linsolve = LS.UMFPACKFactorization())","category":"page"},{"location":"tutorials/large_systems/#Using-Jacobian-Free-Newton-Krylov","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Using Jacobian-Free Newton-Krylov","text":"","category":"section"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"A completely different way to optimize the linear solvers for large sparse matrices is to use a Krylov subspace method. This requires choosing a linear solver for changing to a Krylov method. To swap the linear solver out, we use the linsolve command and choose the GMRES linear solver.","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"BenchmarkTools.@btime NLS.solve(prob_brusselator_2d, NLS.NewtonRaphson(linsolve = LS.KrylovJL_GMRES()));\nnothing # hide","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"Notice that this acceleration does not require the definition of a sparsity pattern, and can thus be an easier way to scale for large problems. For more information on linear solver choices, see the linear solver documentation. linsolve choices are any valid LinearSolve.jl solver.","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"note: Note\nSwitching to a Krylov linear solver will automatically change the nonlinear problem solver into Jacobian-free mode, dramatically reducing the memory required. This can be overridden by adding concrete_jac=true to the algorithm.","category":"page"},{"location":"tutorials/large_systems/#Adding-a-Preconditioner","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Adding a Preconditioner","text":"","category":"section"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"Any LinearSolve.jl-compatible preconditioner can be used as a preconditioner in the linear solver interface. To define preconditioners, one must define a precs function in compatible with linear solvers which returns the left and right preconditioners, matrices which approximate the inverse of W = I - gamma*J used in the solution of the ODE. An example of this with using IncompleteLU.jl is as follows:","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"# FIXME: On 1.10+ this is broken. Skipping this for now.\nimport IncompleteLU\n\nincompletelu(W, p = nothing) = IncompleteLU.ilu(W, œÑ = 50.0), LinearAlgebra.I\n\nBenchmarkTools.@btime NLS.solve(prob_brusselator_2d_sparse,\n    NLS.NewtonRaphson(linsolve = LS.KrylovJL_GMRES(precs = incompletelu), concrete_jac = true)\n);\nnothing # hide","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"Notice a few things about this preconditioner. This preconditioner uses the sparse Jacobian, and thus we set concrete_jac = true to tell the algorithm to generate the Jacobian (otherwise, a Jacobian-free algorithm is used with GMRES by default).","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"We use convert(AbstractMatrix,W) to get the concrete W matrix (matching jac_prototype, thus SpraseMatrixCSC) which we can use in the preconditioner's definition. Then we use IncompleteLU.ilu on that sparse matrix to generate the preconditioner. We return Pl, nothing to say that our preconditioner is a left preconditioner, and that there is no right preconditioning.","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"This method thus uses both the Krylov solver and the sparse Jacobian. Not only that, it is faster than both implementations! IncompleteLU is fussy in that it requires a well-tuned œÑ parameter. Another option is to use AlgebraicMultigrid.jl which is more automatic. The setup is very similar to before:","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"import AlgebraicMultigrid\n\nfunction algebraicmultigrid(W, p = nothing)\n    return AlgebraicMultigrid.aspreconditioner(AlgebraicMultigrid.ruge_stuben(convert(AbstractMatrix, W))),\n    LinearAlgebra.I\nend\n\nBenchmarkTools.@btime NLS.solve(prob_brusselator_2d_sparse,\n    NLS.NewtonRaphson(\n        linsolve = LS.KrylovJL_GMRES(; precs = algebraicmultigrid), concrete_jac = true\n    )\n);\nnothing # hide","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"or with a Jacobi smoother:","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"function algebraicmultigrid2(W, p = nothing)\n    A = convert(AbstractMatrix, W)\n    Pl = AlgebraicMultigrid.aspreconditioner(AlgebraicMultigrid.ruge_stuben(\n        A, presmoother = AlgebraicMultigrid.Jacobi(rand(size(A, 1))),\n        postsmoother = AlgebraicMultigrid.Jacobi(rand(size(A, 1)))\n    ))\n    return Pl, LinearAlgebra.I\nend\n\nBenchmarkTools.@btime NLS.solve(\n    prob_brusselator_2d_sparse,\n    NLS.NewtonRaphson(\n        linsolve = LS.KrylovJL_GMRES(precs = algebraicmultigrid2), concrete_jac = true\n    )\n);\nnothing # hide","category":"page"},{"location":"tutorials/large_systems/#Let's-compare-the-Sparsity-Detection-Methods","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Let's compare the Sparsity Detection Methods","text":"","category":"section"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"We benchmarked the solvers before with approximate and exact sparsity detection. However, for the exact sparsity detection case, we left out the time it takes to perform exact sparsity detection. Let's compare the two by setting the sparsity detection algorithms.","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"import DifferentiationInterface\nimport SparseConnectivityTracer\n\nprob_brusselator_2d_exact_tracer = NLS.NonlinearProblem(\n    NLS.NonlinearFunction(brusselator_2d_loop; sparsity = SparseConnectivityTracer.TracerSparsityDetector()),\n    u0, p; abstol = 1e-10, reltol = 1e-10)\nprob_brusselator_2d_approx_di = NLS.NonlinearProblem(\n    NLS.NonlinearFunction(brusselator_2d_loop;\n        sparsity = DifferentiationInterface.DenseSparsityDetector(ADTypes.AutoForwardDiff(); atol = 1e-4)),\n    u0, p; abstol = 1e-10, reltol = 1e-10)\n\nBenchmarkTools.@btime NLS.solve(prob_brusselator_2d_exact_tracer, NLS.NewtonRaphson());\nBenchmarkTools.@btime NLS.solve(prob_brusselator_2d_approx_di, NLS.NewtonRaphson());\nnothing # hide","category":"page"},{"location":"tutorials/large_systems/","page":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","title":"Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia","text":"For more information on the preconditioner interface, see the linear solver documentation.","category":"page"},{"location":"devdocs/algorithm_helpers/#Internal-Algorithm-Helpers","page":"Internal Algorithm Helpers","title":"Internal Algorithm Helpers","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#Pseudo-Transient-Method","page":"Internal Algorithm Helpers","title":"Pseudo Transient Method","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveFirstOrder.SwitchedEvolutionRelaxation","page":"Internal Algorithm Helpers","title":"NonlinearSolveFirstOrder.SwitchedEvolutionRelaxation","text":"SwitchedEvolutionRelaxation()\n\nMethod for updating the damping parameter in the PseudoTransient method based on \"switched evolution relaxation\" [8] SER method.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveFirstOrder.SwitchedEvolutionRelaxationCache","page":"Internal Algorithm Helpers","title":"NonlinearSolveFirstOrder.SwitchedEvolutionRelaxationCache","text":"SwitchedEvolutionRelaxationCache <: AbstractDampingFunctionCache\n\nCache for the SwitchedEvolutionRelaxation method.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#Approximate-Jacobian-Methods","page":"Internal Algorithm Helpers","title":"Approximate Jacobian Methods","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#Initialization","page":"Internal Algorithm Helpers","title":"Initialization","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.IdentityInitialization","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.IdentityInitialization","text":"IdentityInitialization(alpha, structure)\n\nInitialize the Jacobian to be an Identity Matrix scaled by alpha and maintain the structure as specified by structure.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.TrueJacobianInitialization","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.TrueJacobianInitialization","text":"TrueJacobianInitialization(structure, autodiff)\n\nInitialize the Jacobian to be the true Jacobian and maintain the structure as specified by structure. autodiff is used to compute the true Jacobian and if not specified we make a selection automatically.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.BroydenLowRankInitialization","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.BroydenLowRankInitialization","text":"BroydenLowRankInitialization(alpha, threshold::Val)\n\nAn initialization for LimitedMemoryBroyden that uses a low rank approximation of the Jacobian. The low rank updates to the Jacobian matrix corresponds to what SciPy calls \"simple\".\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#Jacobian-Structure","page":"Internal Algorithm Helpers","title":"Jacobian Structure","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.FullStructure","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.FullStructure","text":"FullStructure()\n\nStores the full matrix.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.DiagonalStructure","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.DiagonalStructure","text":"DiagonalStructure()\n\nPreserves only the Diagonal of the Matrix.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#Jacobian-Caches","page":"Internal Algorithm Helpers","title":"Jacobian Caches","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.InitializedApproximateJacobianCache","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.InitializedApproximateJacobianCache","text":"InitializedApproximateJacobianCache(\n    J, structure, alg, cache, initialized::Bool, internalnorm\n)\n\nA cache for Approximate Jacobian.\n\nArguments\n\nJ: The current Jacobian.\nstructure: The structure of the Jacobian.\nalg: The initialization algorithm.\ncache: The Jacobian cache NonlinearSolveBase.construct_jacobian_cache (if needed).\ninitialized: A boolean indicating whether the Jacobian has been initialized.\ninternalnorm: The norm to be used.\n\nInterface\n\n(cache::InitializedApproximateJacobianCache)(::Nothing)\n\nReturns the current Jacobian cache.J with the proper structure.\n\nInternalAPI.solve!(cache::InitializedApproximateJacobianCache, fu, u, ::Val{reinit})\n\nSolves for the Jacobian cache.J and returns it. If reinit is true, then the Jacobian is reinitialized.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#Reset-Methods","page":"Internal Algorithm Helpers","title":"Reset Methods","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.NoChangeInStateReset","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.NoChangeInStateReset","text":"NoChangeInStateReset(;\n    nsteps::Int = 3, reset_tolerance = nothing,\n    check_du::Bool = true, check_dfu::Bool = true\n)\n\nRecommends a reset if the state or the function value has not changed significantly in nsteps steps. This is used in Broyden.\n\nKeyword Arguments\n\nnsteps: the number of steps to check for no change. Defaults to 3.\nreset_tolerance: the tolerance for the reset check. Defaults to eps(real(eltype(u)))^(3 // 4).\ncheck_du: whether to check the state. Defaults to true.\ncheck_dfu: whether to check the function value. Defaults to true.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.IllConditionedJacobianReset","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.IllConditionedJacobianReset","text":"IllConditionedJacobianReset()\n\nRecommend resetting the Jacobian if the current jacobian is ill-conditioned. This is used in Klement.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#Update-Rules","page":"Internal Algorithm Helpers","title":"Update Rules","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.GoodBroydenUpdateRule","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.GoodBroydenUpdateRule","text":"GoodBroydenUpdateRule()\n\nBroyden Update Rule corresponding to \"good broyden's method\" [3].\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.BadBroydenUpdateRule","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.BadBroydenUpdateRule","text":"BadBroydenUpdateRule()\n\nBroyden Update Rule corresponding to \"bad broyden's method\" [3].\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveQuasiNewton.KlementUpdateRule","page":"Internal Algorithm Helpers","title":"NonlinearSolveQuasiNewton.KlementUpdateRule","text":"KlementUpdateRule()\n\nUpdate rule for Klement.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#Levenberg-Marquardt-Method","page":"Internal Algorithm Helpers","title":"Levenberg Marquardt Method","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveFirstOrder.LevenbergMarquardtTrustRegion","page":"Internal Algorithm Helpers","title":"NonlinearSolveFirstOrder.LevenbergMarquardtTrustRegion","text":"LevenbergMarquardtTrustRegion(b_uphill)\n\nTrust Region method for LevenbergMarquardt. This method is tightly coupled with the Levenberg-Marquardt method and works by directly updating the damping parameter instead of specifying a trust region radius.\n\nArguments\n\nb_uphill: a factor that determines if a step is accepted or rejected. The standard choice in the Levenberg-Marquardt method is to accept all steps that decrease the cost and reject all steps that increase the cost. Although this is a natural and safe choice, it is often not the most efficient. Therefore downhill moves are always accepted, but uphill moves are only conditionally accepted. To decide whether an uphill move will be accepted at each iteration i, we compute beta_i = cos(v_textnew v_textold), which denotes the cosine angle between the proposed velocity v_textnew and the velocity of the last accepted step v_textold. The idea is to accept uphill moves if the angle is small. To specify, uphill moves are accepted if (1-beta_i)^b_textuphill C_i+1 le C_i, where C_i is the cost at iteration i. Reasonable choices for b_uphill are 1.0 or 2.0, with b_uphill = 2.0 allowing higher uphill moves than b_uphill = 1.0. When b_uphill = 0.0, no uphill moves will be accepted. Defaults to 1.0. See Section 4 of Transtrum and Sethna [1].\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#Trust-Region-Method","page":"Internal Algorithm Helpers","title":"Trust Region Method","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveFirstOrder.GenericTrustRegionScheme","page":"Internal Algorithm Helpers","title":"NonlinearSolveFirstOrder.GenericTrustRegionScheme","text":"GenericTrustRegionScheme(;\n    method = RadiusUpdateSchemes.Simple,\n    max_trust_radius = nothing, initial_trust_radius = nothing,\n    step_threshold = nothing, shrink_threshold = nothing, expand_threshold = nothing,\n    shrink_factor = nothing, expand_factor = nothing\n)\n\nTrust Region Method that updates and stores the current trust region radius in trust_region. For any of the keyword arguments, if the value is nothing, then we use the value used in the respective paper.\n\nKeyword Arguments\n\nradius_update_scheme: the choice of radius update scheme to be used. Defaults to RadiusUpdateSchemes.Simple which follows the conventional approach. Other available schemes are documented in RadiusUpdateSchemes,. These schemes have the trust region radius converging to zero that is seen to improve convergence. For more details, see [1].\nmax_trust_radius: the maximal trust region radius. Defaults to max(norm(fu), maximum(u) - minimum(u)), except for RadiusUpdateSchemes.NLsolve where it defaults to Inf.\ninitial_trust_radius: the initial trust region radius. Defaults to max_trust_radius / 11, except for RadiusUpdateSchemes.NLsolve where it defaults to u0_norm > 0 ? u0_norm : 1.\nstep_threshold: the threshold for taking a step. In every iteration, the threshold is compared with a value r, which is the actual reduction in the objective function divided by the predicted reduction. If step_threshold > r the model is not a good approximation, and the step is rejected. Defaults to nothing.\nshrink_threshold: the threshold for shrinking the trust region radius. In every iteration, the threshold is compared with a value r which is the actual reduction in the objective function divided by the predicted reduction. If shrink_threshold > r the trust region radius is shrunk by shrink_factor. Defaults to nothing.\nexpand_threshold: the threshold for expanding the trust region radius. If a step is taken, i.e step_threshold < r (with r defined in shrink_threshold), a check is also made to see if expand_threshold < r. If that is true, the trust region radius is expanded by expand_factor. Defaults to nothing.\nshrink_factor: the factor to shrink the trust region radius with if shrink_threshold > r (with r defined in shrink_threshold). Defaults to 0.25.\nexpand_factor: the factor to expand the trust region radius with if expand_threshold < r (with r defined in shrink_threshold). Defaults to 2.0.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/algorithm_helpers/#Miscellaneous","page":"Internal Algorithm Helpers","title":"Miscellaneous","text":"","category":"section"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveBase.callback_into_cache!","page":"Internal Algorithm Helpers","title":"NonlinearSolveBase.callback_into_cache!","text":"callback_into_cache!(cache, internalcache, args...)\n\nDefine custom operations on internalcache tightly coupled with the calling cache. args... contain the sequence of caches calling into internalcache.\n\nThis unfortunately makes code very tightly coupled and not modular. It is recommended to not use this functionality unless it can't be avoided (like in LevenbergMarquardt).\n\n\n\n\n\n","category":"function"},{"location":"devdocs/algorithm_helpers/#NonlinearSolveBase.concrete_jac","page":"Internal Algorithm Helpers","title":"NonlinearSolveBase.concrete_jac","text":"concrete_jac(alg::AbstractNonlinearSolveAlgorithm)::Bool\n\nWhether the algorithm uses a concrete Jacobian.\n\n\n\n\n\n","category":"function"},{"location":"basics/solve/#solver_options","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"","category":"section"},{"location":"basics/solve/#CommonSolve.solve-Tuple{NonlinearProblem, Vararg{Any}}","page":"Common Solver Options (Solve Keyword Arguments)","title":"CommonSolve.solve","text":"solve(prob::NonlinearProblem, alg::Union{AbstractNonlinearAlgorithm,Nothing}; kwargs...)\n\nArguments\n\nThe only positional argument is alg which is optional. By default, alg = nothing. If alg = nothing, then solve dispatches to the NonlinearSolve.jl automated algorithm selection (if using NonlinearSolve was done, otherwise it will error with a MethodError).\n\nKeyword Arguments\n\nThe NonlinearSolve.jl universe has a large set of common arguments available for the solve function. These arguments apply to solve on any problem type and are only limited by limitations of the specific implementations.\n\nMany of the defaults depend on the algorithm or the package the algorithm derives from. Not all of the interface is provided by every algorithm. For more detailed information on the defaults and the available options for specific algorithms / packages, see the manual pages for the solvers of specific problems.\n\nError Control\n\nabstol: Absolute tolerance.\nreltol: Relative tolerance.\n\nMiscellaneous\n\nmaxiters: Maximum number of iterations before stopping. Defaults to 1e5.\nverbose: Toggles whether warnings are thrown when the solver exits early. Defaults to true.\n\nSensitivity Algorithms (sensealg)\n\nsensealg is used for choosing the way the automatic differentiation is performed.     For more information, see the documentation for SciMLSensitivity:     https://docs.sciml.ai/SciMLSensitivity/stable/\n\n\n\n\n\n","category":"method"},{"location":"basics/solve/#General-Controls","page":"Common Solver Options (Solve Keyword Arguments)","title":"General Controls","text":"","category":"section"},{"location":"basics/solve/","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"alias_u0::Bool: Whether to alias the initial condition or use a copy. Defaults to false.\ninternalnorm::Function: The norm used by the solver. Default depends on algorithm choice.","category":"page"},{"location":"basics/solve/#Iteration-Controls","page":"Common Solver Options (Solve Keyword Arguments)","title":"Iteration Controls","text":"","category":"section"},{"location":"basics/solve/","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"maxiters::Int: The maximum number of iterations to perform. Defaults to 1000.\nmaxtime: The maximum time for solving the nonlinear system of equations. Defaults to nothing which means no time limit. Note that setting a time limit does have a small overhead.\nabstol::Number: The absolute tolerance. Defaults to real(oneunit(T)) * (eps(real(one(T))))^(4 // 5).\nreltol::Number: The relative tolerance. Defaults to real(oneunit(T)) * (eps(real(one(T))))^(4 // 5).\ntermination_condition: Termination Condition from NonlinearSolveBase. Defaults to AbsSafeBestTerminationMode() for NonlinearSolve.jl and AbsTerminateMode() for SimpleNonlinearSolve.jl.","category":"page"},{"location":"basics/solve/#Tracing-Controls","page":"Common Solver Options (Solve Keyword Arguments)","title":"Tracing Controls","text":"","category":"section"},{"location":"basics/solve/","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"These are exclusively available for native NonlinearSolve.jl solvers.","category":"page"},{"location":"basics/solve/","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"show_trace: Must be Val(true) or Val(false). This controls whether the trace is displayed to the console. (Defaults to Val(false))\ntrace_level: Needs to be one of Trace Objects: TraceMinimal, TraceWithJacobianConditionNumber, or TraceAll. This controls the level of detail of the trace. (Defaults to TraceMinimal())\nstore_trace: Must be Val(true) or Val(false). This controls whether the trace is stored in the solution object. (Defaults to Val(false))","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/#Nonlinear-Least-Squares-Solvers","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"","category":"section"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"solve(prob::NonlinearLeastSquaresProblem, alg; kwargs...)","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"Solves the nonlinear least squares problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/#Recommended-Methods","page":"Nonlinear Least Squares Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"The default method FastShortcutNLLSPolyalg is a good choice for most problems. It is a polyalgorithm that attempts to use a fast algorithm (GaussNewton) and if that fails it falls back to a more robust algorithms (LevenbergMarquardt, TrustRegion).","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/#Full-List-of-Methods","page":"Nonlinear Least Squares Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/nonlinear_least_squares_solvers/#NonlinearSolve.jl","page":"Nonlinear Least Squares Solvers","title":"NonlinearSolve.jl","text":"","category":"section"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"LevenbergMarquardt(): An advanced Levenberg-Marquardt implementation with the improvements suggested in the Transtrum and Sethna [1]. Designed for large-scale and numerically-difficult nonlinear systems.\nGaussNewton(): A Gauss-Newton method with swappable nonlinear solvers and autodiff methods for high performance on large and sparse systems.\nTrustRegion(): A Newton Trust Region dogleg method with swappable nonlinear solvers and autodiff methods for high performance on large and sparse systems.","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/#SimpleNonlinearSolve.jl","page":"Nonlinear Least Squares Solvers","title":"SimpleNonlinearSolve.jl","text":"","category":"section"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"These methods are included with NonlinearSolve.jl by default, though SimpleNonlinearSolve.jl can be used  directly to reduce dependencies and improve load times. SimpleNonlinearSolve.jl's methods excel at small problems and problems defined with static arrays.","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"SimpleGaussNewton(): Simple Gauss Newton implementation using QR factorizations for numerical stability (aliased to SimpleNewtonRaphson).","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/#fastlm_wrapper_summary","page":"Nonlinear Least Squares Solvers","title":"FastLevenbergMarquardt.jl","text":"","category":"section"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"A wrapper over FastLevenbergMarquardt.jl. Note that it is called FastLevenbergMarquardt since the original package is called \"Fast\", though benchmarks demonstrate LevenbergMarquardt() usually outperforms.","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"FastLevenbergMarquardtJL(linsolve = :cholesky), can also choose linsolve = :qr.","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/#lso_wrapper_summary","page":"Nonlinear Least Squares Solvers","title":"LeastSquaresOptim.jl","text":"","category":"section"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"A wrapper over LeastSquaresOptim.jl. Has a core algorithm LeastSquaresOptimJL(alg; linsolve) where the choices for alg are:","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":":lm a Levenberg-Marquardt implementation\n:dogleg a trust-region dogleg Gauss-Newton","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"And the choices for linsolve are:","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":":qr\n:cholesky\n:lsmr a conjugate gradient method (LSMR with diagonal preconditioner).","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/#MINPACK.jl","page":"Nonlinear Least Squares Solvers","title":"MINPACK.jl","text":"","category":"section"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"MINPACK.jl methods are fine for medium-sized nonlinear solves. They are the FORTRAN standard methods which are used in many places, such as SciPy. However, our benchmarks demonstrate that these methods are not robust or stable. In addition, they are slower than the standard methods and do not scale due to lack of sparse Jacobian support. Thus they are only recommended for benchmarking and testing code conversions.","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"CMINPACK(): A wrapper for using the classic MINPACK method through MINPACK.jl","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"Submethod choices for this algorithm include:","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":":hybr: Modified version of Powell's algorithm.\n:lm: Levenberg-Marquardt.\n:lmdif: Advanced Levenberg-Marquardt\n:hybrd: Advanced modified version of Powell's algorithm","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/#SciPy-(Python-via-PythonCall)","page":"Nonlinear Least Squares Solvers","title":"SciPy (Python via PythonCall)","text":"","category":"section"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"A wrapper over scipy.optimize.least_squares.  Requires that the Python package scipy is available to PythonCall.","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"SciPyLeastSquares() with convenience constructors SciPyLeastSquaresTRF(), SciPyLeastSquaresDogbox(), and SciPyLeastSquaresLM() for the common method choices.","category":"page"},{"location":"solvers/nonlinear_least_squares_solvers/#Optimization.jl","page":"Nonlinear Least Squares Solvers","title":"Optimization.jl","text":"","category":"section"},{"location":"solvers/nonlinear_least_squares_solvers/","page":"Nonlinear Least Squares Solvers","title":"Nonlinear Least Squares Solvers","text":"NonlinearLeastSquaresProblems can be converted into an OptimizationProblem  and used with any solver of Optimization.jl.","category":"page"},{"location":"devdocs/linear_solve/#Linear-Solve","page":"Linear Solve","title":"Linear Solve","text":"","category":"section"},{"location":"devdocs/linear_solve/#NonlinearSolveBase.AbstractLinearSolverCache","page":"Linear Solve","title":"NonlinearSolveBase.AbstractLinearSolverCache","text":"AbstractLinearSolverCache\n\nAbstract Type for all Linear Solvers used in NonlinearSolveBase. Subtypes of these are meant to be constructured via construct_linear_solver.\n\n\n\n\n\n","category":"type"},{"location":"devdocs/linear_solve/#NonlinearSolveBase.construct_linear_solver","page":"Linear Solve","title":"NonlinearSolveBase.construct_linear_solver","text":"construct_linear_solver(alg, linsolve, A, b, u; stats, kwargs...)\n\nConstruct a cache for solving linear systems of the form A * u = b. Following cases are handled:\n\nA is Number, then we solve it with u = b / A\nA is SMatrix, then we solve it with u = A \\ b (using the defaults from base Julia) (unless a preconditioner is specified)\nIf linsolve is \\, then we solve it with directly using ldiv!(u, A, b)\nIn all other cases, we use alg to solve the linear system using LinearSolve.jl\n\nSolving the System\n\n(cache::LinearSolverCache)(;\n    A = nothing, b = nothing, linu = nothing, reuse_A_if_factorization = false, kwargs...\n)\n\nReturns the solution of the system u and stores the updated cache in cache.lincache.\n\nSpecial Handling for Rank-deficient Matrix A\n\nIf we detect a failure in the linear solve (mostly due to using an algorithm that doesn't support rank-deficient matrices), we emit a warning and attempt to solve the problem using Pivoted QR factorization. This is quite efficient if there are only a few rank-deficient that originate in the problem. However, if these are quite frequent for the main nonlinear system, then it is recommended to use a different linear solver that supports rank-deficient matrices.\n\nKeyword Arguments\n\nreuse_A_if_factorization: If true, then the factorization of A is reused if possible. This is useful when solving the same system with different b values. If the algorithm is an iterative solver, then we reset the internal linear solve cache.\n\nOne distinct feature of this compared to the cache from LinearSolve is that it respects the aliasing arguments even after cache construction, i.e., if we passed in an A that A is not mutated, we do this by copying over A to a preconstructed cache.\n\n\n\n\n\n","category":"function"},{"location":"api/fastlevenbergmarquardt/#FastLevenbergMarquardt.jl","page":"FastLevenbergMarquardt.jl","title":"FastLevenbergMarquardt.jl","text":"","category":"section"},{"location":"api/fastlevenbergmarquardt/","page":"FastLevenbergMarquardt.jl","title":"FastLevenbergMarquardt.jl","text":"This is an extension for importing solvers from FastLevenbergMarquardt.jl into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/fastlevenbergmarquardt/","page":"FastLevenbergMarquardt.jl","title":"FastLevenbergMarquardt.jl","text":"import Pkg\nPkg.add(\"FastLevenbergMarquardt\")\nimport FastLevenbergMarquardt\nimport NonlinearSolve as NLS","category":"page"},{"location":"api/fastlevenbergmarquardt/#Solver-API","page":"FastLevenbergMarquardt.jl","title":"Solver API","text":"","category":"section"},{"location":"api/fastlevenbergmarquardt/#NonlinearSolve.FastLevenbergMarquardtJL","page":"FastLevenbergMarquardt.jl","title":"NonlinearSolve.FastLevenbergMarquardtJL","text":"FastLevenbergMarquardtJL(\n    linsolve::Symbol = :cholesky;\n    factor = 1e-6, factoraccept = 13.0, factorreject = 3.0, factorupdate = :marquardt,\n    minscale = 1e-12, maxscale = 1e16, minfactor = 1e-28, maxfactor = 1e32,\n    autodiff = nothing\n)\n\nWrapper over FastLevenbergMarquardt.jl for solving NonlinearLeastSquaresProblem. For details about the other keyword arguments see the documentation for FastLevenbergMarquardt.jl.\n\nArguments\n\nlinsolve: Linear solver to use. Can be :qr or :cholesky.\n\nKeyword Arguments\n\nautodiff: determines the backend used for the Jacobian. Note that this argument is ignored if an analytical Jacobian is passed, as that will be used instead. Defaults to nothing which means that a default is selected according to the problem specification!\n\nnote: Note\nThis algorithm is only available if FastLevenbergMarquardt.jl is installed and loaded.\n\n\n\n\n\n","category":"type"},{"location":"release_notes/#Release-Notes","page":"Release Notes","title":"Release Notes","text":"","category":"section"},{"location":"release_notes/#Oct-'24","page":"Release Notes","title":"Oct '24","text":"","category":"section"},{"location":"release_notes/#Breaking-Changes-in-NonlinearSolve.jl-v4","page":"Release Notes","title":"Breaking Changes in NonlinearSolve.jl v4","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"ApproximateJacobianSolveAlgorithm has been renamed to QuasiNewtonAlgorithm.\nPreconditioners for the linear solver needs to be passed with the precs keyword argument to the linear solver instead of to the nonlinear solver.\nSee common breaking changes below.","category":"page"},{"location":"release_notes/#Breaking-Changes-in-SimpleNonlinearSolve.jl-v2","page":"Release Notes","title":"Breaking Changes in SimpleNonlinearSolve.jl v2","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"See common breaking changes below.","category":"page"},{"location":"release_notes/#common-breaking-changes-v4v2","page":"Release Notes","title":"Common Breaking Changes","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Use of termination conditions from DiffEqBase has been removed. Use the termination conditions from NonlinearSolveBase instead.\nIf no autodiff is provided, we now choose from a list of autodiffs based on the packages loaded. For example, if Enzyme is loaded, we will default to that (for reverse mode). In general, we don't guarantee the exact autodiff selected if autodiff is not provided (i.e. nothing).","category":"page"},{"location":"release_notes/#Dec-'23","page":"Release Notes","title":"Dec '23","text":"","category":"section"},{"location":"release_notes/#Breaking-Changes-in-NonlinearSolve.jl-v3","page":"Release Notes","title":"Breaking Changes in NonlinearSolve.jl v3","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"GeneralBroyden and GeneralKlement have been renamed to Broyden and Klement respectively.\nCompat for SimpleNonlinearSolve has been bumped to v1.\nThe old style of specifying autodiff with chunksize, standardtag, etc. has been deprecated in favor of directly specifying the autodiff type, like AutoForwardDiff.","category":"page"},{"location":"release_notes/#Breaking-Changes-in-SimpleNonlinearSolve.jl-v1","page":"Release Notes","title":"Breaking Changes in SimpleNonlinearSolve.jl v1","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Batched solvers have been removed in favor of BatchedArrays.jl. Stay tuned for detailed tutorials on how to use BatchedArrays.jl with NonlinearSolve & SimpleNonlinearSolve solvers.\nThe old style of specifying autodiff with chunksize, standardtag, etc. has been deprecated in favor of directly specifying the autodiff type, like AutoForwardDiff.\nBroyden and Klement have been renamed to SimpleBroyden and SimpleKlement to avoid conflicts with NonlinearSolve.jl's GeneralBroyden and GeneralKlement, which will be renamed to Broyden and Klement in the future.\nLBroyden has been renamed to SimpleLimitedMemoryBroyden to make it consistent with NonlinearSolve.jl's LimitedMemoryBroyden.","category":"page"},{"location":"api/homotopycontinuation/#HomotopyContinuation.jl","page":"HomotopyContinuation.jl","title":"HomotopyContinuation.jl","text":"","category":"section"},{"location":"api/homotopycontinuation/","page":"HomotopyContinuation.jl","title":"HomotopyContinuation.jl","text":"NonlinearSolve wraps the homotopy continuation algorithm implemented in HomotopyContinuation.jl. This solver is not included by default and needs to be installed separately:","category":"page"},{"location":"api/homotopycontinuation/","page":"HomotopyContinuation.jl","title":"HomotopyContinuation.jl","text":"import Pkg\nPkg.add(\"NonlinearSolveHomotopyContinuation\")\nimport NonlinearSolveHomotopyContinuation\nimport NonlinearSolve as NLS","category":"page"},{"location":"api/homotopycontinuation/#Solver-API","page":"HomotopyContinuation.jl","title":"Solver API","text":"","category":"section"},{"location":"api/homotopycontinuation/#NonlinearSolveHomotopyContinuation.HomotopyContinuationJL","page":"HomotopyContinuation.jl","title":"NonlinearSolveHomotopyContinuation.HomotopyContinuationJL","text":"HomotopyContinuationJL{AllRoots}(; autodiff = true, kwargs...)\nHomotopyContinuationJL(; kwargs...) = HomotopyContinuationJL{false}(; kwargs...)\n\nThis algorithm is an interface to HomotopyContinuation.jl. It is only valid for fully determined polynomial systems. The AllRoots type parameter can be true or false and controls whether the solver will find all roots of the polynomial or a single root close to the initial guess provided to the NonlinearProblem. The polynomial function must allow complex numbers to be provided as the state.\n\nIf AllRoots is true, the initial guess in the NonlinearProblem is ignored. The function must be traceable using HomotopyContinuation.jl's symbolic variables. Note that higher degree polynomials and systems with multiple unknowns can increase solve time significantly.\n\nIf AllRoots is false, a single path is traced during the homotopy. The traced path depends on the initial guess provided to the NonlinearProblem being solved. This method does not require that the polynomial function is traceable via HomotopyContinuation.jl's symbolic variables.\n\nHomotopyContinuation.jl requires the jacobian of the system. In case a jacobian function is provided, it will be used. Otherwise, the autodiff keyword argument controls the autodiff method used to compute the jacobian. A value of true refers to AutoForwardDiff and false refers to AutoFiniteDiff. Alternate algorithms can be specified using ADTypes.jl.\n\nHomotopyContinuation.jl requires the taylor series of the polynomial system for the single root method. This is automatically computed using TaylorSeries.jl.\n\n\n\n\n\n","category":"type"},{"location":"api/homotopycontinuation/#SciMLBase.HomotopyNonlinearFunction","page":"HomotopyContinuation.jl","title":"SciMLBase.HomotopyNonlinearFunction","text":"struct HomotopyNonlinearFunction{iip, specialize, F, P, Q, D} <: SciMLBase.AbstractNonlinearFunction{iip}\n\nA representation of a polynomial nonlinear system of equations given by\n\n0 = f(polynomialize(u p) p)\n\nDesigned to be used for solving with HomotopyContinuation.\n\nConstructor\n\nNote that only the function f is required. This function should be given as f!(du,u,p) or du = f(u,p). See the section on iip for more details on in-place vs out-of-place handling.\n\nTo provide the Jacobian function etc, f must be a NonlinearFunction with the required fields populated.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the HomotopyNonlinearFunction type directly match the names of the inputs.\n\nf: The polynomial function f. Stored as a NonlinearFunction{iip, specialize}. If not provided to the constructor as a NonlinearFunction, it will be appropriately wrapped.\n\npolynomialize: The nonlinear system may be a polynomial of non-polynomial functions. For example,\nsin^2(x^2) + 2sin(x^2) + 1 = 0\nlog(x + y) ^ 3 = 05\nis a polynomial in [sin(x^2), log(x + y)]. To allow for such systems, polynomialize converts the state vector of the original system (termed as an \"original space\" vector) to the state vector of the polynomial system (termed as a \"polynomial space\" vector). In the above example, it will be the function:\nfunction polynomialize(u, p)\n  x, y = u\n  return [sin(x^2), log(x + y)]\nend\nWe refer to [x, y] as being in \"original space\" and [sin(x^2), log(x + y)] as being in \"polynomial space\".\nThis defaults to a function which returns u as-is.\n\nunpolynomialize: The inverse operation of polynomialize.\nThis function takes as input a vector u‚Ä≤ in \"polynomial space\" and returns a collection of vectors u·µ¢ ‚àÄ i ‚àà {1..n} in \"original space\" such that polynomialize(u·µ¢, p) == u‚Ä≤ ‚àÄ i. A collection is returned since the mapping may not be bijective. For periodic functions which have infinite such vectors u·µ¢, it is up to the user to choose which ones to return.\nIn the aforementioned example in polynomialize, this function will be:\nfunction unpolynomialize(u, p)\n  a, b = u\n  return [\n    [sqrt(asin(a)), exp(b) - sqrt(asin(a))],\n    [-sqrt(asin(a)), exp(b) + sqrt(asin(a))],\n  ]\nend\nThere are of course an infinite number of such functions due to the presence of sin. This example chooses to return the roots in the interval [-œÄ/2, œÄ/2].\n\ndenominator: A function of the form denominator(u, p) -> denoms where denoms is an array of denominator terms which cannot be zero. This is useful when f represents the numerator of a rational function. Roots of f which cause any of the values in denoms to be below a threshold will be excluded. Note that here u must be in \"polynomial space\".\n\n\n\n\n\n","category":"type"},{"location":"api/petsc/#PETSc.jl","page":"PETSc.jl","title":"PETSc.jl","text":"","category":"section"},{"location":"api/petsc/","page":"PETSc.jl","title":"PETSc.jl","text":"This is a extension for importing solvers from PETSc.jl SNES into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/petsc/","page":"PETSc.jl","title":"PETSc.jl","text":"import Pkg\nPkg.add(\"PETSc\")\nimport PETSc\nimport NonlinearSolve as NLS","category":"page"},{"location":"api/petsc/#Solver-API","page":"PETSc.jl","title":"Solver API","text":"","category":"section"},{"location":"api/petsc/#NonlinearSolve.PETScSNES","page":"PETSc.jl","title":"NonlinearSolve.PETScSNES","text":"PETScSNES(; petsclib = missing, autodiff = nothing, mpi_comm = missing, kwargs...)\n\nWrapper over PETSc.jl SNES solvers.\n\nKeyword Arguments\n\npetsclib: PETSc library to use. If set to missing, then we will use the first available PETSc library in PETSc.petsclibs based on the problem element type.\nautodiff: the choice of method for generating the Jacobian. Defaults to nothing which means that a default is selected according to the problem specification. Can be any valid ADTypes.jl autodiff type (conditional on that backend being supported in NonlinearSolve.jl). If set to missing, then PETSc computes the Jacobian using finite differences.\nmpi_comm: MPI communicator to use. If set to missing, then we will use MPI.COMM_SELF.\nkwargs: Keyword arguments to be passed to the PETSc SNES solver. See PETSc SNES documentation and SNESSetFromOptions for more information.\n\nOptions via CommonSolve.solve\n\nThese options are forwarded from solve to the PETSc SNES solver. If these are provided to kwargs, then they will be ignored.\n\nsolve option PETSc SNES option\natol snes_atol\nrtol snes_rtol\nmaxiters snes_max_it\nshow_trace snes_monitor\n\nnote: Note\nThis algorithm is only available if PETSc.jl is installed and loaded.\n\n\n\n\n\n","category":"type"},{"location":"solvers/nonlinear_system_solvers/#nonlinearsystemsolvers","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"solve(prob::NonlinearProblem, alg; kwargs...)","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"Solves for f(u) = 0 in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/nonlinear_system_solvers/#Recommended-Methods","page":"Nonlinear System Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"The default method FastShortcutNonlinearPolyalg is a good choice for most problems. It is a polyalgorithm that attempts to use a fast algorithm (Klement, Broyden) and if that fails it falls back to a more robust algorithm (NewtonRaphson) before falling back the most robust variant of TrustRegion. For basic problems this will be very fast, for harder problems it will make sure to work.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"If one is looking for more robustness then RobustMultiNewton is a good choice. It attempts a set of the most robust methods in succession and only fails if all of the methods fail to converge. Additionally, DynamicSS can be a good choice for high stability if the root corresponds to a stable equilibrium.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"As a balance, NewtonRaphson is a good choice for most problems that aren't too difficult yet need high performance, and  TrustRegion is a bit less performant but more stable. If the problem is well-conditioned, Klement or Broyden may be faster, but highly dependent on the eigenvalues of the Jacobian being sufficiently small.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"NewtonRaphson and TrustRegion are designed for for large systems. They can make use of sparsity patterns for sparse automatic differentiation and sparse linear solving of very large systems. Meanwhile, SimpleNewtonRaphson and SimpleTrustRegion are implementations which are specialized for small equations. They are non-allocating on static arrays and thus really well-optimized for small systems, thus usually outperforming the other methods when such types are used for u0. Additionally, these solvers can be used inside GPU kernels. See ParallelParticleSwarms.jl for an example of this.","category":"page"},{"location":"solvers/nonlinear_system_solvers/#Full-List-of-Methods","page":"Nonlinear System Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"note: Note\nFor the full details on the capabilities and constructors of the different solvers, see the Detailed Solver APIs section!","category":"page"},{"location":"solvers/nonlinear_system_solvers/#NonlinearSolve.jl","page":"Nonlinear System Solvers","title":"NonlinearSolve.jl","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"These are the core solvers, which excel at large-scale problems that need advanced linear solver, automatic differentiation, abstract array types, GPU, sparse/structured matrix support, etc. These methods support the largest set of types and features, but have a bit of overhead on very small problems.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"NewtonRaphson(): A Newton-Raphson method with swappable nonlinear solvers and autodiff methods for high performance on large and sparse systems.\nTrustRegion(): A Newton Trust Region dogleg method with swappable nonlinear solvers and autodiff methods for high performance on large and sparse systems.\nLevenbergMarquardt(): An advanced Levenberg-Marquardt implementation with the improvements suggested in the Transtrum and Sethna [1]. Designed for large-scale and numerically-difficult nonlinear systems.\nPseudoTransient(): A pseudo-transient method which mixes the stability of Euler-type stepping with the convergence speed of a Newton method. Good for highly unstable systems.\nRobustMultiNewton(): A polyalgorithm that mixes highly robust methods (line searches and trust regions) in order to be as robust as possible for difficult problems. If this method fails to converge, then one can be pretty certain that most (all?) other choices would likely fail.\nFastShortcutNonlinearPolyalg(): The default method. A polyalgorithm that mixes fast methods with fallbacks to robust methods to allow for solving easy problems quickly without sacrificing robustness on the hard problems.\nBroyden(): Generalization of Broyden's Quasi-Newton Method with Line Search and Automatic Jacobian Resetting. This is a fast method but unstable when the condition number of the Jacobian matrix is sufficiently large.\nKlement(): Generalization of Klement's Quasi-Newton Method with Line Search and Automatic Jacobian Resetting. This is a fast method but unstable when the condition number of the Jacobian matrix is sufficiently large.\nLimitedMemoryBroyden(): An advanced version of SimpleLimitedMemoryBroyden which uses a limited memory Broyden method. This is a fast method but unstable when the condition number of the Jacobian matrix is sufficiently large. It is recommended to use Broyden or Klement instead unless the memory usage is a concern.","category":"page"},{"location":"solvers/nonlinear_system_solvers/#SimpleNonlinearSolve.jl","page":"Nonlinear System Solvers","title":"SimpleNonlinearSolve.jl","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"These methods are included with NonlinearSolve.jl by default, though SimpleNonlinearSolve.jl can be used directly to reduce dependencies and improve load times. SimpleNonlinearSolve.jl's methods excel at small problems and problems defined with static arrays.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"SimpleNewtonRaphson(): A simplified implementation of the Newton-Raphson method.\nSimpleBroyden(): The classic Broyden's quasi-Newton method.\nSimpleLimitedMemoryBroyden(): A low-memory Broyden implementation, similar to L-BFGS. This method is common in machine learning contexts but is known to be unstable in comparison to many other choices.\nSimpleKlement(): A quasi-Newton method due to Klement. It's supposed to be more efficient than Broyden's method, and it seems to be in the cases that have been tried, but more benchmarking is required.\nSimpleTrustRegion(): A dogleg trust-region Newton method. Improved globalizing stability for more robust fitting over basic Newton methods, though potentially with a cost.\nSimpleDFSane(): A low-overhead implementation of the df-sane method for solving large-scale nonlinear systems of equations.\nSimpleHalley(): A low-overhead implementation of the Halley method. This is a higher order method and thus can converge faster to low tolerances than a Newton method. Requires higher order derivatives, so best used when automatic differentiation is available.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"note: Note\nWhen used with certain types for the states u such as a Number or StaticArray, these solvers are very efficient and non-allocating. These implementations are thus well-suited for small systems of equations.","category":"page"},{"location":"solvers/nonlinear_system_solvers/#SteadyStateDiffEq.jl","page":"Nonlinear System Solvers","title":"SteadyStateDiffEq.jl","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"SteadyStateDiffEq.jl uses ODE solvers to iteratively approach the steady state. It is a very stable method for solving nonlinear systems with stable equilibrium points, though often more computationally expensive than direct methods.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"DynamicSS(): Uses an ODE solver to find the steady state. Automatically terminates when close to the steady state.\nSSRootfind(): Uses a NonlinearSolve compatible solver to find the steady state.","category":"page"},{"location":"solvers/nonlinear_system_solvers/#NLsolve.jl","page":"Nonlinear System Solvers","title":"NLsolve.jl","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"This is a wrapper package for importing solvers from NLsolve.jl into the SciML interface.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"NLsolveJL(): A wrapper for NLsolve.jl","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"Submethod choices for this algorithm include:","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":":anderson: Anderson-accelerated fixed-point iteration\n:newton: Classical Newton method with an optional line search\n:trust_region: Trust region Newton method (the default choice)","category":"page"},{"location":"solvers/nonlinear_system_solvers/#MINPACK.jl","page":"Nonlinear System Solvers","title":"MINPACK.jl","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"MINPACK.jl is a wrapper package for bringing the Fortran solvers from MINPACK. However, our benchmarks reveal that these methods are rarely competitive with our native solvers. Thus, our recommendation is to use these only for benchmarking and debugging purposes.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"CMINPACK(): A wrapper for using the classic MINPACK method through MINPACK.jl","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"Submethod choices for this algorithm include:","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":":hybr: Modified version of Powell's algorithm.\n:lm: Levenberg-Marquardt.\n:lmdif: Advanced Levenberg-Marquardt\n:hybrd: Advanced modified version of Powell's algorithm","category":"page"},{"location":"solvers/nonlinear_system_solvers/#Sundials.jl","page":"Nonlinear System Solvers","title":"Sundials.jl","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"Sundials.jl are a classic set of C/Fortran methods which are known for good scaling of the Newton-Krylov form. However, KINSOL is known to be less stable than some other implementations.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"KINSOL(): The KINSOL method of the SUNDIALS C library","category":"page"},{"location":"solvers/nonlinear_system_solvers/#SIAMFANLEquations.jl","page":"Nonlinear System Solvers","title":"SIAMFANLEquations.jl","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"SIAMFANLEquations.jl is a wrapper for the methods in the SIAMFANLEquations.jl library.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"SIAMFANLEquationsJL(): A wrapper for using the methods in SIAMFANLEquations.jl","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"Other solvers listed in Fixed Point Solvers, FastLevenbergMarquardt.jl and LeastSquaresOptim.jl can also solve nonlinear systems.","category":"page"},{"location":"solvers/nonlinear_system_solvers/#NLSolvers.jl","page":"Nonlinear System Solvers","title":"NLSolvers.jl","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"This is a wrapper package for importing solvers from NLSolvers.jl into the SciML interface.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"NLSolversJL(): A wrapper for NLSolvers.jl","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"For a list of possible solvers see the NLSolvers.jl documentation","category":"page"},{"location":"solvers/nonlinear_system_solvers/#PETSc.jl","page":"Nonlinear System Solvers","title":"PETSc.jl","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"This is a wrapper package for importing solvers from PETSc.jl into the SciML interface.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"PETScSNES(): A wrapper for PETSc.jl","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"For a list of possible solvers see the PETSc.jl documentation","category":"page"},{"location":"solvers/nonlinear_system_solvers/#SciPy-(Python-via-PythonCall)","page":"Nonlinear System Solvers","title":"SciPy (Python via PythonCall)","text":"","category":"section"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"These wrappers let you use the algorithms from scipy.optimize without leaving Julia.  SciPy is loaded lazily through PythonCall, so these methods are available whenever the scipy Python package can be imported.","category":"page"},{"location":"solvers/nonlinear_system_solvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"SciPyRoot(): wrapper for scipy.optimize.root (vector problems)\nSciPyRootScalar(): wrapper for scipy.optimize.root_scalar (scalar/bracketed problems)","category":"page"},{"location":"tutorials/getting_started/#Getting-Started-with-Nonlinear-Rootfinding-in-Julia","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"","category":"section"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"NonlinearSolve.jl is a system for solving rootfinding problems, i.e. finding the value u such that f(u) = 0. In this tutorial we will go through the basics of NonlinearSolve.jl, demonstrating the core ideas and leading you to understanding the deeper parts of the documentation.","category":"page"},{"location":"tutorials/getting_started/#The-Four-Types-of-Nonlinear-Systems","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"The Four Types of Nonlinear Systems","text":"","category":"section"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"There are four types of nonlinear systems:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"The \"standard nonlinear system\", i.e. the NonlinearProblem. This is a system of equations with an initial condition where you want to satisfy all equations simultaneously.\nThe \"interval rootfinding problem\", i.e. the IntervalNonlinearProblem. This is the case where you're given an interval [a,b] and need to find where f(u) = 0 for u inside the bounds.\nThe \"steady state problem\", i.e. find the u such that u' = f(u) = 0. While related to (1), it's not entirely the same because there's a uniquely defined privileged root.\nThe nonlinear least squares problem, which is an under/over-constrained nonlinear system which might not be satisfiable, i.e. there may be no u such that f(u) = 0, and thus we find the u which minimizes ||f(u)|| in the least squares sense.","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"One important distinction is that (1) and (3) require the input and output sizes to be the same, while (4) does not.","category":"page"},{"location":"tutorials/getting_started/#Problem-Type-1:-Solving-Nonlinear-Systems-of-Equations","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Problem Type 1: Solving Nonlinear Systems of Equations","text":"","category":"section"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"A nonlinear system f(u) = 0 is specified by defining a function f(u,p), where p are the parameters of the system. For example, the following solves the vector equation f(u) = u^2 - p for a vector of equations:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"import NonlinearSolve as NLS\n\nf(u, p) = u .* u .- p\nu0 = [1.0, 1.0]\np = 2.0\nprob = NLS.NonlinearProblem(f, u0, p)\nsol = NLS.solve(prob)","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"where u0 is the initial condition for the rootfinder. Native NonlinearSolve.jl solvers use the given type of u0 to determine the type used within the solver and the return. Note that the parameters p can be any type, but most are an AbstractArray for automatic differentiation.","category":"page"},{"location":"tutorials/getting_started/#Investigating-the-Solution","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Investigating the Solution","text":"","category":"section"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"To investigate the solution, one can look at the elements of the NonlinearSolution. The most important value is sol.u: this is the u that satisfies f(u) = 0. For example:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"u = sol.u","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"f(u, p)","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"This final value, the difference of the solution against zero, can also be found with sol.resid:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"sol.resid","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"To know if the solution converged, or why the solution had not converged we can check the return code (retcode):","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"sol.retcode","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"There are multiple return codes which can mean the solve was successful, and thus we can use the general command SciMLBase.successful_retcode to check whether the solution process exited as intended:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"import SciMLBase\nSciMLBase.successful_retcode(sol)","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"If we're curious about what it took to solve this equation, then you're in luck because all of the details can be found in sol.stats:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"sol.stats","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"For more information on NonlinearSolutions, see the NonlinearSolution manual page.","category":"page"},{"location":"tutorials/getting_started/#Interacting-with-the-Solver-Options","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Interacting with the Solver Options","text":"","category":"section"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"While sol = NonlinearSolve.solve(prob) worked for our case here, in many situations you may need to interact more deeply with how the solving process is done. First things first, you can specify the solver using the positional arguments. For example, let's set the solver to TrustRegion():","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"NLS.solve(prob, NLS.TrustRegion())","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"For a complete list of solver choices, see the nonlinear system solvers page.","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"Next we can modify the tolerances. Here let's set some really low tolerances to force a tight solution:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"NLS.solve(prob, NLS.TrustRegion(), reltol = 1e-12, abstol = 1e-12)","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"There are many more options for doing this configuring. Specifically for handling termination conditions, see the Termination Conditions page for more details. And for more details on all of the available keyword arguments, see the solver options page.","category":"page"},{"location":"tutorials/getting_started/#Problem-Type-2:-Solving-Interval-Rootfinding-Problems-with-Bracketing-Methods","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Problem Type 2: Solving Interval Rootfinding Problems with Bracketing Methods","text":"","category":"section"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"For scalar rootfinding problems, bracketing methods exist in NonlinearSolve. The difference with bracketing methods is that with bracketing methods, instead of giving a u0 initial condition, you pass a uspan (a,b) bracket in which the zero is expected to live. For example:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"import NonlinearSolve as NLS\nf(u, p) = u * u - 2.0\nuspan = (1.0, 2.0) # brackets\nprob_int = NLS.IntervalNonlinearProblem(f, uspan)\nsol = NLS.solve(prob_int)","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"All of the same option handling from before works just as before, now just with different solver choices (see the bracketing solvers page for more details). For example, let's set the solver to ITP() and set a high absolute tolerance:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"sol = NLS.solve(prob_int, NLS.ITP(), abstol = 0.01)","category":"page"},{"location":"tutorials/getting_started/#Problem-Type-3:-Solving-Steady-State-Problems","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Problem Type 3: Solving Steady State Problems","text":"","category":"section"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"For Steady State Problems, we have a wrapper package SteadyStateDiffEq.jl. This package automates handling SteadyStateProblems with NonlinearSolve and OrdinaryDiffEq.","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"import NonlinearSolve as NLS\nimport SteadyStateDiffEq as SSDE\n\nf(u, p, t) = [2 - 2u[1]; u[1] - 4u[2]]\nu0 = [0.0, 0.0]\nprob = SSDE.SteadyStateProblem(f, u0)\n\nNLS.solve(prob, SSDE.SSRootfind())","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"If you don't provide a nonlinear solver to SSRootfind it uses a polyalgorithm from NonlinearSolve. We can also provide the actual nonlinear solver to use:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"NLS.solve(prob, SSDE.SSRootfind(NLS.Broyden()))","category":"page"},{"location":"tutorials/getting_started/#Problem-Type-4:-Solving-Nonlinear-Least-Squares-Problems","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Problem Type 4: Solving Nonlinear Least Squares Problems","text":"","category":"section"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"import NonlinearSolve as NLS\n\nfunction nlls!(du, u, p)\n    du[1] = 2u[1] - 2\n    du[2] = u[1] - 4u[2]\n    du[3] = 0\nend","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"Note that here the output array is of length 3 while the input array is of length 2. We need to provide the resid_prototype to tell the solver what the output size is (this can be skipped for out of place problems):","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"u0 = [0.0, 0.0]\nprob = NLS.NonlinearLeastSquaresProblem(\n    NLS.NonlinearFunction(nlls!, resid_prototype = zeros(3)), u0)\n\nNLS.solve(prob)","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"Same as before, we can change the solver and tolerances:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"NLS.solve(prob, NLS.GaussNewton(), reltol = 1e-12, abstol = 1e-12)","category":"page"},{"location":"tutorials/getting_started/#Going-Beyond-the-Basics:-How-to-use-the-Documentation","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Going Beyond the Basics: How to use the Documentation","text":"","category":"section"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"Congrats, you now know how to use the basics of NonlinearSolve.jl! However, there is so much more to see. Next check out:","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"Some code optimization tricks to know about with NonlinearSolve.jl\nAn iterator interface which lets you step through the solving process step by step\nHow to handle large systems of equations efficiently\nWays to use NonlinearSolve.jl that is faster to startup and can statically compile\nMore detailed termination conditions","category":"page"},{"location":"tutorials/getting_started/","page":"Getting Started with Nonlinear Rootfinding in Julia","title":"Getting Started with Nonlinear Rootfinding in Julia","text":"And also check out the rest of the manual.","category":"page"},{"location":"#NonlinearSolve.jl:-High-Performance-Unified-Nonlinear-Solvers","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"","category":"section"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"NonlinearSolve.jl is a unified interface for the nonlinear solving packages of Julia. The package includes its own high-performance nonlinear solvers which include the ability to swap out to fast direct and iterative linear solvers, along with the ability to use sparse automatic differentiation for Jacobian construction and Jacobian-vector products. NonlinearSolve.jl interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping. It also interfaces with the ModelingToolkit.jl world of symbolic modeling to allow for automatically generating high-performance code.","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"Performance is key: the current methods are made to be highly performant on scalar and statically sized small problems, with options for large-scale systems. If you run into any performance issues, please file an issue. Consult the NonlinearSystemSolvers page for information on how to import solvers from different packages.","category":"page"},{"location":"#Installation","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"Installation","text":"","category":"section"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"To install NonlinearSolve.jl, use the Julia package manager:","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"using Pkg\nPkg.add(\"NonlinearSolve\")","category":"page"},{"location":"#Contributing","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"Contributing","text":"","category":"section"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Reproducibility","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"</details>","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"</details>","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"</details>","category":"page"},{"location":"","page":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" *\n                name *\n                \".jl/tree/gh-pages/v\" *\n                version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" *\n               name *\n               \".jl/tree/gh-pages/v\" *\n               version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"basics/diagnostics_api/#diagnostics_api","page":"Diagnostics API","title":"Diagnostics API","text":"","category":"section"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"Detailed API Documentation is provided at Diagnostics API Reference.","category":"page"},{"location":"basics/diagnostics_api/#Logging-the-Solve-Process","page":"Diagnostics API","title":"Logging the Solve Process","text":"","category":"section"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"All NonlinearSolve.jl native solvers allow storing and displaying the trace of the nonlinear solve process. This is controlled by 3 keyword arguments to solve:","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"show_trace: Must be Val(true) or Val(false). This controls whether the trace is displayed to the console. (Defaults to Val(false))\ntrace_level: Needs to be one of Trace Objects: TraceMinimal, TraceWithJacobianConditionNumber, or TraceAll. This controls the level of detail of the trace. (Defaults to TraceMinimal())\nstore_trace: Must be Val(true) or Val(false). This controls whether the trace is stored in the solution object. (Defaults to Val(false))","category":"page"},{"location":"basics/diagnostics_api/#Detailed-Internal-Timings","page":"Diagnostics API","title":"Detailed Internal Timings","text":"","category":"section"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"All the native NonlinearSolve.jl algorithms come with in-built TimerOutputs.jl support. However, this is disabled by default and can be enabled via NonlinearSolveBase.enable_timer_outputs.","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"Note that you will have to restart Julia to disable the timer outputs once enabled.","category":"page"},{"location":"basics/diagnostics_api/#Example-Usage","page":"Diagnostics API","title":"Example Usage","text":"","category":"section"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"import NonlinearSolve as NLS\n\nfunction nlfunc(resid, u0, p)\n    resid[1] = u0[1] * u0[1] - p\n    resid[2] = u0[2] * u0[2] - p\n    resid[3] = u0[3] * u0[3] - p\n    nothing\nend\n\nprob = NLS.NonlinearProblem(nlfunc, [1.0, 3.0, 5.0], 2.0)\n\nNLS.solve(prob)","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"This produced the output, but it is hard to diagnose what is going on. We can turn on the trace to see what is happening:","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"NLS.solve(prob; show_trace = Val(true), trace_level = NLS.TraceAll(10))\nnothing; # hide","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"You can also store the trace in the solution object:","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"sol = NLS.solve(prob; trace_level = NLS.TraceAll(), store_trace = Val(true));\n\nsol.trace","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"Now, let's try to investigate the time it took for individual internal steps. We will have to use the init and solve! API for this. The TimerOutput will be present in cache.timer. However, note that for poly-algorithms this is currently not implemented.","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"cache = NLS.init(prob, NLS.NewtonRaphson(); show_trace = Val(true));\nNLS.solve!(cache)\ncache.timer","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"Let's try for some other solver:","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"cache = NLS.init(prob, NLS.DFSane(); show_trace = Val(true), trace_level = NLS.TraceMinimal(50));\nNLS.solve!(cache)\ncache.timer","category":"page"},{"location":"basics/diagnostics_api/","page":"Diagnostics API","title":"Diagnostics API","text":"note: Note\nFor iteration == 0 only the norm(fu, Inf) is guaranteed to be meaningful. The other values being meaningful are solver dependent.","category":"page"},{"location":"api/nlsolvers/#NLSolvers.jl","page":"NLSolvers.jl","title":"NLSolvers.jl","text":"","category":"section"},{"location":"api/nlsolvers/","page":"NLSolvers.jl","title":"NLSolvers.jl","text":"This is a extension for importing solvers from NLSolvers.jl into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/nlsolvers/","page":"NLSolvers.jl","title":"NLSolvers.jl","text":"import Pkg\nPkg.add(\"NLSolvers\")\nimport NLSolvers\nimport NonlinearSolve as NLS","category":"page"},{"location":"api/nlsolvers/#Solver-API","page":"NLSolvers.jl","title":"Solver API","text":"","category":"section"},{"location":"api/nlsolvers/#NonlinearSolve.NLSolversJL","page":"NLSolvers.jl","title":"NonlinearSolve.NLSolversJL","text":"NLSolversJL(method; autodiff = nothing)\nNLSolversJL(; method, autodiff = nothing)\n\nWrapper over NLSolvers.jl Nonlinear Equation Solvers. We automatically construct the jacobian function and supply it to the solver.\n\nArguments\n\nmethod: the choice of method for solving the nonlinear system. See the documentation for NLSolvers.jl for more information.\nautodiff: the choice of method for generating the Jacobian. Defaults to nothing which means that a default is selected according to the problem specification. Can be any valid ADTypes.jl autodiff type (conditional on that backend being supported in NonlinearSolve.jl).\n\n\n\n\n\n","category":"type"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"M.¬†K.¬†Transtrum and J.¬†P.¬†Sethna. Improvements to the Levenberg-Marquardt algorithm for nonlinear least-squares minimization, arXiv¬†preprint¬†arXiv:1201.5885 (2012).\n\n\n\nW.¬†La Cruz, J.¬†Martƒ±ÃÅnez and M.¬†Raydan. Spectral residual method without gradient information for solving large-scale nonlinear systems of equations. Mathematics¬†of¬†computation 75, 1429‚Äì1448 (2006).\n\n\n\nC.¬†G.¬†Broyden. A class of methods for solving nonlinear simultaneous equations. Mathematics¬†of¬†computation 19, 577‚Äì593 (1965).\n\n\n\nJ.¬†Klement. On using quasi-newton algorithms of the Broyden class for model-to-test correlation. Journal¬†of¬†Aerospace¬†Technology¬†and¬†Management 6, 407‚Äì414 (2014).\n\n\n\nM.¬†Ziani and F.¬†Guyomarc‚Äôh. An autoadaptative limited memory Broyden‚Äôs method to solve systems of nonlinear equations. Applied¬†mathematics¬†and¬†computation 205, 202‚Äì211 (2008).\n\n\n\nY.-x.¬†Yuan. Recent advances in trust region algorithms. Mathematical¬†Programming 151, 249‚Äì281 (2015).\n\n\n\nT.¬†S.¬†Coffey, C.¬†T.¬†Kelley and D.¬†E.¬†Keyes. Pseudotransient continuation and differential-algebraic equations. SIAM¬†Journal¬†on¬†Scientific¬†Computing 25, 553‚Äì569 (2003).\n\n\n\nC.¬†T.¬†Kelley and D.¬†E.¬†Keyes. Convergence analysis of pseudo-transient continuation. SIAM¬†Journal¬†on¬†Numerical¬†Analysis 35, 508‚Äì523 (1998).\n\n\n\nL.¬†Hei. A self-adaptive trust region algorithm. Journal¬†of¬†Computational¬†Mathematics, 229‚Äì236 (2003).\n\n\n\nF.¬†Bastin, V.¬†Malmedy, M.¬†Mouffe, P.¬†L.¬†Toint and D.¬†Tomanos. A retrospective trust-region method for unconstrained optimization. Mathematical¬†programming 123, 395‚Äì418 (2010).\n\n\n\nJ.¬†Fan. Convergence rate of the trust region method for nonlinear equations under local error bound condition. Computational¬†Optimization¬†and¬†Applications 34, 215‚Äì227 (2006).\n\n\n\nN.¬†Lepage-Saucier. Alternating cyclic extrapolation methods for optimization algorithms, arXiv¬†preprint¬†arXiv:2104.04974 (2021).\n\n\n\n","category":"page"},{"location":"basics/nonlinear_functions/#nonlinearfunctions","page":"Nonlinear Functions and Jacobian Types","title":"Nonlinear Functions and Jacobian Types","text":"","category":"section"},{"location":"basics/nonlinear_functions/","page":"Nonlinear Functions and Jacobian Types","title":"Nonlinear Functions and Jacobian Types","text":"The SciML ecosystem provides an extensive interface for declaring extra functions associated with the differential equation's data. In traditional libraries, there is usually only one option: the Jacobian. However, we allow for a large array of pre-computed functions to speed up the calculations. This is offered via the NonlinearFunction types, which can be passed to the problems.","category":"page"},{"location":"basics/nonlinear_functions/#Function-Type-Definitions","page":"Nonlinear Functions and Jacobian Types","title":"Function Type Definitions","text":"","category":"section"},{"location":"basics/nonlinear_functions/#SciMLBase.IntervalNonlinearFunction","page":"Nonlinear Functions and Jacobian Types","title":"SciMLBase.IntervalNonlinearFunction","text":"DocStringExtensions.TypeDefinition()\n\nA representation of an interval nonlinear system of equations f, defined by:\n\nf(tp) = u = 0\n\nand all of its related functions. For all cases, p are the parameters and t is the interval variable.\n\nConstructor\n\nIntervalNonlinearFunction{iip, specialize}(f;\n                           analytic = __has_analytic(f) ? f.analytic : nothing,\n                           sys = __has_sys(f) ? f.sys : nothing)\n\nNote that only the function f itself is required. This function should be given as f!(u,t,p) or u = f(t,p). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nanalytic(p): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the IntervalNonlinearFunction type directly match the names of the inputs.\n\n\n\n\n\n","category":"type"},{"location":"basics/nonlinear_functions/#SciMLBase.NonlinearFunction","page":"Nonlinear Functions and Jacobian Types","title":"SciMLBase.NonlinearFunction","text":"DocStringExtensions.TypeDefinition()\n\nA representation of a nonlinear system of equations f, defined by:\n\n0 = f(up)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nNonlinearFunction{iip, specialize}(f;\n                           analytic = __has_analytic(f) ? f.analytic : nothing,\n                           jac = __has_jac(f) ? f.jac : nothing,\n                           jvp = __has_jvp(f) ? f.jvp : nothing,\n                           vjp = __has_vjp(f) ? f.vjp : nothing,\n                           jac_prototype = __has_jac_prototype(f) ? f.jac_prototype : nothing,\n                           sparsity = __has_sparsity(f) ? f.sparsity : jac_prototype,\n                           paramjac = __has_paramjac(f) ? f.paramjac : nothing,\n                           colorvec = __has_colorvec(f) ? f.colorvec : nothing,\n                           sys = __has_sys(f) ? f.sys : nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,p) or du = f(u,p). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nanalytic(u0,p): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\njac(J,u,p) or J=jac(u,p): returns fracdfdu\njvp(Jv,v,u,p) or Jv=jvp(v,u,p): returns the directional derivative fracdfdu v\nvjp(Jv,v,u,p) or Jv=vjp(v,u,p): returns the adjoint derivative fracdfdu^ast v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p): returns the parameter Jacobian fracdfdp.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the NonlinearFunction type directly match the names of the inputs.\n\n\n\n\n\n","category":"type"},{"location":"native/bracketingnonlinearsolve/#BracketingNonlinearSolve.jl","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.jl","text":"","category":"section"},{"location":"native/bracketingnonlinearsolve/","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.jl","text":"These methods can be used independently of the rest of NonlinearSolve.jl","category":"page"},{"location":"native/bracketingnonlinearsolve/","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.jl","text":"Pages = [\"bracketingnonlinearsolve.md\"]","category":"page"},{"location":"native/bracketingnonlinearsolve/#Interval-Methods","page":"BracketingNonlinearSolve.jl","title":"Interval Methods","text":"","category":"section"},{"location":"native/bracketingnonlinearsolve/","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.jl","text":"These methods are suited for interval (scalar) root-finding problems, i.e. IntervalNonlinearProblem.","category":"page"},{"location":"native/bracketingnonlinearsolve/#BracketingNonlinearSolve.Alefeld","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.Alefeld","text":"Alefeld()\n\nAn implementation of algorithm 4.2 from Alefeld.\n\nThe paper brought up two new algorithms. Here choose to implement algorithm 4.2 rather than algorithm 4.1 because, in certain sense, the second algorithm(4.2) is an optimal procedure.\n\n\n\n\n\n","category":"type"},{"location":"native/bracketingnonlinearsolve/#BracketingNonlinearSolve.Bisection","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.Bisection","text":"Bisection(; exact_left = false, exact_right = false)\n\nA common bisection method.\n\nKeyword Arguments\n\nexact_left: whether to enforce whether the left side of the interval must be exactly zero for the returned result. Defaults to false.\nexact_right: whether to enforce whether the right side of the interval must be exactly zero for the returned result. Defaults to false.\n\ndanger: Keyword Arguments\nCurrently, the keyword arguments are not implemented.\n\n\n\n\n\n","category":"type"},{"location":"native/bracketingnonlinearsolve/#BracketingNonlinearSolve.Brent","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.Brent","text":"Brent()\n\nLeft non-allocating Brent method.\n\n\n\n\n\n","category":"type"},{"location":"native/bracketingnonlinearsolve/#BracketingNonlinearSolve.Falsi","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.Falsi","text":"Falsi()\n\nA non-allocating regula falsi method.\n\n\n\n\n\n","category":"type"},{"location":"native/bracketingnonlinearsolve/#BracketingNonlinearSolve.ITP","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.ITP","text":"ITP(; k1::Real = 0.007, k2::Real = 1.5, n0::Int = 10)\n\nITP (Interpolate Truncate & Project)\n\nUse the ITP method to find a root of a bracketed function, with a convergence rate between 1 and 1.62.\n\nThis method was introduced in the paper \"An Enhancement of the Bisection Method Average Performance Preserving Minmax Optimality\" (https://doi.org/10.1145/3423597) by I. F. D. Oliveira and R. H. C. Takahashi.\n\nTuning Parameters\n\nThe following keyword parameters are accepted.\n\nn‚ÇÄ::Int = 10, the 'slack'. Must not be negative. When n‚ÇÄ = 0 the worst-case is identical to that of bisection, but increasing n‚ÇÄ provides greater opportunity for superlinearity.\nscaled_Œ∫‚ÇÅ::Float64 = 0.2. Must not be negative. The recommended value is 0.2. Lower values produce tighter asymptotic behaviour, while higher values improve the steady-state behaviour when truncation is not helpful.\nŒ∫‚ÇÇ::Real = 2. Must lie in [1, 1+œï ‚âà 2.62). Higher values allow for a greater convergence rate, but also make the method more succeptable to worst-case performance. In practice, Œ∫‚ÇÇ=1, 2 seems to work well due to the computational simplicity, as Œ∫‚ÇÇ is used as an exponent in the method.\n\nComputation of Œ∫‚ÇÅ\n\nIn the current implementation, we compute Œ∫‚ÇÅ = scaled_Œ∫‚ÇÅ¬∑|Œîx‚ÇÄ|^(1 - Œ∫‚ÇÇ); this allows Œ∫‚ÇÅ to adapt to the length of the interval and keep the proposed steps proportional to Œîx.\n\nWorst Case Performance\n\nn¬Ω + n‚ÇÄ iterations, where n¬Ω is the number of iterations using bisection (n¬Ω = ‚åàlog2(Œîx)/2tol‚åâ).\n\nAsymptotic Performance\n\nIf f is twice differentiable and the root is simple, then with n‚ÇÄ > 0 the convergence rate is ‚àöŒ∫‚ÇÇ.\n\n\n\n\n\n","category":"type"},{"location":"native/bracketingnonlinearsolve/#BracketingNonlinearSolve.Muller","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.Muller","text":"Muller(; middle = nothing)\n\nMuller's method for determining a root of a univariate, scalar function.\n\nThe algorithm, described in Sec. 9.5.2 of Press et al. (2007), is a generalization of the secant method, using quadratic interpolation of three points to find the next estimate for the root. Due to the quadratic interpolation, the method is well suited for obtaining complex roots.\n\nThis method requires three initial guesses (left, middle, right) for the solution. The guesses (left, right) = tspan are provided by the IntervalNonlinearProblem, while the middle guess may be specified as an optional keyword argument. In notable contrast to the other BracketingNonlinearSolve.jl solvers, the Muller algorithm does not need (left, right) to bracket the root.\n\nKeyword Arguments\n\nmiddle: the initial guess for the middle point. If not provided, the midpoint of the interval (left, right) is used.\n\n\n\n\n\n","category":"type"},{"location":"native/bracketingnonlinearsolve/#BracketingNonlinearSolve.Ridder","page":"BracketingNonlinearSolve.jl","title":"BracketingNonlinearSolve.Ridder","text":"Ridder()\n\nA non-allocating ridder method.\n\n\n\n\n\n","category":"type"},{"location":"native/globalization/#Globalization-Subroutines","page":"Globalization Subroutines","title":"Globalization Subroutines","text":"","category":"section"},{"location":"native/globalization/","page":"Globalization Subroutines","title":"Globalization Subroutines","text":"The following globalization subroutines are available.","category":"page"},{"location":"native/globalization/","page":"Globalization Subroutines","title":"Globalization Subroutines","text":"Pages = [\"globalization.md\"]","category":"page"},{"location":"native/globalization/#line-search","page":"Globalization Subroutines","title":"Line Search Algorithms","text":"","category":"section"},{"location":"native/globalization/","page":"Globalization Subroutines","title":"Globalization Subroutines","text":"Line Searches have been moved to an external package. Take a look at the LineSearch.jl package and its documentation.","category":"page"},{"location":"native/globalization/#Radius-Update-Schemes-for-Trust-Region","page":"Globalization Subroutines","title":"Radius Update Schemes for Trust Region","text":"","category":"section"},{"location":"native/globalization/#NonlinearSolveFirstOrder.RadiusUpdateSchemes","page":"Globalization Subroutines","title":"NonlinearSolveFirstOrder.RadiusUpdateSchemes","text":"RadiusUpdateSchemes\n\nRadiusUpdateSchemes is provides different types of radius update schemes implemented in the Trust Region method. These schemes specify how the radius of the so-called trust region is updated after each iteration of the algorithm. The specific role and caveats associated with each scheme are provided below.\n\nUsing RadiusUpdateSchemes\n\nSimply put the desired scheme as follows: sol = solve(prob, alg = TrustRegion(radius_update_scheme = RadiusUpdateSchemes.Hei)).\n\n\n\n\n\n","category":"module"},{"location":"native/globalization/#Available-Radius-Update-Schemes","page":"Globalization Subroutines","title":"Available Radius Update Schemes","text":"","category":"section"},{"location":"native/globalization/#NonlinearSolveFirstOrder.RadiusUpdateSchemes.Simple","page":"Globalization Subroutines","title":"NonlinearSolveFirstOrder.RadiusUpdateSchemes.Simple","text":"RadiusUpdateSchemes.Simple\n\nThe simple or conventional radius update scheme. This scheme is chosen by default and follows the conventional approach to update the trust region radius, i.e. if the trial step is accepted it increases the radius by a fixed factor (bounded by a maximum radius) and if the trial step is rejected, it shrinks the radius by a fixed factor.\n\n\n\n\n\n","category":"constant"},{"location":"native/globalization/#NonlinearSolveFirstOrder.RadiusUpdateSchemes.Hei","page":"Globalization Subroutines","title":"NonlinearSolveFirstOrder.RadiusUpdateSchemes.Hei","text":"RadiusUpdateSchemes.Hei\n\nThis scheme is proposed in Hei [9]. The trust region radius depends on the size (norm) of the current step size. The hypothesis is to let the radius converge to zero as the iterations progress, which is more reliable and robust for ill-conditioned as well as degenerate problems.\n\n\n\n\n\n","category":"constant"},{"location":"native/globalization/#NonlinearSolveFirstOrder.RadiusUpdateSchemes.Yuan","page":"Globalization Subroutines","title":"NonlinearSolveFirstOrder.RadiusUpdateSchemes.Yuan","text":"RadiusUpdateSchemes.Yuan\n\nThis scheme is proposed by Yuan [6]. Similar to Hei's scheme, the trust region is updated in a way so that it converges to zero, however here, the radius depends on the size (norm) of the current gradient of the objective (merit) function. The hypothesis is that the step size is bounded by the gradient size, so it makes sense to let the radius depend on the gradient.\n\n\n\n\n\n","category":"constant"},{"location":"native/globalization/#NonlinearSolveFirstOrder.RadiusUpdateSchemes.Bastin","page":"Globalization Subroutines","title":"NonlinearSolveFirstOrder.RadiusUpdateSchemes.Bastin","text":"RadiusUpdateSchemes.Bastin\n\nThis scheme is proposed by Bastin et al. [10]. The scheme is called a retrospective update scheme as it uses the model function at the current iteration to compute the ratio of the actual reduction and the predicted reduction in the previous trial step, and use this ratio to update the trust region radius. The hypothesis is to exploit the information made available during the optimization process in order to vary the accuracy of the objective function computation.\n\n\n\n\n\n","category":"constant"},{"location":"native/globalization/#NonlinearSolveFirstOrder.RadiusUpdateSchemes.Fan","page":"Globalization Subroutines","title":"NonlinearSolveFirstOrder.RadiusUpdateSchemes.Fan","text":"RadiusUpdateSchemes.Fan\n\nThis scheme is proposed by Fan [11]. It is very much similar to Hei's and Yuan's schemes as it lets the trust region radius depend on the current size (norm) of the objective (merit) function itself. These new update schemes are known to improve local convergence.\n\n\n\n\n\n","category":"constant"},{"location":"native/globalization/#NonlinearSolveFirstOrder.RadiusUpdateSchemes.NLsolve","page":"Globalization Subroutines","title":"NonlinearSolveFirstOrder.RadiusUpdateSchemes.NLsolve","text":"RadiusUpdateSchemes.NLsolve\n\nThe same updating scheme as in NLsolve's (https://github.com/JuliaNLSolvers/NLsolve.jl) trust region dogleg implementation.\n\n\n\n\n\n","category":"constant"},{"location":"native/globalization/#NonlinearSolveFirstOrder.RadiusUpdateSchemes.NocedalWright","page":"Globalization Subroutines","title":"NonlinearSolveFirstOrder.RadiusUpdateSchemes.NocedalWright","text":"RadiusUpdateSchemes.NocedalWright\n\nTrust region updating scheme as in Nocedal and Wright [see Alg 11.5, page 291].\n\n\n\n\n\n","category":"constant"},{"location":"solvers/bracketing_solvers/#bracketing","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"Interval Root-Finding Methods (Bracketing Solvers)","text":"","category":"section"},{"location":"solvers/bracketing_solvers/","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"Interval Root-Finding Methods (Bracketing Solvers)","text":"solve(prob::IntervalNonlinearProblem, alg; kwargs...)","category":"page"},{"location":"solvers/bracketing_solvers/","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"Interval Root-Finding Methods (Bracketing Solvers)","text":"Solves for f(t) = 0 in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/bracketing_solvers/#Recommended-Methods","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/bracketing_solvers/","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"Interval Root-Finding Methods (Bracketing Solvers)","text":"ITP is the recommended method for the scalar interval root-finding problems. It is particularly well-suited for cases where the function is smooth and well-behaved; and achieved superlinear convergence while retaining the optimal worst-case performance of the Bisection method. For more details, consult the detailed solver API docs.","category":"page"},{"location":"solvers/bracketing_solvers/","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"Interval Root-Finding Methods (Bracketing Solvers)","text":"Ridder is a hybrid method that uses the value of function at the midpoint of the interval to perform an exponential interpolation to the root. This gives a fast convergence with a guaranteed convergence of at most twice the number of iterations as the bisection method.","category":"page"},{"location":"solvers/bracketing_solvers/","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"Interval Root-Finding Methods (Bracketing Solvers)","text":"Brent is a combination of the bisection method, the secant method and inverse quadratic interpolation. At every iteration, Brent's method decides which method out of these three is likely to do best, and proceeds by doing a step according to that method. This gives a robust and fast method, which therefore enjoys considerable popularity.","category":"page"},{"location":"solvers/bracketing_solvers/#Full-List-of-Methods","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/bracketing_solvers/#BracketingNonlinearSolve.jl","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"BracketingNonlinearSolve.jl","text":"","category":"section"},{"location":"solvers/bracketing_solvers/","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"Interval Root-Finding Methods (Bracketing Solvers)","text":"These methods are automatically included as part of NonlinearSolve.jl. Though, one can use BracketingNonlinearSolve.jl directly to decrease the dependencies and improve load time.","category":"page"},{"location":"solvers/bracketing_solvers/","page":"Interval Root-Finding Methods (Bracketing Solvers)","title":"Interval Root-Finding Methods (Bracketing Solvers)","text":"Alefeld: A non-allocating Alefeld method\nBisection: A common bisection method\nBrent: A non-allocating Brent method\nFalsi: A non-allocating regula falsi method\nITP: A non-allocating ITP (Interpolate, Truncate & Project) method\nMuller: A non-allocating Muller's method\nRidder: A non-allocating Ridder method","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/#Accelerated-Rootfinding-on-GPUs","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"","category":"section"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"NonlinearSolve.jl supports GPU acceleration on a wide array of devices, such as:","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"GPU Manufacturer GPU Kernel Language Julia Support Package Backend Type\nNVIDIA CUDA CUDA.jl CUDA.CUDABackend()\nAMD ROCm AMDGPU.jl AMDGPU.ROCBackend()\nIntel OneAPI OneAPI.jl oneAPI.oneAPIBackend()\nApple (M-Series) Metal Metal.jl Metal.MetalBackend()","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"To use NonlinearSolve.jl on GPUs, there are two distinctly different approaches:","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"You can build a NonlinearProblem / NonlinearLeastSquaresProblem where the elements of the problem, i.e. u0 and p, are defined on GPUs. This will make the evaluations of f occur on the GPU, and all internal updates of the solvers will be completely on the GPU as well. This is the optimal form for large systems of nonlinear equations.\nYou can use SimpleNonlinearSolve.jl as kernels in KernelAbstractions.jl. This will build problem-specific GPU kernels in order to parallelize the solution of the chosen nonlinear system over a large number of inputs. This is useful for cases where you have a small NonlinearProblem / NonlinearLeastSquaresProblem which you want to solve over a large number of initial guesses or parameters.","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"For a deeper dive into the computational difference between these techniques and why it leads to different pros/cons, see the DiffEqGPU.jl technical paper. In particular, the second form is unique to NonlinearSolve.jl and offers orders of magnitude performance improvements over libraries in Jax and PyTorch which are restricted to only using the first form.","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"In this tutorial we will highlight both use cases in separate parts.","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"note: Note\nIf you're looking for GPU-accelerated neural networks inside of nonlinear solvers, check out DeepEquilibriumNetworks.jl.","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/#GPU-Acceleration-of-Large-Nonlinear-Systems-using-GPUArrays","page":"Accelerated Rootfinding on GPUs","title":"GPU Acceleration of Large Nonlinear Systems using GPUArrays","text":"","category":"section"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"The simplest way to GPU accelerate a large system is to simply make your u0 and p values be on the GPU via GPUArrays. For example, CUDA.jl has the CuArray type which implements standard array operations, such as broadcasting and linear algebra. And since NonlinearSolve.jl respects your chosen array types, if you choose to make u0 be a type that is on the GPU, then all internal broadcasting and linear algebra takes place on the GPU.","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"This means the one major limitation is that you as a user must write your f to be compatible with GPU arrays. Those limitations are discussed in detail in the GPU libraries, for example the CuArray documentation discusses the operations available for CUDA arrays","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"In practice when this comes together, it looks like:","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"import NonlinearSolve as NLS\nimport CUDA\n\nf(u, p) = u .* u .- p\nu0 = CUDA.cu(ones(1000))\np = CUDA.cu(collect(1:1000))\nprob = NLS.NonlinearProblem(f, u0, p)\nsol = NLS.solve(prob, NLS.NewtonRaphson(), abstol = 1.0f-4)","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"Notice a few things here. One, nothing is different except the input array types. But notice that cu arrays automatically default to Float32 precision. Since NonlinearSolve.jl respects the user's chosen types, this changes NonlinearSolve.jl to use Float32 precision, and thus the tolerances are adjusted accordingly.","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/#GPU-Acceleration-over-Large-Parameter-Searches-using-KernelAbstractions.jl","page":"Accelerated Rootfinding on GPUs","title":"GPU Acceleration over Large Parameter Searches using KernelAbstractions.jl","text":"","category":"section"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"If one has a \"small\" (200 equations or less) system of equations which they wish to solve over many different inputs (parameters), then using the kernel generation approach will be much more efficient than using the GPU-based array approach. In short, the GPU array approach strings together standard GPU kernel calls (matrix multiply, +, etc.) where each operation is an optimized GPU-accelerated call. In the kernel-building approach, we build a custom kernel f that is then compiled specifically for our problem and ran in parallel. This is equivalent to having built custom CUDA code for our problem! The reason this is so much faster is because each kernel call has startup overhead, and we can cut that all down to simply one optimized call.","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"To do this, we use KernelAbstractions.jl. First we have to say \"what\" our kernel is. The kernel is the thing you want to embaressingly parallel call a bunch. For this nonlinear solving, it will be the rebuilding of our nonlinear problem to new parameters and solving it. This function must be defined using the KernelAbstractions.@kernel macro. This looks like:","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"import NonlinearSolve as NLS\nimport StaticArrays\nimport SciMLBase\nimport KernelAbstractions # For writing vendor-agnostic kernels\nimport CUDA # For if you have an NVIDIA GPU\nimport AMDGPU # For if you have an AMD GPU\nimport Metal # For if you have a Mac M-series device and want to use the built-in GPU\nimport OneAPI # For if you have an Intel GPU\n\nKernelAbstractions.@kernel function parallel_nonlinearsolve_kernel!(result, @Const(prob), @Const(alg))\n    i = @index(Global)\n    prob_i = SciMLBase.remake(prob; p = prob.p[i])\n    sol = NLS.solve(prob_i, alg)\n    @inbounds result[i] = sol.u\nend","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"Note that i = @index(Global) is used to get a global index. I.e. this kernel will be called with N different prob objects, and this i means \"for the ith call\". So this is saying, \"for the ith call, get the i'th parameter set and solve with these parameters. The ith result is then this solution\".","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"note: Note\nBecause kernel code needs to be able to be compiled to a GPU kernel, it has very strict specifications of what's allowed because GPU cores are not as flexible as CPU cores. In general, this means that you need to avoid any runtime operations in kernel code, such as allocating vectors, dynamic dispatch, type instabilities, etc. The main thing to note is that most NonlinearSolve.jl algorithms will not be compatible with being in kernels. However, SimpleNonlinearSolve.jl solvers are tested to be compatible, and thus one should only choose SimpleNonlinearSolve.jl methods within kernels.","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"Once you have defined your kernel, you need to use KernelAbstractions in order to distribute your call. This looks like:","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"function vectorized_solve(prob, alg; backend = CPU())\n    result = KernelAbstractions.allocate(backend, eltype(prob.p), length(prob.p))\n    groupsize = min(length(prob.p), 1024)\n    kernel! = parallel_nonlinearsolve_kernel!(backend, groupsize, length(prob.p))\n    kernel!(result, prob, alg)\n    KernelAbstractions.synchronize(backend)\n    return result\nend","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"Now let's build a nonlinear system to test it on.","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"@inbounds function p2_f(x, p)\n    out1 = x[1] + p[1] * x[2]\n    out2 = sqrt(p[2]) * (x[3] - x[4])\n    out3 = (x[2] - p[3] * x[3])^2\n    out4 = sqrt(p[4]) * (x[1] - x[4]) * (x[1] - x[4])\n    StaticArrays.SA[out1, out2, out3, out4]\nend\n\np = StaticArrays.@SVector [StaticArrays.@SVector(rand(Float32, 4)) for _ in 1:1024]\nu0 = StaticArrays.SA[1.0f0, 2.0f0, 3.0f0, 4.0f0]\nprob = SciMLBase.ImmutableNonlinearProblem{false}(p2_f, u0, p)","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"note: Note\nBecause the custom kernel is going to need to embed the the code for our nonlinear problem into the kernel, it also must be written to be GPU compatible. In general, this means that you need to avoid any runtime operations in kernel code, such as allocating vectors, dynamic dispatch, type instabilities, etc. Thus to make this work, your f function should be non-allocating, your u0 function should use StaticArrays, and you must use SciMLBase.ImmutableNonlinearProblem (which is exactly the same as NonlinearProblem except it's immutable to satisfy the requirements of GPU kernels). Also, it's recommended that for most GPUs you use Float32 precision because many GPUs are much slower on 64-bit floating point operations.","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"and we then simply call our vectorized kernel to parallelize it:","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"# Threaded CPU\nvectorized_solve(prob, NLS.SimpleNewtonRaphson(); backend = KernelAbstractions.CPU())\n# AMD ROCM GPU\nvectorized_solve(prob, NLS.SimpleNewtonRaphson(); backend = AMDGPU.ROCBackend())\n# NVIDIA CUDA GPU\nvectorized_solve(prob, NLS.SimpleNewtonRaphson(); backend = CUDA.CUDABackend())\n# Intel GPU\nvectorized_solve(prob, NLS.SimpleNewtonRaphson(); backend = OneAPI.oneAPIBackend())\n# Mac M-Series, such as M3Max\nvectorized_solve(prob, NLS.SimpleNewtonRaphson(); backend = Metal.MetalBackend())","category":"page"},{"location":"tutorials/nonlinear_solve_gpus/","page":"Accelerated Rootfinding on GPUs","title":"Accelerated Rootfinding on GPUs","text":"warn: Warn\nThe GPU-based calls will only work on your machine if you have a compatible GPU!","category":"page"},{"location":"native/simplenonlinearsolve/#SimpleNonlinearSolve.jl","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.jl","text":"","category":"section"},{"location":"native/simplenonlinearsolve/","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.jl","text":"These methods can be used independently of the rest of NonlinearSolve.jl","category":"page"},{"location":"native/simplenonlinearsolve/","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.jl","text":"Pages = [\"simplenonlinearsolve.md\"]","category":"page"},{"location":"native/simplenonlinearsolve/#General-Methods","page":"SimpleNonlinearSolve.jl","title":"General Methods","text":"","category":"section"},{"location":"native/simplenonlinearsolve/","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.jl","text":"These methods are suited for any general nonlinear root-finding problem, i.e. NonlinearProblem.","category":"page"},{"location":"native/simplenonlinearsolve/","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.jl","text":"Solver In-place Out of Place Non-Allocating (Scalars) Non-Allocating (SArray)\nSimpleNewtonRaphson ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è\nSimpleBroyden ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è\nSimpleHalley ‚ùå ‚úîÔ∏è ‚úîÔ∏è ‚ùå\nSimpleKlement ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è\nSimpleTrustRegion ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è\nSimpleDFSane ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è[1] ‚úîÔ∏è\nSimpleLimitedMemoryBroyden ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è ‚úîÔ∏è[2]","category":"page"},{"location":"native/simplenonlinearsolve/","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.jl","text":"The algorithms which are non-allocating can be used directly inside GPU Kernels[3]. See ParallelParticleSwarms.jl for more details.","category":"page"},{"location":"native/simplenonlinearsolve/#SimpleNonlinearSolve.SimpleNewtonRaphson","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.SimpleNewtonRaphson","text":"SimpleNewtonRaphson(autodiff)\nSimpleNewtonRaphson(; autodiff = nothing)\n\nA low-overhead implementation of Newton-Raphson. This method is non-allocating on scalar and static array problems.\n\nnote: Note\nAs part of the decreased overhead, this method omits some of the higher level error catching of the other methods. Thus, to see better error messages, use one of the other methods like NewtonRaphson.\n\nKeyword Arguments\n\nautodiff: determines the backend used for the Jacobian. Defaults to  nothing (i.e. automatic backend selection). Valid choices include jacobian backends from DifferentiationInterface.jl.\n\n\n\n\n\n","category":"type"},{"location":"native/simplenonlinearsolve/#SimpleNonlinearSolve.SimpleBroyden","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.SimpleBroyden","text":"SimpleBroyden(; linesearch = Val(false), alpha = nothing)\n\nA low-overhead implementation of Broyden. This method is non-allocating on scalar and static array problems.\n\nKeyword Arguments\n\nlinesearch: If linesearch is Val(true), then we use the LiFukushimaLineSearch line search else no line search is used. For advanced customization of the line search, use Broyden from NonlinearSolve.jl.\nalpha: Scale the initial jacobian initialization with alpha. If it is nothing, we will compute the scaling using 2 * norm(fu) / max(norm(u), true).\n\n\n\n\n\n","category":"type"},{"location":"native/simplenonlinearsolve/#SimpleNonlinearSolve.SimpleHalley","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.SimpleHalley","text":"SimpleHalley(autodiff)\nSimpleHalley(; autodiff = nothing)\n\nA low-overhead implementation of Halley's Method.\n\nnote: Note\nAs part of the decreased overhead, this method omits some of the higher level error catching of the other methods. Thus, to see better error messages, use one of the other methods like NewtonRaphson.\n\nKeyword Arguments\n\nautodiff: determines the backend used for the Jacobian. Defaults to  nothing (i.e. automatic backend selection). Valid choices include jacobian backends from DifferentiationInterface.jl.\n\n\n\n\n\n","category":"type"},{"location":"native/simplenonlinearsolve/#SimpleNonlinearSolve.SimpleKlement","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.SimpleKlement","text":"SimpleKlement()\n\nA low-overhead implementation of Klement [4]. This method is non-allocating on scalar and static array problems.\n\n\n\n\n\n","category":"type"},{"location":"native/simplenonlinearsolve/#SimpleNonlinearSolve.SimpleTrustRegion","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.SimpleTrustRegion","text":"SimpleTrustRegion(;\n    autodiff = AutoForwardDiff(), max_trust_radius = 0.0,\n    initial_trust_radius = 0.0, step_threshold = nothing,\n    shrink_threshold = nothing, expand_threshold = nothing,\n    shrink_factor = 0.25, expand_factor = 2.0, max_shrink_times::Int = 32,\n    nlsolve_update_rule = Val(false)\n)\n\nA low-overhead implementation of a trust-region solver. This method is non-allocating on scalar and static array problems.\n\nKeyword Arguments\n\nautodiff: determines the backend used for the Jacobian. Defaults to nothing (i.e. automatic backend selection). Valid choices include jacobian backends from DifferentiationInterface.jl.\nmax_trust_radius: the maximum radius of the trust region. Defaults to max(norm(f(u0)), maximum(u0) - minimum(u0)).\ninitial_trust_radius: the initial trust region radius. Defaults to max_trust_radius / 11.\nstep_threshold: the threshold for taking a step. In every iteration, the threshold is compared with a value r, which is the actual reduction in the objective function divided by the predicted reduction. If step_threshold > r the model is not a good approximation, and the step is rejected. Defaults to 0.0001. For more details, see Rahpeymaii, F.\nshrink_threshold: the threshold for shrinking the trust region radius. In every iteration, the threshold is compared with a value r which is the actual reduction in the objective function divided by the predicted reduction. If shrink_threshold > r the trust region radius is shrunk by shrink_factor. Defaults to 0.25. For more details, see Rahpeymaii, F.\nexpand_threshold: the threshold for expanding the trust region radius. If a step is taken, i.e step_threshold < r (with r defined in shrink_threshold), a check is also made to see if expand_threshold < r. If that is true, the trust region radius is expanded by expand_factor. Defaults to 0.75.\nshrink_factor: the factor to shrink the trust region radius with if shrink_threshold > r (with r defined in shrink_threshold). Defaults to 0.25.\nexpand_factor: the factor to expand the trust region radius with if expand_threshold < r (with r defined in shrink_threshold). Defaults to 2.0.\nmax_shrink_times: the maximum number of times to shrink the trust region radius in a row, max_shrink_times is exceeded, the algorithm returns. Defaults to 32.\nnlsolve_update_rule: If set to Val(true), updates the trust region radius using the update rule from NLSolve.jl. Defaults to Val(false). If set to Val(true), few of the radius update parameters ‚Äì step_threshold = 0.05, expand_threshold = 0.9, and shrink_factor = 0.5 ‚Äì have different defaults.\n\n\n\n\n\n","category":"type"},{"location":"native/simplenonlinearsolve/#SimpleNonlinearSolve.SimpleDFSane","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.SimpleDFSane","text":"SimpleDFSane(;\n    œÉ_min::Real = 1e-10, œÉ_max::Real = 1e10, œÉ_1::Real = 1.0,\n    M::Union{Int, Val} = Val(10), Œ≥::Real = 1e-4, œÑ_min::Real = 0.1, œÑ_max::Real = 0.5,\n    nexp::Int = 2, Œ∑_strategy::Function = (f_1, k, x, F) -> f_1 ./ k^2\n)\n\nA low-overhead implementation of the df-sane method for solving large-scale nonlinear systems of equations. For in depth information about all the parameters and the algorithm, see La Cruz et al. [2].\n\nKeyword Arguments\n\nœÉ_min: the minimum value of the spectral coefficient œÉ_k which is related to the step size in the algorithm. Defaults to 1e-10.\nœÉ_max: the maximum value of the spectral coefficient œÉ_k which is related to the step size in the algorithm. Defaults to 1e10.\nœÉ_1: the initial value of the spectral coefficient œÉ_k which is related to the step size in the algorithm.. Defaults to 1.0.\nM: The monotonicity of the algorithm is determined by a this positive integer. A value of 1 for M would result in strict monotonicity in the decrease of the L2-norm of the function f. However, higher values allow for more flexibility in this reduction. Despite this, the algorithm still ensures global convergence through the use of a non-monotone line-search algorithm that adheres to the Grippo-Lampariello-Lucidi condition. Values in the range of 5 to 20 are usually sufficient, but some cases may call for a higher value of M. The default setting is 10.\nŒ≥: a parameter that influences if a proposed step will be accepted. Higher value of Œ≥ will make the algorithm more restrictive in accepting steps. Defaults to 1e-4.\nœÑ_min: if a step is rejected the new step size will get multiplied by factor, and this parameter is the minimum value of that factor. Defaults to 0.1.\nœÑ_max: if a step is rejected the new step size will get multiplied by factor, and this parameter is the maximum value of that factor. Defaults to 0.5.\nnexp: the exponent of the loss, i.e. f_k=F(x_k)^nexp. The paper uses nexp ‚àà {1,2}. Defaults to 2.\nŒ∑_strategy:  function to determine the parameter Œ∑_k, which enables growth of F^2. Called as Œ∑_k = Œ∑_strategy(f_1, k, x, F) with f_1 initialized as f_1=F(x_1)^nexp, k is the iteration number, x is the current x-value and F the current residual. Should satisfy Œ∑_k  0 and ‚Çñ Œ∑‚Çñ  . Defaults to F^2  k^2.\n\n\n\n\n\n","category":"type"},{"location":"native/simplenonlinearsolve/#SimpleNonlinearSolve.SimpleLimitedMemoryBroyden","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.SimpleLimitedMemoryBroyden","text":"SimpleLimitedMemoryBroyden(;\n    threshold::Union{Val, Int} = Val(27), linesearch = Val(false), alpha = nothing\n)\n\nA limited memory implementation of Broyden. This method applies the L-BFGS scheme to Broyden's method.\n\nIf the threshold is larger than the problem size, then this method will use SimpleBroyden.\n\nKeyword Arguments:\n\nlinesearch: If linesearch is Val(true), then we use the LiFukushimaLineSearch line search else no line search is used. For advanced customization of the line search, use Broyden from NonlinearSolve.jl.\nalpha: Scale the initial jacobian initialization with alpha. If it is nothing, we will compute the scaling using 2 * norm(fu) / max(norm(u), true).\n\nwarning: Warning\nCurrently alpha is only used for StaticArray problems. This will be fixed in the future.\n\n\n\n\n\n","category":"type"},{"location":"native/simplenonlinearsolve/","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.jl","text":"SimpleGaussNewton is aliased to SimpleNewtonRaphson for solving Nonlinear Least Squares problems.","category":"page"},{"location":"native/simplenonlinearsolve/","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.jl","text":"[1]: Needs StaticArrays.jl to be installed and loaded for the non-allocating version.","category":"page"},{"location":"native/simplenonlinearsolve/","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.jl","text":"[2]: This method is non-allocating if the termination condition is set to either nothing (default) or NonlinearSolveBase.AbsNormTerminationMode.","category":"page"},{"location":"native/simplenonlinearsolve/","page":"SimpleNonlinearSolve.jl","title":"SimpleNonlinearSolve.jl","text":"[3]: Only the defaults are guaranteed to work inside kernels. We try to provide warnings if the used version is not non-allocating.","category":"page"},{"location":"api/minpack/#MINPACK.jl","page":"MINPACK.jl","title":"MINPACK.jl","text":"","category":"section"},{"location":"api/minpack/","page":"MINPACK.jl","title":"MINPACK.jl","text":"This is a extension for importing solvers from MINPACK into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/minpack/","page":"MINPACK.jl","title":"MINPACK.jl","text":"import Pkg\nPkg.add(\"MINPACK\")\nimport MINPACK\nimport NonlinearSolve as NLS","category":"page"},{"location":"api/minpack/#Solver-API","page":"MINPACK.jl","title":"Solver API","text":"","category":"section"},{"location":"api/minpack/#NonlinearSolve.CMINPACK","page":"MINPACK.jl","title":"NonlinearSolve.CMINPACK","text":"CMINPACK(; method::Symbol = :auto, autodiff = missing)\n\nKeyword Arguments\n\nmethod: the choice of method for the solver.\nautodiff: Defaults to missing, which means we will default to letting MINPACK construct the jacobian if f.jac is not provided. In other cases, we use it to generate a jacobian similar to other NonlinearSolve solvers.\n\nSubmethod Choice\n\nThe keyword argument method can take on different value depending on which method of fsolve you are calling. The standard choices of method are:\n\n:hybr: Modified version of Powell's algorithm. Uses MINPACK routine hybrd1\n:lm: Levenberg-Marquardt. Uses MINPACK routine lmdif1\n:lmdif: Advanced Levenberg-Marquardt (more options available with ; kwargs...). See MINPACK routine lmdif for more information\n:hybrd: Advanced modified version of Powell's algorithm (more options available with ; kwargs...). See MINPACK routine hybrd for more information\n\nIf a Jacobian is supplied as part of the NonlinearFunction, then the following methods are allowed:\n\n:hybr: Advanced modified version of Powell's algorithm with user supplied Jacobian. Additional arguments are available via ; kwargs.... See MINPACK routine hybrj for more information\n:lm: Advanced Levenberg-Marquardt with user supplied Jacobian. Additional arguments are available via ; kwargs.... See MINPACK routine lmder for more information\n\nThe default choice of :auto selects :hybr for NonlinearProblem and :lm for NonlinearLeastSquaresProblem.\n\nnote: Note\nThis algorithm is only available if MINPACK.jl is installed and loaded.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/snes_ex2/#snes_ex2","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"","category":"section"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"This implements src/snes/examples/tutorials/ex2.c from PETSc and examples/SNES_ex2.jl from PETSc.jl using automatic sparsity detection and automatic differentiation using NonlinearSolve.jl.","category":"page"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"This solves the equations sequentially. Newton method to solve u'' + u^{2} = f, sequentially.","category":"page"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"import NonlinearSolve as NLS\nimport PETSc\nimport LinearAlgebra\nimport SparseConnectivityTracer\nimport BenchmarkTools\n\nu0 = fill(0.5, 128)\n\nfunction form_residual!(resid, x, _)\n    n = length(x)\n    xp = LinRange(0.0, 1.0, n)\n    F = 6xp .+ (xp .+ 1e-12) .^ 6\n\n    dx = 1 / (n - 1)\n    resid[1] = x[1]\n    for i in 2:(n - 1)\n        resid[i] = (x[i - 1] - 2x[i] + x[i + 1]) / dx^2 + x[i] * x[i] - F[i]\n    end\n    resid[n] = x[n] - 1\n\n    return\nend","category":"page"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"To use automatic sparsity detection, we need to specify sparsity keyword argument to NonlinearFunction. See Automatic Sparsity Detection for more details.","category":"page"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"nlfunc_dense = NLS.NonlinearFunction(form_residual!)\nnlfunc_sparse = NLS.NonlinearFunction(\n    form_residual!; sparsity = SparseConnectivityTracer.TracerSparsityDetector())\n\nnlprob_dense = NLS.NonlinearProblem(nlfunc_dense, u0)\nnlprob_sparse = NLS.NonlinearProblem(nlfunc_sparse, u0)","category":"page"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"Now we can solve the problem using PETScSNES or with one of the native NonlinearSolve.jl solvers.","category":"page"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"sol_dense_nr = NLS.solve(nlprob_dense, NLS.NewtonRaphson(); abstol = 1e-8)\nsol_dense_snes = NLS.solve(nlprob_dense, NLS.PETScSNES(); abstol = 1e-8)\nsol_dense_nr .- sol_dense_snes","category":"page"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"sol_sparse_nr = NLS.solve(nlprob_sparse, NLS.NewtonRaphson(); abstol = 1e-8)\nsol_sparse_snes = NLS.solve(nlprob_sparse, NLS.PETScSNES(); abstol = 1e-8)\nsol_sparse_nr .- sol_sparse_snes","category":"page"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"As expected the solutions are the same (upto floating point error). Now let's compare the runtimes.","category":"page"},{"location":"tutorials/snes_ex2/#Runtimes","page":"PETSc SNES Example 2","title":"Runtimes","text":"","category":"section"},{"location":"tutorials/snes_ex2/#Dense-Jacobian","page":"PETSc SNES Example 2","title":"Dense Jacobian","text":"","category":"section"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"BenchmarkTools.@benchmark NLS.solve($(nlprob_dense), $(NLS.NewtonRaphson()); abstol = 1e-8)","category":"page"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"BenchmarkTools.@benchmark NLS.solve($(nlprob_dense), $(NLS.PETScSNES()); abstol = 1e-8)","category":"page"},{"location":"tutorials/snes_ex2/#Sparse-Jacobian","page":"PETSc SNES Example 2","title":"Sparse Jacobian","text":"","category":"section"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"BenchmarkTools.@benchmark NLS.solve($(nlprob_sparse), $(NLS.NewtonRaphson()); abstol = 1e-8)","category":"page"},{"location":"tutorials/snes_ex2/","page":"PETSc SNES Example 2","title":"PETSc SNES Example 2","text":"BenchmarkTools.@benchmark NLS.solve($(nlprob_sparse), $(NLS.PETScSNES()); abstol = 1e-8)","category":"page"},{"location":"api/leastsquaresoptim/#LeastSquaresOptim.jl","page":"LeastSquaresOptim.jl","title":"LeastSquaresOptim.jl","text":"","category":"section"},{"location":"api/leastsquaresoptim/","page":"LeastSquaresOptim.jl","title":"LeastSquaresOptim.jl","text":"This is an extension for importing solvers from LeastSquaresOptim.jl into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/leastsquaresoptim/","page":"LeastSquaresOptim.jl","title":"LeastSquaresOptim.jl","text":"import Pkg\nPkg.add(\"LeastSquaresOptim\")\nimport LeastSquaresOptim\nimport NonlinearSolve as NLS","category":"page"},{"location":"api/leastsquaresoptim/#Solver-API","page":"LeastSquaresOptim.jl","title":"Solver API","text":"","category":"section"},{"location":"api/leastsquaresoptim/#NonlinearSolve.LeastSquaresOptimJL","page":"LeastSquaresOptim.jl","title":"NonlinearSolve.LeastSquaresOptimJL","text":"LeastSquaresOptimJL(alg = :lm; linsolve = nothing, autodiff::Symbol = :central)\n\nWrapper over LeastSquaresOptim.jl for solving NonlinearLeastSquaresProblem.\n\nArguments\n\nalg: Algorithm to use. Can be :lm or :dogleg.\n\nKeyword Arguments\n\nlinsolve: Linear solver to use. Can be :qr, :cholesky or :lsmr. If nothing, then LeastSquaresOptim.jl will choose the best linear solver based on the Jacobian structure.\nautodiff: Automatic differentiation / Finite Differences. Can be :central or :forward.\n\nnote: Note\nThis algorithm is only available if LeastSquaresOptim.jl is installed and loaded.\n\n\n\n\n\n","category":"type"},{"location":"solvers/steady_state_solvers/#ss_solvers","page":"Steady State Solvers","title":"Steady State Solvers","text":"","category":"section"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"solve(prob::SteadyStateProblem, alg; kwargs)","category":"page"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"Solves for the steady states in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/steady_state_solvers/#Recommended-Methods","page":"Steady State Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"Conversion to a NonlinearProblem is generally the fastest method. However, this will not guarantee the preferred root (the stable equilibrium), and thus if the preferred root is required, then it's recommended that one uses DynamicSS. For DynamicSS, often an adaptive stiff solver, like a Rosenbrock or BDF method (Rodas5 or QNDF), is a good way to allow for very large time steps as the steady state approaches.","category":"page"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"The SteadyStateDiffEq.jl methods on a SteadyStateProblem respect the time definition in the nonlinear definition, i.e., u' = f(u, t) uses the correct values for t as the solution evolves. A conversion of a SteadyStateProblem to a NonlinearProblem replaces this with the nonlinear system u' = f(u, ‚àû), and thus the direct SteadyStateProblem approach can give different answers (i.e., the correct unique fixed point) on ODEs with non-autonomous dynamics.","category":"page"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"If you have an unstable equilibrium and you want to solve for the unstable equilibrium, then DynamicSS will not converge to that equilibrium for any initial condition. However, Nonlinear Solvers don't suffer from this issue, and thus it's recommended to use a nonlinear solver if you want to solve for the unstable equilibrium.","category":"page"},{"location":"solvers/steady_state_solvers/#Full-List-of-Methods","page":"Steady State Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/steady_state_solvers/#Conversion-to-NonlinearProblem","page":"Steady State Solvers","title":"Conversion to NonlinearProblem","text":"","category":"section"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"Any SteadyStateProblem can be trivially converted to a NonlinearProblem via NonlinearProblem(prob::SteadyStateProblem). Using this approach, any of the solvers from the Nonlinear System Solvers page can be used. As a convenience, users can use:","category":"page"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"SSRootfind: A wrapper around NonlinearSolve.jl compliant solvers which converts the SteadyStateProblem to a NonlinearProblem and solves it.","category":"page"},{"location":"solvers/steady_state_solvers/#SteadyStateDiffEq.jl","page":"Steady State Solvers","title":"SteadyStateDiffEq.jl","text":"","category":"section"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"SteadyStateDiffEq.jl uses ODE solvers to iteratively approach the steady state. It is a very stable method for solving nonlinear systems, though often computationally more expensive than direct methods.","category":"page"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"DynamicSS : Uses an ODE solver to find the steady state. Automatically terminates when close to the steady state. DynamicSS(alg; tspan = Inf) requires that an ODE algorithm is given as the first argument. The absolute and relative tolerances specify the termination conditions on the derivative's closeness to zero. This internally uses the TerminateSteadyState callback from the Callback Library. The simulated time, for which the ODE is solved, can be limited by tspan.  If tspan is a number, it is equivalent to passing (zero(tspan), tspan).","category":"page"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"Example usage:","category":"page"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"import NonlinearSolve as NLS\nimport SteadyStateDiffEq as SSDE\nimport OrdinaryDiffEq as ODE\nsol = NLS.solve(prob, SSDE.DynamicSS(ODE.Tsit5()))\n\nimport Sundials\nsol = NLS.solve(prob, SSDE.DynamicSS(Sundials.CVODE_BDF()), dt = 1.0)","category":"page"},{"location":"solvers/steady_state_solvers/","page":"Steady State Solvers","title":"Steady State Solvers","text":"note: Note\nIf you use CVODE_BDF you may need to give a starting dt via dt=.....","category":"page"},{"location":"api/scipy/#SciPy","page":"SciPy","title":"SciPy","text":"","category":"section"},{"location":"api/scipy/","page":"SciPy","title":"SciPy","text":"This is an extension for importing solvers from SciPy into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/scipy/","page":"SciPy","title":"SciPy","text":"import Pkg\nPkg.add(\"NonlinearSolveSciPy\")\nimport NonlinearSolveSciPy as NLSP","category":"page"},{"location":"api/scipy/","page":"SciPy","title":"SciPy","text":"Note that using this package requires Python and SciPy to be available via PythonCall.jl.","category":"page"},{"location":"api/scipy/#Solver-API","page":"SciPy","title":"Solver API","text":"","category":"section"},{"location":"api/scipy/#NonlinearSolveSciPy.SciPyLeastSquares","page":"SciPy","title":"NonlinearSolveSciPy.SciPyLeastSquares","text":"SciPyLeastSquares(; method=\"trf\", loss=\"linear\")\n\nWrapper over scipy.optimize.least_squares (via PythonCall) for solving NonlinearLeastSquaresProblems.  The keyword arguments correspond to the method (\"trf\", \"dogbox\", \"lm\") and the robust loss function (\"linear\", \"soft_l1\", \"huber\", \"cauchy\", \"arctan\").\n\n\n\n\n\n","category":"type"},{"location":"api/scipy/#NonlinearSolveSciPy.SciPyRoot","page":"SciPy","title":"NonlinearSolveSciPy.SciPyRoot","text":"SciPyRoot(; method=\"hybr\")\n\nWrapper over scipy.optimize.root for solving NonlinearProblems.  Available methods include \"hybr\" (default), \"lm\", \"broyden1\", \"broyden2\", \"anderson\", \"diagbroyden\", \"linearmixing\", \"excitingmixing\", \"krylov\", \"df-sane\" ‚Äì any method accepted by SciPy.\n\n\n\n\n\n","category":"type"},{"location":"api/scipy/#NonlinearSolveSciPy.SciPyRootScalar","page":"SciPy","title":"NonlinearSolveSciPy.SciPyRootScalar","text":"SciPyRootScalar(; method=\"brentq\")\n\nWrapper over scipy.optimize.root_scalar for scalar IntervalNonlinearProblems (bracketing problems).  The default method uses Brent's algorithm (\"brentq\"). Other valid options: \"bisect\", \"brentq\", \"brenth\", \"ridder\", \"toms748\", \"secant\", \"newton\" (secant/Newton do not require brackets).\n\n\n\n\n\n","category":"type"},{"location":"tutorials/small_compile/#fast_startup","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"","category":"section"},{"location":"tutorials/small_compile/","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"In many instances one may want a very lightweight version of NonlinearSolve.jl. For this case there exists the solver package SimpleNonlinearSolve.jl. SimpleNonlinearSolve.jl solvers all satisfy the same interface as NonlinearSolve.jl, but they are designed to be simpler, lightweight, and thus have a faster startup time. Everything that can be done with NonlinearSolve.jl can be done with SimpleNonlinearSolve.jl. Thus for example, we can solve the core tutorial problem with just SimpleNonlinearSolve.jl as follows:","category":"page"},{"location":"tutorials/small_compile/","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"import SimpleNonlinearSolve as SNLS\n\nf(u, p) = u .* u .- p\nu0 = [1.0, 1.0]\np = 2.0\nprob = SNLS.NonlinearProblem(f, u0, p)\nsol = SNLS.solve(prob, SNLS.SimpleNewtonRaphson())","category":"page"},{"location":"tutorials/small_compile/","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"However, there are a few downsides to SimpleNonlinearSolve's SimpleX style algorithms to note:","category":"page"},{"location":"tutorials/small_compile/","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"SimpleNonlinearSolve.jl's methods are not hooked into the LinearSolve.jl system, and thus do not have the ability to specify linear solvers, use sparse matrices, preconditioners, and all of the other features which are required to scale for very large systems of equations.\nSimpleNonlinearSolve.jl's methods have less robust error handling and termination conditions, and thus these methods are missing some flexibility and give worse hints for debugging. Note that these can be enabled but are disabled by default.","category":"page"},{"location":"tutorials/small_compile/","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"However, the major upsides of SimpleNonlinearSolve.jl are:","category":"page"},{"location":"tutorials/small_compile/","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"The methods are optimized and non-allocating on StaticArrays\nThe methods are minimal in compilation","category":"page"},{"location":"tutorials/small_compile/","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"As such, you can use the code as shown above to have very low startup with good methods, but for more scaling and debuggability we recommend the full NonlinearSolve.jl. But that said,","category":"page"},{"location":"tutorials/small_compile/","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"import StaticArrays\n\nu0 = StaticArrays.SA[1.0, 1.0]\np = 2.0\nprob = SNLS.NonlinearProblem(f, u0, p)\nsol = SNLS.solve(prob, SNLS.SimpleNewtonRaphson())","category":"page"},{"location":"tutorials/small_compile/","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"using StaticArrays.jl is also the fastest form for small equations, so if you know your system is small then SimpleNonlinearSolve.jl is not only sufficient but optimal.","category":"page"},{"location":"tutorials/small_compile/#Static-Compilation","page":"Faster Startup and and Static Compilation","title":"Static Compilation","text":"","category":"section"},{"location":"tutorials/small_compile/","page":"Faster Startup and and Static Compilation","title":"Faster Startup and and Static Compilation","text":"Julia has tools for building small binaries via static compilation with StaticCompiler.jl. However, these tools are currently limited to type-stable non-allocating functions. That said, SimpleNonlinearSolve.jl's solvers are precisely the subset of NonlinearSolve.jl which are compatible with static compilation.","category":"page"},{"location":"devdocs/jacobian/#Jacobian-Wrappers","page":"Jacobian Wrappers","title":"Jacobian Wrappers","text":"","category":"section"},{"location":"devdocs/jacobian/#NonlinearSolveBase.construct_jacobian_cache","page":"Jacobian Wrappers","title":"NonlinearSolveBase.construct_jacobian_cache","text":"construct_jacobian_cache(\n    prob, alg, f, fu, u = prob.u0, p = prob.p;\n    autodiff = nothing, vjp_autodiff = nothing, jvp_autodiff = nothing,\n    linsolve = missing\n)\n\nConstruct a cache for the Jacobian of f w.r.t. u.\n\nArguments\n\nprob: A NonlinearProblem or a NonlinearLeastSquaresProblem.\nalg: A AbstractNonlinearSolveAlgorithm. Used to check for concrete_jac.\nf: The function to compute the Jacobian of.\nfu: The evaluation of f(u, p) or f(_, u, p). Used to determine the size of the result cache and Jacobian.\nu: The current value of the state.\np: The current value of the parameters.\n\nKeyword Arguments\n\nautodiff: Automatic Differentiation or Finite Differencing backend for computing the jacobian. By default, selects a backend based on sparsity parameters, type of state, function properties, etc.\nvjp_autodiff: Automatic Differentiation or Finite Differencing backend for computing the vector-Jacobian product.\njvp_autodiff: Automatic Differentiation or Finite Differencing backend for computing the Jacobian-vector product.\nlinsolve: Linear Solver Algorithm used to determine if we need a concrete jacobian or if possible we can just use a JacobianOperator instead.\n\n\n\n\n\n","category":"function"},{"location":"api/sundials/#Sundials.jl","page":"Sundials.jl","title":"Sundials.jl","text":"","category":"section"},{"location":"api/sundials/","page":"Sundials.jl","title":"Sundials.jl","text":"This is a wrapper package for importing solvers from Sundials into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/sundials/","page":"Sundials.jl","title":"Sundials.jl","text":"import Pkg\nPkg.add(\"Sundials\")\nimport Sundials","category":"page"},{"location":"api/sundials/","page":"Sundials.jl","title":"Sundials.jl","text":"These methods can be used independently of the rest of NonlinearSolve.jl.","category":"page"},{"location":"api/sundials/#Solver-API","page":"Sundials.jl","title":"Solver API","text":"","category":"section"},{"location":"api/sundials/#Sundials.KINSOL","page":"Sundials.jl","title":"Sundials.KINSOL","text":"KINSOL: Newton-Krylov technique solver\n\nKINSOL(;\n    linear_solver = :Dense,\n    jac_upper = 0,\n    jac_lower = 0,\n    userdata = nothing,\n    prec_side = 0,\n    krylov_dim = 0,\n    globalization_strategy = :None\n    maxsetupcalls=0\n)\n\nThe choices for the linear solver are:\n\n:Dense: A dense linear solver\n:Band: A solver specialized for banded Jacobians. If used, you must set the position of the upper and lower non-zero diagonals via jac_upper and jac_lower.\n:LapackDense: A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Dense on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:LapackBand: A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Band on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:GMRES: A GMRES method. Recommended first choice Krylov method.\n:BCG: A biconjugate gradient method\n:PCG: A preconditioned conjugate gradient method. Only for symmetric linear systems.\n:TFQMR: A TFQMR method.\n:KLU: A sparse factorization method. Requires that the user specify a Jacobian. The Jacobian must be set as a sparse matrix in the ODEProblem type.\n\nThe choices for globalization strategy are:\n\n:None: No globalization strategy\n:LineSearch: A line search globalization strategy\n\nOther options:\n\nmaxsetupcalls: Maximum number of nonlinear iterations that can be performed between calls to the preconditioner or Jacobian setup function.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/optimizing_parameterized_ode/#optimizing-parameterized-ode","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"","category":"section"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"Let us fit a parameterized ODE to some data. We will use the Lotka-Volterra model as an example. We will use Single Shooting to fit the parameters.","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"import OrdinaryDiffEqTsit5 as ODE\nimport NonlinearSolve as NLS\nimport Plots","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"Let us simulate some real data from the Lotka-Volterra model.","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"function lotka_volterra!(du, u, p, t)\n    x, y = u\n    Œ±, Œ≤, Œ¥, Œ≥ = p\n    du[1] = dx = Œ± * x - Œ≤ * x * y\n    du[2] = dy = -Œ¥ * y + Œ≥ * x * y\nend\n\n# Initial condition\nu0 = [1.0, 1.0]\n\n# Simulation interval and intermediary points\ntspan = (0.0, 2.0)\ntsteps = 0.0:0.1:10.0\n\n# LV equation parameter. p = [Œ±, Œ≤, Œ¥, Œ≥]\np = [1.5, 1.0, 3.0, 1.0]\n\n# Setup the ODE problem, then solve\nprob = ODE.ODEProblem(lotka_volterra!, u0, tspan, p)\nsol = ODE.solve(prob, ODE.Tsit5(); saveat = tsteps)\n\n# Plot the solution\nPlots.plot(sol; linewidth = 3)","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"Let us now formulate the parameter estimation as a Nonlinear Least Squares Problem.","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"function loss_function(ode_param, data)\n    sol = ODE.solve(prob, ODE.Tsit5(); p = ode_param, saveat = tsteps)\n    return vec(reduce(hcat, sol.u)) .- data\nend\n\np_init = zeros(4)\n\nnlls_prob = NLS.NonlinearLeastSquaresProblem(loss_function, p_init, vec(reduce(hcat, sol.u)))","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"Now, we can use any NLLS solver to solve this problem.","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"res = NLS.solve(\n    nlls_prob, NLS.LevenbergMarquardt(); maxiters = 1000, show_trace = Val(true),\n    trace_level = NLS.TraceWithJacobianConditionNumber(25))\nnothing # hide","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"res","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"We can also use Trust Region methods.","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"res = NLS.solve(nlls_prob, NLS.TrustRegion(); maxiters = 1000, show_trace = Val(true),\n    trace_level = NLS.TraceWithJacobianConditionNumber(25))\nnothing # hide","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"res","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"Let's plot the solution.","category":"page"},{"location":"tutorials/optimizing_parameterized_ode/","page":"Optimizing a Parameterized ODE","title":"Optimizing a Parameterized ODE","text":"prob2 = ODE.remake(prob; tspan = (0.0, 10.0))\nsol_fit = ODE.solve(prob2, ODE.Tsit5(); p = res.u)\nsol_true = ODE.solve(prob2, ODE.Tsit5(); p = p)\nPlots.plot(sol_true; linewidth = 3)\nPlots.plot!(sol_fit; linewidth = 3, linestyle = :dash)","category":"page"},{"location":"api/speedmapping/#SpeedMapping.jl","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"","category":"section"},{"location":"api/speedmapping/","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"This is a extension for importing solvers from SpeedMapping.jl into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"api/speedmapping/","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"import Pkg\nPkg.add(\"SpeedMapping\")\nimport SpeedMapping\nimport NonlinearSolve as NLS","category":"page"},{"location":"api/speedmapping/#Solver-API","page":"SpeedMapping.jl","title":"Solver API","text":"","category":"section"},{"location":"api/speedmapping/#NonlinearSolve.SpeedMappingJL","page":"SpeedMapping.jl","title":"NonlinearSolve.SpeedMappingJL","text":"SpeedMappingJL(;\n    œÉ_min = 0.0, stabilize::Bool = false, check_obj::Bool = false,\n    orders::Vector{Int} = [3, 3, 2]\n)\n\nWrapper over SpeedMapping.jl for solving Fixed Point Problems. We allow using this algorithm to solve root finding problems as well.\n\nKeyword Arguments\n\nœÉ_min: Setting to 1 may avoid stalling (see [12]).\nstabilize: performs a stabilization mapping before extrapolating. Setting to true may improve the performance for applications like accelerating the EM or MM algorithms (see [12]).\ncheck_obj: In case of NaN or Inf values, the algorithm restarts at the best past iterate.\norders: determines ACX's alternating order. Must be between 1 and 3 (where 1 means no extrapolation). The two recommended orders are [3, 2] and [3, 3, 2], the latter being potentially better for highly non-linear applications (see [12]).\n\nnote: Note\nThis algorithm is only available if SpeedMapping.jl is installed and loaded.\n\n\n\n\n\n","category":"type"},{"location":"basics/termination_condition/#termination_condition","page":"Termination Conditions","title":"Termination Conditions","text":"","category":"section"},{"location":"basics/termination_condition/","page":"Termination Conditions","title":"Termination Conditions","text":"Provides a API to specify termination conditions for NonlinearProblem and SteadyStateProblem. For details on the various termination modes:","category":"page"},{"location":"basics/termination_condition/#Termination-Conditions","page":"Termination Conditions","title":"Termination Conditions","text":"","category":"section"},{"location":"basics/termination_condition/","page":"Termination Conditions","title":"Termination Conditions","text":"The termination condition is constructed as:","category":"page"},{"location":"basics/termination_condition/","page":"Termination Conditions","title":"Termination Conditions","text":"cache = init(du, u, AbsSafeBestTerminationMode(); abstol = 1e-9, reltol = 1e-9)","category":"page"},{"location":"basics/termination_condition/","page":"Termination Conditions","title":"Termination Conditions","text":"If abstol and reltol are not supplied, then we choose a default based on the element types of du and u.","category":"page"},{"location":"basics/termination_condition/","page":"Termination Conditions","title":"Termination Conditions","text":"To test for termination simply call the cache:","category":"page"},{"location":"basics/termination_condition/","page":"Termination Conditions","title":"Termination Conditions","text":"terminated = cache(du, u, uprev)","category":"page"},{"location":"basics/termination_condition/#Absolute-Tolerance","page":"Termination Conditions","title":"Absolute Tolerance","text":"","category":"section"},{"location":"basics/termination_condition/#NonlinearSolveBase.AbsTerminationMode","page":"Termination Conditions","title":"NonlinearSolveBase.AbsTerminationMode","text":"AbsTerminationMode <: AbstractNonlinearTerminationMode\n\nTerminates if all left(  Delta u  leq abstol right). .\n\nDelta u denotes the increment computed by the nonlinear solver and u denotes the solution.\n\n\n\n\n\n","category":"type"},{"location":"basics/termination_condition/#NonlinearSolveBase.AbsNormTerminationMode","page":"Termination Conditions","title":"NonlinearSolveBase.AbsNormTerminationMode","text":"AbsNormTerminationMode <: AbstractNonlinearTerminationMode\n\nTerminates if  Delta u  leq abstol. .\n\nDelta u denotes the increment computed by the inner nonlinear solver.\n\nConstructor\n\nAbsNormTerminationMode(internalnorm = nothing)\n\nwhere internalnorm is the norm to use for the termination condition. Special handling is done for norm(_, 2), norm, norm(_, Inf), and maximum(abs, _)..\n\n\n\n\n\n","category":"type"},{"location":"basics/termination_condition/#NonlinearSolveBase.AbsNormSafeTerminationMode","page":"Termination Conditions","title":"NonlinearSolveBase.AbsNormSafeTerminationMode","text":"AbsNormSafeTerminationMode <: NonlinearSolveBase.AbstractSafeNonlinearTerminationMode\n\nEssentially AbsNormTerminationMode + terminate if there has been no improvement for the last patience_steps + terminate if the solution blows up (diverges).\n\nConstructor\n\nAbsNormSafeTerminationMode(\n    internalnorm; protective_threshold = nothing,\n    patience_steps = 100, patience_objective_multiplier = 3,\n    min_max_factor = 1.3, max_stalled_steps = nothing\n)\n\nwhere internalnorm is the norm to use for the termination condition. Special handling is done for norm(_, 2), norm, norm(_, Inf), and maximum(abs, _)..\n\n\n\n\n\n","category":"type"},{"location":"basics/termination_condition/#NonlinearSolveBase.AbsNormSafeBestTerminationMode","page":"Termination Conditions","title":"NonlinearSolveBase.AbsNormSafeBestTerminationMode","text":"AbsNormSafeBestTerminationMode <: NonlinearSolveBase.AbstractSafeBestNonlinearTerminationMode\n\nEssentially AbsNormSafeTerminationMode, but caches the bestsolution found so far.\n\nConstructor\n\nAbsNormSafeBestTerminationMode(\n    internalnorm; protective_threshold = nothing,\n    patience_steps = 100, patience_objective_multiplier = 3,\n    min_max_factor = 1.3, max_stalled_steps = nothing\n)\n\nwhere internalnorm is the norm to use for the termination condition. Special handling is done for norm(_, 2), norm, norm(_, Inf), and maximum(abs, _)..\n\n\n\n\n\n","category":"type"},{"location":"basics/termination_condition/#Relative-Tolerance","page":"Termination Conditions","title":"Relative Tolerance","text":"","category":"section"},{"location":"basics/termination_condition/#NonlinearSolveBase.RelTerminationMode","page":"Termination Conditions","title":"NonlinearSolveBase.RelTerminationMode","text":"RelTerminationMode <: AbstractNonlinearTerminationMode\n\nTerminates if all left( Delta u  leq reltol times  u  right). .\n\nDelta u denotes the increment computed by the nonlinear solver and u denotes the solution.\n\n\n\n\n\n","category":"type"},{"location":"basics/termination_condition/#NonlinearSolveBase.RelNormTerminationMode","page":"Termination Conditions","title":"NonlinearSolveBase.RelNormTerminationMode","text":"RelNormTerminationMode <: AbstractNonlinearTerminationMode\n\nTerminates if  Delta u  leq reltol times  Delta u + u . .\n\nDelta u denotes the increment computed by the inner nonlinear solver.\n\nConstructor\n\nRelNormTerminationMode(internalnorm = nothing)\n\nwhere internalnorm is the norm to use for the termination condition. Special handling is done for norm(_, 2), norm, norm(_, Inf), and maximum(abs, _)..\n\n\n\n\n\n","category":"type"},{"location":"basics/termination_condition/#NonlinearSolveBase.RelNormSafeTerminationMode","page":"Termination Conditions","title":"NonlinearSolveBase.RelNormSafeTerminationMode","text":"RelNormSafeTerminationMode <: NonlinearSolveBase.AbstractSafeNonlinearTerminationMode\n\nEssentially RelNormTerminationMode + terminate if there has been no improvement for the last patience_steps + terminate if the solution blows up (diverges).\n\nConstructor\n\nRelNormSafeTerminationMode(\n    internalnorm; protective_threshold = nothing,\n    patience_steps = 100, patience_objective_multiplier = 3,\n    min_max_factor = 1.3, max_stalled_steps = nothing\n)\n\nwhere internalnorm is the norm to use for the termination condition. Special handling is done for norm(_, 2), norm, norm(_, Inf), and maximum(abs, _)..\n\n\n\n\n\n","category":"type"},{"location":"basics/termination_condition/#NonlinearSolveBase.RelNormSafeBestTerminationMode","page":"Termination Conditions","title":"NonlinearSolveBase.RelNormSafeBestTerminationMode","text":"RelNormSafeBestTerminationMode <: NonlinearSolveBase.AbstractSafeBestNonlinearTerminationMode\n\nEssentially RelNormSafeTerminationMode, but caches the bestsolution found so far.\n\nConstructor\n\nRelNormSafeBestTerminationMode(\n    internalnorm; protective_threshold = nothing,\n    patience_steps = 100, patience_objective_multiplier = 3,\n    min_max_factor = 1.3, max_stalled_steps = nothing\n)\n\nwhere internalnorm is the norm to use for the termination condition. Special handling is done for norm(_, 2), norm, norm(_, Inf), and maximum(abs, _)..\n\n\n\n\n\n","category":"type"},{"location":"basics/termination_condition/#Both-Tolerances","page":"Termination Conditions","title":"Both Tolerances","text":"","category":"section"},{"location":"basics/termination_condition/#NonlinearSolveBase.NormTerminationMode","page":"Termination Conditions","title":"NonlinearSolveBase.NormTerminationMode","text":"NormTerminationMode <: AbstractNonlinearTerminationMode\n\nTerminates if  Delta u  leq reltol times  Delta u + u  or  Delta u  leq abstol. .\n\nDelta u denotes the increment computed by the inner nonlinear solver.\n\nConstructor\n\nNormTerminationMode(internalnorm = nothing)\n\nwhere internalnorm is the norm to use for the termination condition. Special handling is done for norm(_, 2), norm, norm(_, Inf), and maximum(abs, _)..\n\n\n\n\n\n","category":"type"},{"location":"solvers/fixed_point_solvers/#Fixed-Point-Solvers","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"","category":"section"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"Currently we don't have an API to directly specify Fixed Point Solvers. However, a Fixed Point Problem can be trivially converted to a Root Finding Problem. Say we want to solve:","category":"page"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"f(u) = u","category":"page"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"This can be written as:","category":"page"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"g(u) = f(u) - u = 0","category":"page"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"g(u) = 0 is a root finding problem. Note that we can use any root finding algorithm to solve this problem. However, this is often not the most efficient way to solve a fixed point problem. We provide a few algorithms available via extensions that are more efficient for fixed point problems.","category":"page"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"Note that even if you use one of the Fixed Point Solvers mentioned here, you must still use the NonlinearProblem API to specify the problem, i.e., g(u) = 0.","category":"page"},{"location":"solvers/fixed_point_solvers/#Recommended-Methods","page":"Fixed Point Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"Using native NonlinearSolve.jl methods is the recommended approach. For systems where constructing Jacobian Matrices are expensive, we recommend using a Krylov Method with one of those solvers.","category":"page"},{"location":"solvers/fixed_point_solvers/#fixed_point_methods_full_list","page":"Fixed Point Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"We are only listing the methods that natively solve fixed point problems.","category":"page"},{"location":"solvers/fixed_point_solvers/#SpeedMapping.jl","page":"Fixed Point Solvers","title":"SpeedMapping.jl","text":"","category":"section"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"SpeedMappingJL(): accelerates the convergence of a mapping to a fixed point by the Alternating cyclic extrapolation algorithm (ACX).","category":"page"},{"location":"solvers/fixed_point_solvers/#FixedPointAcceleration.jl","page":"Fixed Point Solvers","title":"FixedPointAcceleration.jl","text":"","category":"section"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"FixedPointAccelerationJL(): accelerates the convergence of a mapping to a fixed point by the Anderson acceleration algorithm and a few other methods.","category":"page"},{"location":"solvers/fixed_point_solvers/#NLsolve.jl","page":"Fixed Point Solvers","title":"NLsolve.jl","text":"","category":"section"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"In our tests, we have found the anderson method implemented here to NOT be the most robust.","category":"page"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"NLsolveJL(; method = :anderson): Anderson acceleration for fixed point problems.","category":"page"},{"location":"solvers/fixed_point_solvers/#SIAMFANLEquations.jl","page":"Fixed Point Solvers","title":"SIAMFANLEquations.jl","text":"","category":"section"},{"location":"solvers/fixed_point_solvers/","page":"Fixed Point Solvers","title":"Fixed Point Solvers","text":"SIAMFANLEquationsJL(; method = :anderson): Anderson acceleration for fixed point problems.","category":"page"}]
}
