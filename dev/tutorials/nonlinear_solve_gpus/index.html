<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Accelerated Rootfinding on GPUs · NonlinearSolve.jl</title><meta name="title" content="Accelerated Rootfinding on GPUs · NonlinearSolve.jl"/><meta property="og:title" content="Accelerated Rootfinding on GPUs · NonlinearSolve.jl"/><meta property="twitter:title" content="Accelerated Rootfinding on GPUs · NonlinearSolve.jl"/><meta name="description" content="Documentation for NonlinearSolve.jl."/><meta property="og:description" content="Documentation for NonlinearSolve.jl."/><meta property="twitter:description" content="Documentation for NonlinearSolve.jl."/><meta property="og:url" content="https://docs.sciml.ai/NonlinearSolve/stable/tutorials/nonlinear_solve_gpus/"/><meta property="twitter:url" content="https://docs.sciml.ai/NonlinearSolve/stable/tutorials/nonlinear_solve_gpus/"/><link rel="canonical" href="https://docs.sciml.ai/NonlinearSolve/stable/tutorials/nonlinear_solve_gpus/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/citations.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NonlinearSolve.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NonlinearSolve.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers</a></li><li><a class="tocitem" href="../getting_started/">Getting Started with Nonlinear Rootfinding in Julia</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../code_optimization/">Code Optimization for Small Nonlinear Systems in Julia</a></li><li><a class="tocitem" href="../large_systems/">Efficiently Solving Large Sparse Ill-Conditioned Nonlinear Systems in Julia</a></li><li><a class="tocitem" href="../modelingtoolkit/">Symbolic Nonlinear System Definition and Acceleration via ModelingToolkit</a></li><li><a class="tocitem" href="../small_compile/">Faster Startup and and Static Compilation</a></li><li><a class="tocitem" href="../iterator_interface/">Nonlinear Solver Iterator Interface</a></li><li><a class="tocitem" href="../optimizing_parameterized_ode/">Optimizing a Parameterized ODE</a></li><li class="is-active"><a class="tocitem" href>Accelerated Rootfinding on GPUs</a><ul class="internal"><li><a class="tocitem" href="#GPU-Acceleration-of-Large-Nonlinear-Systems-using-GPUArrays"><span>GPU Acceleration of Large Nonlinear Systems using GPUArrays</span></a></li><li><a class="tocitem" href="#GPU-Acceleration-over-Large-Parameter-Searches-using-KernelAbstractions.jl"><span>GPU Acceleration over Large Parameter Searches using KernelAbstractions.jl</span></a></li></ul></li><li><a class="tocitem" href="../snes_ex2/">PETSc SNES Example 2</a></li></ul></li><li><span class="tocitem">Basics</span><ul><li><a class="tocitem" href="../../basics/nonlinear_problem/">Nonlinear Problems</a></li><li><a class="tocitem" href="../../basics/nonlinear_functions/">Nonlinear Functions and Jacobian Types</a></li><li><a class="tocitem" href="../../basics/solve/">Common Solver Options (Solve Keyword Arguments)</a></li><li><a class="tocitem" href="../../basics/nonlinear_solution/">Nonlinear Solutions</a></li><li><a class="tocitem" href="../../basics/autodiff/">Automatic Differentiation Backends</a></li><li><a class="tocitem" href="../../basics/termination_condition/">Termination Conditions</a></li><li><a class="tocitem" href="../../basics/diagnostics_api/">Diagnostics API</a></li><li><a class="tocitem" href="../../basics/sparsity_detection/">(Semi-)Automatic Sparsity Detection</a></li><li><a class="tocitem" href="../../basics/faq/">Frequently Asked Questions</a></li></ul></li><li><span class="tocitem">Solver Summaries and Recommendations</span><ul><li><a class="tocitem" href="../../solvers/nonlinear_system_solvers/">Nonlinear System Solvers</a></li><li><a class="tocitem" href="../../solvers/bracketing_solvers/">Interval Root-Finding Methods (Bracketing Solvers)</a></li><li><a class="tocitem" href="../../solvers/steady_state_solvers/">Steady State Solvers</a></li><li><a class="tocitem" href="../../solvers/nonlinear_least_squares_solvers/">Nonlinear Least Squares Solvers</a></li><li><a class="tocitem" href="../../solvers/fixed_point_solvers/">Fixed Point Solvers</a></li></ul></li><li><span class="tocitem">Native Functionalities</span><ul><li><a class="tocitem" href="../../native/solvers/">NonlinearSolve.jl Solvers</a></li><li><a class="tocitem" href="../../native/simplenonlinearsolve/">SimpleNonlinearSolve.jl</a></li><li><a class="tocitem" href="../../native/bracketingnonlinearsolve/">BracketingNonlinearSolve.jl</a></li><li><a class="tocitem" href="../../native/steadystatediffeq/">SteadyStateDiffEq.jl</a></li><li><a class="tocitem" href="../../native/descent/">Descent Subroutines</a></li><li><a class="tocitem" href="../../native/globalization/">Globalization Subroutines</a></li><li><a class="tocitem" href="../../native/diagnostics/">Diagnostics API</a></li></ul></li><li><span class="tocitem">Wrapped Solver APIs</span><ul><li><a class="tocitem" href="../../api/fastlevenbergmarquardt/">FastLevenbergMarquardt.jl</a></li><li><a class="tocitem" href="../../api/fixedpointacceleration/">FixedPointAcceleration.jl</a></li><li><a class="tocitem" href="../../api/leastsquaresoptim/">LeastSquaresOptim.jl</a></li><li><a class="tocitem" href="../../api/minpack/">MINPACK.jl</a></li><li><a class="tocitem" href="../../api/nlsolve/">NLsolve.jl</a></li><li><a class="tocitem" href="../../api/nlsolvers/">NLSolvers.jl</a></li><li><a class="tocitem" href="../../api/petsc/">PETSc.jl</a></li><li><a class="tocitem" href="../../api/scipy/">SciPy</a></li><li><a class="tocitem" href="../../api/siamfanlequations/">SIAMFANLEquations.jl</a></li><li><a class="tocitem" href="../../api/speedmapping/">SpeedMapping.jl</a></li><li><a class="tocitem" href="../../api/sundials/">Sundials.jl</a></li><li><a class="tocitem" href="../../api/homotopycontinuation/">HomotopyContinuation.jl</a></li></ul></li><li><span class="tocitem">Sub-Packages</span><ul><li><a class="tocitem" href="../../api/SciMLJacobianOperators/">SciMLJacobianOperators.jl</a></li></ul></li><li><span class="tocitem">Development Documentation</span><ul><li><a class="tocitem" href="../../devdocs/internal_interfaces/">Internal Abstract Types</a></li><li><a class="tocitem" href="../../devdocs/linear_solve/">Linear Solve</a></li><li><a class="tocitem" href="../../devdocs/jacobian/">Jacobian Wrappers</a></li><li><a class="tocitem" href="../../devdocs/operators/">Custom SciML Operators</a></li><li><a class="tocitem" href="../../devdocs/algorithm_helpers/">Internal Algorithm Helpers</a></li></ul></li><li><a class="tocitem" href="../../release_notes/">Release Notes</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Accelerated Rootfinding on GPUs</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Accelerated Rootfinding on GPUs</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/NonlinearSolve.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/NonlinearSolve.jl/blob/master/docs/src/tutorials/nonlinear_solve_gpus.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Accelerated-Rootfinding-on-GPUs"><a class="docs-heading-anchor" href="#Accelerated-Rootfinding-on-GPUs">Accelerated Rootfinding on GPUs</a><a id="Accelerated-Rootfinding-on-GPUs-1"></a><a class="docs-heading-anchor-permalink" href="#Accelerated-Rootfinding-on-GPUs" title="Permalink"></a></h1><p>NonlinearSolve.jl supports GPU acceleration on a wide array of devices, such as:</p><table><tr><th style="text-align: left">GPU Manufacturer</th><th style="text-align: left">GPU Kernel Language</th><th style="text-align: left">Julia Support Package</th><th style="text-align: left">Backend Type</th></tr><tr><td style="text-align: left">NVIDIA</td><td style="text-align: left">CUDA</td><td style="text-align: left"><a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a></td><td style="text-align: left"><code>CUDA.CUDABackend()</code></td></tr><tr><td style="text-align: left">AMD</td><td style="text-align: left">ROCm</td><td style="text-align: left"><a href="https://github.com/JuliaGPU/AMDGPU.jl">AMDGPU.jl</a></td><td style="text-align: left"><code>AMDGPU.ROCBackend()</code></td></tr><tr><td style="text-align: left">Intel</td><td style="text-align: left">OneAPI</td><td style="text-align: left"><a href="https://github.com/JuliaGPU/oneAPI.jl">OneAPI.jl</a></td><td style="text-align: left"><code>oneAPI.oneAPIBackend()</code></td></tr><tr><td style="text-align: left">Apple (M-Series)</td><td style="text-align: left">Metal</td><td style="text-align: left"><a href="https://github.com/JuliaGPU/Metal.jl">Metal.jl</a></td><td style="text-align: left"><code>Metal.MetalBackend()</code></td></tr></table><p>To use NonlinearSolve.jl on GPUs, there are two distinctly different approaches:</p><ol><li>You can build a <code>NonlinearProblem</code> / <code>NonlinearLeastSquaresProblem</code> where the elements of the problem, i.e. <code>u0</code> and <code>p</code>, are defined on GPUs. This will make the evaluations of <code>f</code> occur on the GPU, and all internal updates of the solvers will be completely on the GPU as well. This is the optimal form for large systems of nonlinear equations.</li><li>You can use SimpleNonlinearSolve.jl as kernels in KernelAbstractions.jl. This will build problem-specific GPU kernels in order to parallelize the solution of the chosen nonlinear system over a large number of inputs. This is useful for cases where you have a small <code>NonlinearProblem</code> / <code>NonlinearLeastSquaresProblem</code> which you want to solve over a large number of initial guesses or parameters.</li></ol><p>For a deeper dive into the computational difference between these techniques and why it leads to different pros/cons, see the <a href="https://www.sciencedirect.com/science/article/abs/pii/S0045782523007156">DiffEqGPU.jl technical paper</a>. In particular, the second form is unique to NonlinearSolve.jl and offers orders of magnitude performance improvements over libraries in Jax and PyTorch which are restricted to only using the first form.</p><p>In this tutorial we will highlight both use cases in separate parts.</p><div class="admonition is-info" id="Note-54d9bea3cd56b6f8"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-54d9bea3cd56b6f8" title="Permalink"></a></header><div class="admonition-body"><p>If you&#39;re looking for GPU-accelerated neural networks inside of nonlinear solvers, check out <a href="https://docs.sciml.ai/DeepEquilibriumNetworks/stable/">DeepEquilibriumNetworks.jl</a>.</p></div></div><h2 id="GPU-Acceleration-of-Large-Nonlinear-Systems-using-GPUArrays"><a class="docs-heading-anchor" href="#GPU-Acceleration-of-Large-Nonlinear-Systems-using-GPUArrays">GPU Acceleration of Large Nonlinear Systems using GPUArrays</a><a id="GPU-Acceleration-of-Large-Nonlinear-Systems-using-GPUArrays-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Acceleration-of-Large-Nonlinear-Systems-using-GPUArrays" title="Permalink"></a></h2><p>The simplest way to GPU accelerate a large system is to simply make your <code>u0</code> and <code>p</code> values be on the GPU via GPUArrays. For example, CUDA.jl has the CuArray type which implements standard array operations, such as broadcasting and linear algebra. And since NonlinearSolve.jl respects your chosen array types, if you choose to make <code>u0</code> be a type that is on the GPU, then all internal broadcasting and linear algebra takes place on the GPU.</p><p>This means the one major limitation is that you as a user must write your <code>f</code> to be compatible with GPU arrays. Those limitations are discussed in detail in the GPU libraries, for example <a href="https://cuda.juliagpu.org/stable/usage/array/">the CuArray documentation discusses the operations available for CUDA arrays</a></p><p>In practice when this comes together, it looks like:</p><pre><code class="language-julia hljs">import NonlinearSolve as NLS
import CUDA

f(u, p) = u .* u .- p
u0 = CUDA.cu(ones(1000))
p = CUDA.cu(collect(1:1000))
prob = NLS.NonlinearProblem(f, u0, p)
sol = NLS.solve(prob, NLS.NewtonRaphson(), abstol = 1.0f-4)</code></pre><p>Notice a few things here. One, nothing is different except the input array types. But notice that <code>cu</code> arrays automatically default to <code>Float32</code> precision. Since NonlinearSolve.jl respects the user&#39;s chosen types, this changes NonlinearSolve.jl to use <code>Float32</code> precision, and thus the tolerances are adjusted accordingly.</p><h2 id="GPU-Acceleration-over-Large-Parameter-Searches-using-KernelAbstractions.jl"><a class="docs-heading-anchor" href="#GPU-Acceleration-over-Large-Parameter-Searches-using-KernelAbstractions.jl">GPU Acceleration over Large Parameter Searches using KernelAbstractions.jl</a><a id="GPU-Acceleration-over-Large-Parameter-Searches-using-KernelAbstractions.jl-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Acceleration-over-Large-Parameter-Searches-using-KernelAbstractions.jl" title="Permalink"></a></h2><p>If one has a &quot;small&quot; (200 equations or less) system of equations which they wish to solve over many different inputs (parameters), then using the kernel generation approach will be much more efficient than using the GPU-based array approach. In short, the GPU array approach strings together standard GPU kernel calls (matrix multiply, +, etc.) where each operation is an optimized GPU-accelerated call. In the kernel-building approach, we build a custom kernel <code>f</code> that is then compiled specifically for our problem and ran in parallel. This is equivalent to having built custom CUDA code for our problem! The reason this is so much faster is because each kernel call has startup overhead, and we can cut that all down to simply one optimized call.</p><p>To do this, we use KernelAbstractions.jl. First we have to say &quot;what&quot; our kernel is. The kernel is the thing you want to embaressingly parallel call a bunch. For this nonlinear solving, it will be the rebuilding of our nonlinear problem to new parameters and solving it. This function must be defined using the <code>KernelAbstractions.@kernel</code> macro. This looks like:</p><pre><code class="language-julia hljs">import NonlinearSolve as NLS
import StaticArrays
import SciMLBase
import KernelAbstractions # For writing vendor-agnostic kernels
import CUDA # For if you have an NVIDIA GPU
import AMDGPU # For if you have an AMD GPU
import Metal # For if you have a Mac M-series device and want to use the built-in GPU
import OneAPI # For if you have an Intel GPU

KernelAbstractions.@kernel function parallel_nonlinearsolve_kernel!(result, @Const(prob), @Const(alg))
    i = @index(Global)
    prob_i = SciMLBase.remake(prob; p = prob.p[i])
    sol = NLS.solve(prob_i, alg)
    @inbounds result[i] = sol.u
end</code></pre><p>Note that <code>i = @index(Global)</code> is used to get a global index. I.e. this kernel will be called with <code>N</code> different <code>prob</code> objects, and this <code>i</code> means &quot;for the ith call&quot;. So this is saying, &quot;for the ith call, get the i&#39;th parameter set and solve with these parameters. The ith result is then this solution&quot;.</p><div class="admonition is-info" id="Note-520921a1a05fd5a4"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-520921a1a05fd5a4" title="Permalink"></a></header><div class="admonition-body"><p>Because kernel code needs to be able to be compiled to a GPU kernel, it has very strict specifications of what&#39;s allowed because GPU cores are not as flexible as CPU cores. In general, this means that you need to avoid any runtime operations in kernel code, such as allocating vectors, dynamic dispatch, type instabilities, etc. The main thing to note is that most NonlinearSolve.jl algorithms will not be compatible with being in kernels. However, <strong>SimpleNonlinearSolve.jl solvers are tested to be compatible</strong>, and thus one should only choose SimpleNonlinearSolve.jl methods within kernels.</p></div></div><p>Once you have defined your kernel, you need to use KernelAbstractions in order to distribute your call. This looks like:</p><pre><code class="language-julia hljs">function vectorized_solve(prob, alg; backend = CPU())
    result = KernelAbstractions.allocate(backend, eltype(prob.p), length(prob.p))
    groupsize = min(length(prob.p), 1024)
    kernel! = parallel_nonlinearsolve_kernel!(backend, groupsize, length(prob.p))
    kernel!(result, prob, alg)
    KernelAbstractions.synchronize(backend)
    return result
end</code></pre><p>Now let&#39;s build a nonlinear system to test it on.</p><pre><code class="language-julia hljs">@inbounds function p2_f(x, p)
    out1 = x[1] + p[1] * x[2]
    out2 = sqrt(p[2]) * (x[3] - x[4])
    out3 = (x[2] - p[3] * x[3])^2
    out4 = sqrt(p[4]) * (x[1] - x[4]) * (x[1] - x[4])
    StaticArrays.SA[out1, out2, out3, out4]
end

p = StaticArrays.@SVector [StaticArrays.@SVector(rand(Float32, 4)) for _ in 1:1024]
u0 = StaticArrays.SA[1.0f0, 2.0f0, 3.0f0, 4.0f0]
prob = SciMLBase.ImmutableNonlinearProblem{false}(p2_f, u0, p)</code></pre><div class="admonition is-info" id="Note-fcf4517ad56d4606"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-fcf4517ad56d4606" title="Permalink"></a></header><div class="admonition-body"><p>Because the custom kernel is going to need to embed the the code for our nonlinear problem into the kernel, it also must be written to be GPU compatible. In general, this means that you need to avoid any runtime operations in kernel code, such as allocating vectors, dynamic dispatch, type instabilities, etc. Thus to make this work, your <code>f</code> function should be non-allocating, your <code>u0</code> function should use StaticArrays, and you must use <code>SciMLBase.ImmutableNonlinearProblem</code> (which is exactly the same as NonlinearProblem except it&#39;s immutable to satisfy the requirements of GPU kernels). Also, it&#39;s recommended that for most GPUs you use Float32 precision because many GPUs are much slower on 64-bit floating point operations.</p></div></div><p>and we then simply call our vectorized kernel to parallelize it:</p><pre><code class="language-julia hljs"># Threaded CPU
vectorized_solve(prob, NLS.SimpleNewtonRaphson(); backend = KernelAbstractions.CPU())
# AMD ROCM GPU
vectorized_solve(prob, NLS.SimpleNewtonRaphson(); backend = AMDGPU.ROCBackend())
# NVIDIA CUDA GPU
vectorized_solve(prob, NLS.SimpleNewtonRaphson(); backend = CUDA.CUDABackend())
# Intel GPU
vectorized_solve(prob, NLS.SimpleNewtonRaphson(); backend = OneAPI.oneAPIBackend())
# Mac M-Series, such as M3Max
vectorized_solve(prob, NLS.SimpleNewtonRaphson(); backend = Metal.MetalBackend())</code></pre><div class="admonition is-category-warn" id="Warn-8a3fb3dfedc3753b"><header class="admonition-header">Warn<a class="admonition-anchor" href="#Warn-8a3fb3dfedc3753b" title="Permalink"></a></header><div class="admonition-body"><p>The GPU-based calls will only work on your machine if you have a compatible GPU!</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../optimizing_parameterized_ode/">« Optimizing a Parameterized ODE</a><a class="docs-footer-nextpage" href="../snes_ex2/">PETSc SNES Example 2 »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Tuesday 4 November 2025 21:01">Tuesday 4 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
